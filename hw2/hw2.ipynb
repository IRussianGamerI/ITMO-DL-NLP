{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Домашнее задание 2\n",
        "Барабанщиков Лев Романович\n",
        "\n",
        "В этом задании продолжаем работу с датасетом lenta-ru-news для задачи классификации текстов по топикам, но используем word2vec эмбеддинги.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Установка random seed и импорт библиотек\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random seed установлен на значение: 777\n",
            "Все библиотеки импортированы успешно!\n"
          ]
        }
      ],
      "source": [
        "# Импорт необходимых библиотек\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import gc\n",
        "import warnings\n",
        "\n",
        "# NLP библиотеки\n",
        "import nltk\n",
        "import pymorphy2\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Эмбеддинги\n",
        "from gensim.models import Word2Vec\n",
        "import navec\n",
        "\n",
        "# ML библиотеки\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Обработка данных\n",
        "from corus import load_lenta\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Установка random seed для воспроизводимости результатов\n",
        "RANDOM_SEED = 777\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "print(\"Random seed установлен на значение:\", RANDOM_SEED)\n",
        "print(\"Все библиотеки импортированы успешно!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Загрузка и предобработка данных\n",
        "\n",
        "Переиспользуем данные из ДЗ1 и подготовим их для работы с эмбеддингами\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Данные загружены!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fc1848f5fd74ba180d829ce5af69b52",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Обработка записей: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Загружено 739351 записей\n"
          ]
        }
      ],
      "source": [
        "# Загрузка данных из ДЗ1 (переиспользуем подготовленные данные)\n",
        "path = '../hw1/lenta-ru-news.csv.gz'\n",
        "records = load_lenta(path)\n",
        "\n",
        "print(\"Данные загружены!\")\n",
        "\n",
        "# Преобразуем в список\n",
        "data = []\n",
        "for record in tqdm(records, desc=\"Обработка записей\"):\n",
        "    if record.topic is None:\n",
        "        continue\n",
        "    data.append({\n",
        "        'title': record.title,\n",
        "        'text': record.text,\n",
        "        'topic': record.topic\n",
        "    })\n",
        "\n",
        "print(f\"Загружено {len(data)} записей\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Размер датасета: 739351\n",
            "Количество уникальных топиков: 24\n",
            "\n",
            "Топ-10 самых популярных топиков:\n",
            "topic\n",
            "Россия             160519\n",
            "Мир                136680\n",
            "Экономика           79538\n",
            "Спорт               64421\n",
            "Культура            53803\n",
            "Бывший СССР         53402\n",
            "Наука и техника     53136\n",
            "Интернет и СМИ      44675\n",
            "Из жизни            27611\n",
            "Дом                 21734\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Пример записи:\n",
            "Топик: Россия\n",
            "Содержание: Названы регионы России с самой высокой смертностью от рака Вице-премьер по социальным вопросам Татьяна Голикова рассказала, в каких регионах России зафиксирована наиболее высокая смертность от рака, с...\n"
          ]
        }
      ],
      "source": [
        "# Создаем DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Вывод информации о датасете\n",
        "print(\"Размер датасета:\", len(df))\n",
        "print(\"Количество уникальных топиков:\", df['topic'].nunique())\n",
        "print(\"\\nТоп-10 самых популярных топиков:\")\n",
        "print(df['topic'].value_counts().head(10))\n",
        "\n",
        "# Объединяем title и text\n",
        "df['content'] = df['title'] + ' ' + df['text']\n",
        "\n",
        "print(\"\\nПример записи:\")\n",
        "print(\"Топик:\", df.iloc[0]['topic'])\n",
        "print(\"Содержание:\", df.iloc[0]['content'][:200] + \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Предобработка текстов\n",
        "\n",
        "Подготовка текстов для обучения word2vec эмбеддингов и классификации\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Инструменты предобработки готовы!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/bitcoin/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/bitcoin/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/bitcoin/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Загрузка NLTK данных\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Инициализация морфологического анализатора и стоп-слов\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "stop_words = set(stopwords.words('russian'))\n",
        "\n",
        "print(\"Инструменты предобработки готовы!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Функции предобработки определены!\n",
            "\n",
            "Тест предобработки:\n",
            "Исходный текст: Это тестовый текст, который мы будем обрабатывать для проверки!\n",
            "Токены: ['это', 'тестовый', 'текст', 'который', 'быть', 'обрабатывать', 'проверка']\n",
            "Строка: это тестовый текст который быть обрабатывать проверка\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Предобработка текста: очистка, токенизация, лемматизация\n",
        "    Возвращает список токенов для обучения эмбеддингов\n",
        "    \"\"\"\n",
        "    # Очистка текста\n",
        "    text = re.sub(r'[^а-яёa-z\\s]|\\d+', '', text.lower())\n",
        "    \n",
        "    # Токенизация\n",
        "    tokens = word_tokenize(text, language=\"russian\")\n",
        "    \n",
        "    # Лемматизация и фильтрация\n",
        "    tokens = [morph.parse(token)[0].normal_form \n",
        "              for token in tokens \n",
        "              if token not in stop_words and len(token) > 2]\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "def preprocess_text_string(text):\n",
        "    \"\"\"\n",
        "    Предобработка текста с возвращением строки (для совместимости с ДЗ1)\n",
        "    \"\"\"\n",
        "    tokens = preprocess_text(text)\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "print(\"Функции предобработки определены!\")\n",
        "\n",
        "# Тест функции\n",
        "test_text = \"Это тестовый текст, который мы будем обрабатывать для проверки!\"\n",
        "print(\"\\nТест предобработки:\")\n",
        "print(f\"Исходный текст: {test_text}\")\n",
        "print(f\"Токены: {preprocess_text(test_text)}\")\n",
        "print(f\"Строка: {preprocess_text_string(test_text)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Разделение датасета на обучающую, валидационную и тестовую выборки (60/20/20)\n",
        "\n",
        "**Обоснование:** Используем стратификацию для сохранения пропорций классов в каждой выборке. Пропорция 60/20/20 обеспечивает достаточно данных для обучения эмбеддингов и корректную валидацию моделей.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Работаем с выборкой размером: 100000 записей\n",
            "Предобрабатываем тексты с использованием параллелизма...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "756ed2824f5a47a682815a312bfd1500",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=12500), Label(value='0 / 12500')))…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28e6f64ef54b4ad08879ac840628320a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=12500), Label(value='0 / 12500')))…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "После фильтрации пустых текстов: 100000 записей\n"
          ]
        }
      ],
      "source": [
        "# Берем подвыборку для ускорения работы (можно увеличить для полного датасета)\n",
        "sample_size = 100000  # Уменьшаем для демонстрации\n",
        "df_sample = df.sample(n=min(sample_size, len(df)), random_state=RANDOM_SEED)\n",
        "\n",
        "print(f\"Работаем с выборкой размером: {len(df_sample)} записей\")\n",
        "\n",
        "# Инициализация pandarallel для параллельной обработки\n",
        "from pandarallel import pandarallel\n",
        "pandarallel.initialize(progress_bar=True, nb_workers=8, verbose=1)\n",
        "\n",
        "# Предобработка текстов с использованием параллелизма\n",
        "print(\"Предобрабатываем тексты с использованием параллелизма...\")\n",
        "\n",
        "# Параллельная обработка токенов\n",
        "df_sample['processed_tokens'] = df_sample['content'].parallel_apply(preprocess_text)\n",
        "\n",
        "# Параллельная обработка строк\n",
        "df_sample['processed_content'] = df_sample['content'].parallel_apply(preprocess_text_string)\n",
        "\n",
        "# Фильтруем пустые тексты\n",
        "df_sample = df_sample[df_sample['processed_tokens'].apply(len) > 0]\n",
        "print(f\"После фильтрации пустых текстов: {len(df_sample)} записей\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Анализ распределения классов...\n",
            "Всего классов: 21\n",
            "Классы с минимальным количеством образцов:\n",
            "topic\n",
            "Ценности          1031\n",
            "Бизнес            1015\n",
            "Путешествия        904\n",
            "69-я параллель     167\n",
            "Крым                94\n",
            "Культпросвет        50\n",
            "                    26\n",
            "Легпром             15\n",
            "Библиотека           7\n",
            "МедНовости           1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Классы с достаточным количеством образцов (>=6): 20\n",
            "Размер отфильтрованного датасета: 99999 (было 100000)\n",
            "\n",
            "Проводим стратифицированное разделение...\n",
            "\n",
            "Размеры выборок:\n",
            "Train: 59999 (60.0%)\n",
            "Valid: 20000 (20.0%)\n",
            "Test: 20000 (20.0%)\n",
            "\n",
            "Топ-5 классов в train выборке:\n",
            "topic\n",
            "Россия         13146\n",
            "Мир            11068\n",
            "Экономика       6440\n",
            "Спорт           5193\n",
            "Бывший СССР     4409\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Проверка стратификации - доли классов в выборках:\n",
            ": train=0.000, valid=0.000, test=0.000\n",
            "69-я параллель: train=0.002, valid=0.002, test=0.002\n",
            "Библиотека: train=0.000, valid=0.000, test=0.000\n",
            "Бизнес: train=0.010, valid=0.010, test=0.010\n",
            "Бывший СССР: train=0.073, valid=0.073, test=0.073\n",
            "\n",
            "Память очищена!\n"
          ]
        }
      ],
      "source": [
        "# Подготовка данных для разделения\n",
        "print(\"Анализ распределения классов...\")\n",
        "class_counts = df_sample['topic'].value_counts()\n",
        "print(f\"Всего классов: {len(class_counts)}\")\n",
        "print(f\"Классы с минимальным количеством образцов:\")\n",
        "print(class_counts.tail(10))\n",
        "\n",
        "# Фильтрация классов с малым количеством образцов (менее 6 для корректной стратификации)\n",
        "min_samples_per_class = 6\n",
        "valid_classes = class_counts[class_counts >= min_samples_per_class].index\n",
        "print(f\"\\nКлассы с достаточным количеством образцов (>={min_samples_per_class}): {len(valid_classes)}\")\n",
        "\n",
        "# Фильтруем датасет\n",
        "df_filtered = df_sample[df_sample['topic'].isin(valid_classes)].copy()\n",
        "print(f\"Размер отфильтрованного датасета: {len(df_filtered)} (было {len(df_sample)})\")\n",
        "\n",
        "# Разделение на train/valid/test (60/20/20)\n",
        "X = df_filtered['processed_content']\n",
        "y = df_filtered['topic']\n",
        "X_tokens = df_filtered['processed_tokens']\n",
        "\n",
        "print(f\"\\nПроводим стратифицированное разделение...\")\n",
        "\n",
        "# Первое разделение: 60% train, 40% остальное\n",
        "X_train, X_temp, y_train, y_temp, X_tokens_train, X_tokens_temp = train_test_split(\n",
        "    X, y, X_tokens, test_size=0.4, random_state=RANDOM_SEED, stratify=y\n",
        ")\n",
        "\n",
        "# Второе разделение: 20% valid, 20% test\n",
        "X_valid, X_test, y_valid, y_test, X_tokens_valid, X_tokens_test = train_test_split(\n",
        "    X_temp, y_temp, X_tokens_temp, test_size=0.5, random_state=RANDOM_SEED, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"\\nРазмеры выборок:\")\n",
        "print(f\"Train: {len(X_train)} ({len(X_train)/len(df_filtered)*100:.1f}%)\")\n",
        "print(f\"Valid: {len(X_valid)} ({len(X_valid)/len(df_filtered)*100:.1f}%)\")\n",
        "print(f\"Test: {len(X_test)} ({len(X_test)/len(df_filtered)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nТоп-5 классов в train выборке:\")\n",
        "print(y_train.value_counts().head())\n",
        "\n",
        "print(f\"\\nПроверка стратификации - доли классов в выборках:\")\n",
        "train_proportions = y_train.value_counts(normalize=True).sort_index()\n",
        "valid_proportions = y_valid.value_counts(normalize=True).sort_index()\n",
        "test_proportions = y_test.value_counts(normalize=True).sort_index()\n",
        "\n",
        "for class_name in train_proportions.index[:5]:  # Показываем первые 5 классов\n",
        "    print(f\"{class_name}: train={train_proportions[class_name]:.3f}, \"\n",
        "          f\"valid={valid_proportions[class_name]:.3f}, \"\n",
        "          f\"test={test_proportions[class_name]:.3f}\")\n",
        "\n",
        "# Очистка памяти\n",
        "del df_sample, df_filtered, data, records, df\n",
        "gc.collect()\n",
        "print(\"\\nПамять очищена!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Обучение Word2Vec эмбеддингов - 2 балла\n",
        "\n",
        "**Обоснование гиперпараметров:**\n",
        "- `vector_size=100` - размер векторов эмбеддингов. 100 измерений достаточно для русского языка\n",
        "- `window=5` - размер контекстного окна. 5 слов вокруг целевого слова обеспечивает хороший баланс между локальным и глобальным контекстом  \n",
        "- `min_count=5` - минимальная частота слова. Фильтрует редкие слова, которые могут внести шум\n",
        "- `workers=4` - количество потоков для ускорения обучения\n",
        "- `sg=0` - используем CBOW (Continuous Bag of Words), который лучше работает на больших датасетах\n",
        "- `epochs=10` - количество эпох обучения для получения качественных эмбеддингов\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Размер корпуса для обучения: 59999 текстов\n",
            "Пример токенов: ['награждение', 'премия', 'область', 'качество', 'сервис', 'товар', 'состояться', 'июнь', 'церемония', 'награждение']\n",
            "Word2Vec модель обучена!\n",
            "Размер словаря: 50533\n",
            "Размер векторов: 100\n"
          ]
        }
      ],
      "source": [
        "# Подготовка корпуса для обучения Word2Vec\n",
        "# Объединяем все токены из обучающей выборки\n",
        "train_corpus = X_tokens_train.tolist()\n",
        "\n",
        "print(f\"Размер корпуса для обучения: {len(train_corpus)} текстов\")\n",
        "print(f\"Пример токенов: {train_corpus[0][:10]}\")\n",
        "\n",
        "# Создание и обучение Word2Vec модели\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=train_corpus,\n",
        "    vector_size=100,     # размер векторов\n",
        "    window=5,            # размер контекстного окна\n",
        "    min_count=5,         # минимальная частота слова\n",
        "    workers=4,           # количество потоков\n",
        "    sg=0,                # CBOW алгоритм\n",
        "    epochs=10,           # количество эпох\n",
        "    seed=RANDOM_SEED     # параметр для воспроизводимости (не random_state!)\n",
        ")\n",
        "\n",
        "print(\"Word2Vec модель обучена!\")\n",
        "print(f\"Размер словаря: {len(w2v_model.wv.key_to_index)}\")\n",
        "print(f\"Размер векторов: {w2v_model.vector_size}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Оценка качества Word2Vec эмбеддингов ===\n",
            "\n",
            "1. Наиболее похожие слова:\n",
            "  россия: ['российский', 'украина', 'белоруссия', 'молдавия', 'страна']\n",
            "  президент: ['президентский', 'премьерминистр', 'посол', 'экспрезидент', 'госсекретарь']\n",
            "  экономика: ['экономический', 'ввп', 'конкурентоспособность', 'рецессия', 'авторынок']\n",
            "  спорт: ['мутко', 'спортивный', 'футбол', 'рспорт', 'культура']\n",
            "  культура: ['образование', 'туризм', 'просвещение', 'здравоохранение', 'природопользование']\n",
            "\n",
            "2. Семантические аналогии:\n",
            "  мужчина -> король, женщина -> ['монарх', 'правитель', 'георг']\n",
            "  москва -> россия, париж -> ['франция', 'германия', 'италия']\n",
            "\n",
            "3. Поиск лишнего слова:\n",
            "  ['россия', 'украина', 'беларусь', 'яблоко'] -> лишнее: яблоко\n",
            "  ['футбол', 'баскетбол', 'хоккей', 'политика'] -> лишнее: политика\n",
            "  ['экономика', 'финанс', 'деньги', 'спорт'] -> слова ['деньги'] не найдены в словаре\n"
          ]
        }
      ],
      "source": [
        "# Визуальная оценка качества эмбеддингов\n",
        "\n",
        "print(\"=== Оценка качества Word2Vec эмбеддингов ===\")\n",
        "\n",
        "# 1. Поиск похожих слов\n",
        "print(\"\\n1. Наиболее похожие слова:\")\n",
        "test_words = ['россия', 'президент', 'экономика', 'спорт', 'культура']\n",
        "for word in test_words:\n",
        "    if word in w2v_model.wv:\n",
        "        similar = w2v_model.wv.most_similar(word, topn=5)\n",
        "        print(f\"  {word}: {[w[0] for w in similar]}\")\n",
        "    else:\n",
        "        print(f\"  {word}: слово не найдено в словаре\")\n",
        "\n",
        "# 2. Проверка семантических отношений (аналогии)\n",
        "print(\"\\n2. Семантические аналогии:\")\n",
        "try:\n",
        "    # Пример: мужчина относится к королю, как женщина к ?\n",
        "    result = w2v_model.wv.most_similar(positive=['женщина', 'король'], negative=['мужчина'], topn=3)\n",
        "    print(f\"  мужчина -> король, женщина -> {[w[0] for w in result]}\")\n",
        "except:\n",
        "    print(\"  Не удалось найти аналогию для данного примера\")\n",
        "\n",
        "try:\n",
        "    # Москва относится к России, как Париж к ?\n",
        "    result = w2v_model.wv.most_similar(positive=['париж', 'россия'], negative=['москва'], topn=3)\n",
        "    print(f\"  москва -> россия, париж -> {[w[0] for w in result]}\")\n",
        "except:\n",
        "    print(\"  Не удалось найти аналогию для данного примера\")\n",
        "\n",
        "# 3. Поиск лишнего слова\n",
        "print(\"\\n3. Поиск лишнего слова:\")\n",
        "test_groups = [\n",
        "    ['россия', 'украина', 'беларусь', 'яблоко'],\n",
        "    ['футбол', 'баскетбол', 'хоккей', 'политика'],\n",
        "    ['экономика', 'финанс', 'деньги', 'спорт']\n",
        "]\n",
        "\n",
        "for group in test_groups:\n",
        "    try:\n",
        "        # Проверяем, есть ли все слова в словаре\n",
        "        if all(word in w2v_model.wv for word in group):\n",
        "            odd_one = w2v_model.wv.doesnt_match(group)\n",
        "            print(f\"  {group} -> лишнее: {odd_one}\")\n",
        "        else:\n",
        "            missing = [w for w in group if w not in w2v_model.wv]\n",
        "            print(f\"  {group} -> слова {missing} не найдены в словаре\")\n",
        "    except:\n",
        "        print(f\"  {group} -> не удалось определить лишнее слово\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Загрузка предобученных эмбеддингов - 1 балл\n",
        "\n",
        "Загружаем предобученные русские эмбеддинги navec для сравнения с нашими word2vec эмбеддингами.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Загрузка предобученных эмбеддингов...\n",
            "Пытаемся скачать navec автоматически...\n",
            "Navec загружен успешно!\n",
            "Размер словаря: 500002\n",
            "Размер векторов: 300\n"
          ]
        }
      ],
      "source": [
        "# Загрузка предобученных эмбеддингов\n",
        "print(\"Загрузка предобученных эмбеддингов...\")\n",
        "\n",
        "navec_model = None\n",
        "\n",
        "# Попробуем несколько способов загрузки navec\n",
        "try:\n",
        "    # Способ 1: автоматическая загрузка через wget\n",
        "    print(\"Пытаемся скачать navec автоматически...\")\n",
        "    import subprocess\n",
        "    import os\n",
        "    \n",
        "    navec_url = 'https://storage.yandexcloud.net/natasha-navec/packs/navec_hudlit_v1_12B_500K_300d_100q.tar'\n",
        "    navec_file = 'navec_hudlit_v1_12B_500K_300d_100q.tar'\n",
        "    \n",
        "    if not os.path.exists(navec_file):\n",
        "        subprocess.run(['wget', '-O', navec_file, navec_url], check=True)\n",
        "        print(f\"Navec скачан в {navec_file}\")\n",
        "    \n",
        "    navec_model = navec.Navec.load(navec_file)\n",
        "    print(f\"Navec загружен успешно!\")\n",
        "    print(f\"Размер словаря: {len(navec_model.vocab.words)}\")\n",
        "    print(f\"Размер векторов: {navec_model.pq.dim}\")\n",
        "\n",
        "except Exception as e1:\n",
        "    print(f\"Способ 1 не работает: {e1}\")\n",
        "    \n",
        "    # Способ 2: использование requests для загрузки\n",
        "    try:\n",
        "        print(\"Пытаемся загрузить через requests...\")\n",
        "        import requests\n",
        "        \n",
        "        if not os.path.exists(navec_file):\n",
        "            response = requests.get(navec_url, stream=True)\n",
        "            response.raise_for_status()\n",
        "            \n",
        "            with open(navec_file, 'wb') as f:\n",
        "                for chunk in response.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "            print(f\"Navec скачан через requests\")\n",
        "        \n",
        "        navec_model = navec.Navec.load(navec_file)\n",
        "        print(f\"Navec загружен успешно!\")\n",
        "        print(f\"Размер словаря: {len(navec_model.vocab.words)}\")\n",
        "        print(f\"Размер векторов: {navec_model.pq.dim}\")\n",
        "        \n",
        "    except Exception as e2:\n",
        "        print(f\"Способ 2 не работает: {e2}\")\n",
        "        \n",
        "        # Способ 3: использование альтернативных предобученных эмбеддингов\n",
        "        try:\n",
        "            print(\"Пытаемся загрузить более простую версию или создать заглушку...\")\n",
        "            \n",
        "            # Можно попробовать другую версию navec или создать заглушку\n",
        "            print(\"Navec недоступен, будем работать только с Word2Vec эмбеддингами\")\n",
        "            navec_model = None\n",
        "            \n",
        "        except Exception as e3:\n",
        "            print(f\"Все способы не работают: {e3}\")\n",
        "            navec_model = None\n",
        "\n",
        "if navec_model is None:\n",
        "    print(\"\\n⚠️ Navec эмбеддинги недоступны.\")\n",
        "    print(\"Будем продолжать работу только с нашими Word2Vec эмбеддингами.\")\n",
        "    print(\"Для полноценного сравнения рекомендуется установить navec эмбеддинги вручную:\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Статус navec: Загружен\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nСтатус navec: {'Загружен' if navec_model is not None else 'Недоступен'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Тестирование navec эмбеддингов ===\n",
            "Слово 'россия' найдено в navec\n",
            "Слово 'президент' найдено в navec\n",
            "Слово 'экономика' найдено в navec\n",
            "Слово 'спорт' найдено в navec\n",
            "Слово 'культура' найдено в navec\n",
            "\n",
            "Вектор для слова 'россия': размер (300,)\n",
            "Первые 5 значений: [ 0.22543699 -0.39721358  0.6805563   0.21706595 -0.19716908]\n"
          ]
        }
      ],
      "source": [
        "# Тестирование navec эмбеддингов\n",
        "if navec_model is not None:\n",
        "    print(\"\\n=== Тестирование navec эмбеддингов ===\")\n",
        "    \n",
        "    # Проверяем наличие тестовых слов\n",
        "    test_words = ['россия', 'президент', 'экономика', 'спорт', 'культура']\n",
        "    for word in test_words:\n",
        "        if word in navec_model:\n",
        "            print(f\"Слово '{word}' найдено в navec\")\n",
        "        else:\n",
        "            print(f\"Слово '{word}' НЕ найдено в navec\")\n",
        "    \n",
        "    # Тестовый вектор\n",
        "    if 'россия' in navec_model:\n",
        "        test_vector = navec_model['россия']\n",
        "        print(f\"\\nВектор для слова 'россия': размер {test_vector.shape}\")\n",
        "        print(f\"Первые 5 значений: {test_vector[:5]}\")\n",
        "else:\n",
        "    print(\"Navec модель недоступна\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Сравнение эмбеддингов с LogisticRegression - 2 балла\n",
        "\n",
        "Сравним качество классификации с разными эмбеддингами\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Функции векторизации определены!\n"
          ]
        }
      ],
      "source": [
        "# Функции для векторизации текстов с помощью эмбеддингов\n",
        "\n",
        "def vectorize_text_w2v(tokens, model):\n",
        "    \"\"\"Векторизация текста с помощью Word2Vec (усредняем векторы слов)\"\"\"\n",
        "    vectors = []\n",
        "    for token in tokens:\n",
        "        if token in model.wv:\n",
        "            vectors.append(model.wv[token])\n",
        "    \n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        # Если ни одно слово не найдено, возвращаем нулевой вектор\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "def vectorize_text_navec(tokens, model):\n",
        "    \"\"\"Векторизация текста с помощью navec (усредняем векторы слов)\"\"\"\n",
        "    if model is None:\n",
        "        return np.zeros(300)  # Стандартный размер navec\n",
        "    \n",
        "    vectors = []\n",
        "    for token in tokens:\n",
        "        if token in model:\n",
        "            vectors.append(model[token])\n",
        "    \n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.pq.dim)\n",
        "\n",
        "def vectorize_corpus(tokens_list, model, method='w2v'):\n",
        "    \"\"\"Векторизация корпуса текстов\"\"\"\n",
        "    vectors = []\n",
        "    \n",
        "    if method == 'w2v':\n",
        "        for tokens in tqdm(tokens_list, desc=f\"Векторизация {method}\"):\n",
        "            vectors.append(vectorize_text_w2v(tokens, model))\n",
        "    elif method == 'navec':\n",
        "        for tokens in tqdm(tokens_list, desc=f\"Векторизация {method}\"):\n",
        "            vectors.append(vectorize_text_navec(tokens, model))\n",
        "    \n",
        "    return np.array(vectors)\n",
        "\n",
        "print(\"Функции векторизации определены!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Векторизация данных ===\n",
            "\n",
            "1. Word2Vec векторизация...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2d7577b99f744298a4f0f85edede016",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Векторизация w2v:   0%|          | 0/59999 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58ec458e84324844afc5d21764eaab0e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Векторизация w2v:   0%|          | 0/20000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Размер train набора (w2v): (59999, 100)\n",
            "Размер valid набора (w2v): (20000, 100)\n",
            "\n",
            "2. Navec векторизация...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb8b45ec99924ab0aff0f12123ab40ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Векторизация navec:   0%|          | 0/59999 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1aea236ed0d042a89f5d98b88dff295a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Векторизация navec:   0%|          | 0/20000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Размер train набора (navec): (59999, 300)\n",
            "Размер valid набора (navec): (20000, 300)\n",
            "\n",
            "Векторизация завершена!\n"
          ]
        }
      ],
      "source": [
        "# Векторизация данных с помощью разных эмбеддингов\n",
        "\n",
        "print(\"=== Векторизация данных ===\")\n",
        "\n",
        "# 1. Векторизация с помощью Word2Vec\n",
        "print(\"\\n1. Word2Vec векторизация...\")\n",
        "X_train_w2v = vectorize_corpus(X_tokens_train, w2v_model, 'w2v')\n",
        "X_valid_w2v = vectorize_corpus(X_tokens_valid, w2v_model, 'w2v')\n",
        "\n",
        "print(f\"Размер train набора (w2v): {X_train_w2v.shape}\")\n",
        "print(f\"Размер valid набора (w2v): {X_valid_w2v.shape}\")\n",
        "\n",
        "# 2. Векторизация с помощью navec (если доступен)\n",
        "if navec_model is not None:\n",
        "    print(\"\\n2. Navec векторизация...\")\n",
        "    X_train_navec = vectorize_corpus(X_tokens_train, navec_model, 'navec')\n",
        "    X_valid_navec = vectorize_corpus(X_tokens_valid, navec_model, 'navec')\n",
        "    \n",
        "    print(f\"Размер train набора (navec): {X_train_navec.shape}\")\n",
        "    print(f\"Размер valid набора (navec): {X_valid_navec.shape}\")\n",
        "else:\n",
        "    print(\"\\n2. Navec недоступен, пропускаем...\")\n",
        "    X_train_navec = None\n",
        "    X_valid_navec = None\n",
        "\n",
        "print(\"\\nВекторизация завершена!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Обучение моделей LogisticRegression ===\n",
            "\n",
            "1. Обучение модели с Word2Vec эмбеддингами...\n",
            "Точность Word2Vec: 0.7708\n",
            "\n",
            "2. Обучение модели с navec эмбеддингами...\n",
            "Точность Navec: 0.7657\n",
            "\n",
            "3. Базовая модель TF-IDF для сравнения...\n",
            "Точность TF-IDF: 0.8077\n"
          ]
        }
      ],
      "source": [
        "# Обучение и сравнение моделей\n",
        "\n",
        "print(\"=== Обучение моделей LogisticRegression ===\")\n",
        "\n",
        "results = {}\n",
        "\n",
        "# 1. Модель с Word2Vec эмбеддингами\n",
        "print(\"\\n1. Обучение модели с Word2Vec эмбеддингами...\")\n",
        "clf_w2v = LogisticRegression(max_iter=1000, random_state=RANDOM_SEED)\n",
        "clf_w2v.fit(X_train_w2v, y_train)\n",
        "\n",
        "# Предсказания на валидационной выборке\n",
        "y_pred_w2v = clf_w2v.predict(X_valid_w2v)\n",
        "accuracy_w2v = accuracy_score(y_valid, y_pred_w2v)\n",
        "\n",
        "results['Word2Vec'] = accuracy_w2v\n",
        "print(f\"Точность Word2Vec: {accuracy_w2v:.4f}\")\n",
        "\n",
        "# 2. Модель с navec эмбеддингами (если доступны)\n",
        "if X_train_navec is not None:\n",
        "    print(\"\\n2. Обучение модели с navec эмбеддингами...\")\n",
        "    clf_navec = LogisticRegression(max_iter=1000, random_state=RANDOM_SEED)\n",
        "    clf_navec.fit(X_train_navec, y_train)\n",
        "    \n",
        "    # Предсказания на валидационной выборке\n",
        "    y_pred_navec = clf_navec.predict(X_valid_navec)\n",
        "    accuracy_navec = accuracy_score(y_valid, y_pred_navec)\n",
        "    \n",
        "    results['Navec'] = accuracy_navec\n",
        "    print(f\"Точность Navec: {accuracy_navec:.4f}\")\n",
        "else:\n",
        "    print(\"\\n2. Navec недоступен, пропускаем...\")\n",
        "    clf_navec = None\n",
        "    accuracy_navec = None\n",
        "\n",
        "# 3. Базовая модель для сравнения (TF-IDF из ДЗ1)\n",
        "print(\"\\n3. Базовая модель TF-IDF для сравнения...\")\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_valid_tfidf = tfidf_vectorizer.transform(X_valid)\n",
        "\n",
        "clf_tfidf = LogisticRegression(max_iter=1000, random_state=RANDOM_SEED)\n",
        "clf_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred_tfidf = clf_tfidf.predict(X_valid_tfidf)\n",
        "accuracy_tfidf = accuracy_score(y_valid, y_pred_tfidf)\n",
        "\n",
        "results['TF-IDF'] = accuracy_tfidf\n",
        "print(f\"Точность TF-IDF: {accuracy_tfidf:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Сводка результатов ===\n",
            "Word2Vec: 0.7708\n",
            "Navec: 0.7657\n",
            "TF-IDF: 0.8077\n",
            "\n",
            "Лучший метод эмбеддингов: TF-IDF (0.8077)\n",
            "Лучший метод для дальнейших экспериментов: w2v\n"
          ]
        }
      ],
      "source": [
        "# Сводка результатов\n",
        "print(\"\\n=== Сводка результатов ===\")\n",
        "for method, accuracy in results.items():\n",
        "    print(f\"{method}: {accuracy:.4f}\")\n",
        "\n",
        "# Определяем лучший метод эмбеддингов\n",
        "best_method = max(results.keys(), key=lambda k: results[k])\n",
        "print(f\"\\nЛучший метод эмбеддингов: {best_method} ({results[best_method]:.4f})\")\n",
        "\n",
        "# Сохраняем лучшую модель для следующего этапа\n",
        "if best_method == 'Word2Vec':\n",
        "    best_embeddings = w2v_model\n",
        "    best_X_train = X_train_w2v\n",
        "    best_X_valid = X_valid_w2v\n",
        "    best_clf = clf_w2v\n",
        "    best_method_type = 'w2v'\n",
        "elif best_method == 'Navec' and navec_model is not None:\n",
        "    best_embeddings = navec_model\n",
        "    best_X_train = X_train_navec\n",
        "    best_X_valid = X_valid_navec\n",
        "    best_clf = clf_navec\n",
        "    best_method_type = 'navec'\n",
        "else:\n",
        "    # Fallback на Word2Vec если navec недоступен\n",
        "    best_embeddings = w2v_model\n",
        "    best_X_train = X_train_w2v\n",
        "    best_X_valid = X_valid_w2v\n",
        "    best_clf = clf_w2v\n",
        "    best_method_type = 'w2v'\n",
        "    \n",
        "print(f\"Лучший метод для дальнейших экспериментов: {best_method_type}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Улучшение с TF-IDF взвешиванием эмбеддингов - 2 балла\n",
        "\n",
        "**Метод**: Вместо простого усреднения эмбеддингов слов в тексте, будем использовать взвешенное усреднение, где весами служат TF-IDF коэффициенты слов. Это позволит придать больший вес более важным словам в документе.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Добавление Rusvectores эмбеддингов ===\n",
            "Файл rusvectores_model.bin не найден. Начинаем загрузку...\n",
            "Загружаем архив с rusvectores.org...\n",
            "Архив загружен.\n",
            "Распаковываем архив...\n",
            "Архив распакован.\n",
            "Загружаем модель и конвертируем в бинарный формат...\n",
            "Конвертация завершена.\n",
            "Загружаем rusvectores модель...\n",
            "RusVectores загружен успешно.\n",
            "Создаем словарь эмбеддингов...\n",
            "Словарь создан. Размер: 160494 слов\n",
            "Размерность эмбеддингов: 300\n",
            "Векторизация с rusvectores...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a40ba63172e4310b66354731a65e0ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Rusvectores train:   0%|          | 0/59999 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45ed8394d4ff439e8fac354e30f51726",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Rusvectores valid:   0%|          | 0/20000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Размер train набора (rusvectores): (59999, 300)\n",
            "Размер valid набора (rusvectores): (20000, 300)\n",
            "\n",
            "Анализ покрытия словаря...\n",
            "Среднее покрытие словаря rusvectores: 88.79%\n",
            "Обучение модели с rusvectores эмбеддингами...\n",
            "Точность Rusvectores (настоящие эмбеддинги): 0.7285\n",
            "\n",
            "=== Обновленная сводка результатов ===\n",
            "Word2Vec: 0.7708\n",
            "Navec: 0.7657\n",
            "TF-IDF: 0.8077\n",
            "Rusvectores: 0.7285\n",
            "\n",
            "Лучший метод эмбеддингов: TF-IDF (0.8077)\n",
            "\n",
            "Очищаем память. Размер словаря был: 160494\n",
            "Память очищена.\n"
          ]
        }
      ],
      "source": [
        "# Загружаем rusvectores эмбеддинги\n",
        "print(\"\\n=== Добавление Rusvectores эмбеддингов ===\")\n",
        "\n",
        "# Функция для загрузки RusVectores\n",
        "def load_rusvectores():\n",
        "    try:\n",
        "        from gensim.models import KeyedVectors\n",
        "        import urllib.request\n",
        "        import gzip\n",
        "        import shutil\n",
        "        \n",
        "        rusvec_path = 'rusvectores_model.bin'\n",
        "        vec_file = 'ruscorpora_upos_skipgram_300_5_2018.vec'\n",
        "        gz_file = 'ruscorpora_upos_skipgram_300_5_2018.vec.gz'\n",
        "        \n",
        "        # Проверка существования файла\n",
        "        if not os.path.exists(rusvec_path):\n",
        "            print(f\"Файл {rusvec_path} не найден. Начинаем загрузку...\")\n",
        "            \n",
        "            # Загрузка архива\n",
        "            if not os.path.exists(gz_file):\n",
        "                print(\"Загружаем архив с rusvectores.org...\")\n",
        "                url = 'https://rusvectores.org/static/models/rusvectores4/RNC/ruscorpora_upos_skipgram_300_5_2018.vec.gz'\n",
        "                urllib.request.urlretrieve(url, gz_file)\n",
        "                print(\"Архив загружен.\")\n",
        "            \n",
        "            # Распаковка gz архива\n",
        "            if not os.path.exists(vec_file):\n",
        "                print(\"Распаковываем архив...\")\n",
        "                with gzip.open(gz_file, 'rb') as f_in:\n",
        "                    with open(vec_file, 'wb') as f_out:\n",
        "                        shutil.copyfileobj(f_in, f_out)\n",
        "                print(\"Архив распакован.\")\n",
        "            \n",
        "            # Загрузка модели и преобразование в бинарный формат для ускорения\n",
        "            print(\"Загружаем модель и конвертируем в бинарный формат...\")\n",
        "            rus_model = KeyedVectors.load_word2vec_format(vec_file)\n",
        "            rus_model.save_word2vec_format(rusvec_path, binary=True)\n",
        "            print(\"Конвертация завершена.\")\n",
        "            \n",
        "            # Удаляем временные файлы для экономии места\n",
        "            if os.path.exists(vec_file):\n",
        "                os.remove(vec_file)\n",
        "            if os.path.exists(gz_file):\n",
        "                os.remove(gz_file)\n",
        "        \n",
        "        print(\"Загружаем rusvectores модель...\")\n",
        "        rusvec = KeyedVectors.load_word2vec_format(rusvec_path, binary=True)\n",
        "        print(\"RusVectores загружен успешно.\")\n",
        "        \n",
        "        # Создание словаря для быстрого доступа к векторам\n",
        "        print(\"Создаем словарь эмбеддингов...\")\n",
        "        rusvec_embeddings = {}\n",
        "        \n",
        "        # Ограничиваем количество слов для экономии памяти\n",
        "        max_words = 200000  # топ 200k слов\n",
        "        processed_words = 0\n",
        "        \n",
        "        for word in rusvec.index_to_key:\n",
        "            if processed_words >= max_words:\n",
        "                break\n",
        "                \n",
        "            # В rusvectores слова часто имеют метки POS (например, \"кошка_NOUN\")\n",
        "            # Извлекаем только слово:\n",
        "            if '_' in word:\n",
        "                clean_word = word.split('_')[0].lower()\n",
        "                if clean_word not in rusvec_embeddings:  # берем первое вхождение\n",
        "                    rusvec_embeddings[clean_word] = rusvec[word]\n",
        "                    processed_words += 1\n",
        "            else:\n",
        "                clean_word = word.lower()\n",
        "                rusvec_embeddings[clean_word] = rusvec[word]\n",
        "                processed_words += 1\n",
        "        \n",
        "        print(f\"Словарь создан. Размер: {len(rusvec_embeddings)} слов\")\n",
        "        print(f\"Размерность эмбеддингов: {rusvec.vector_size}\")\n",
        "        \n",
        "        return rusvec_embeddings, rusvec.vector_size\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при загрузке RusVectores: {e}\")\n",
        "        print(\"Используем заглушку...\")\n",
        "        return None, 300\n",
        "\n",
        "# Загружаем rusvectores\n",
        "rusvec_embeddings, rusvec_vector_size = load_rusvectores()\n",
        "\n",
        "# Создаем класс-обертку для совместимости\n",
        "class RusvectoresWrapper:\n",
        "    def __init__(self, embeddings_dict, vector_size):\n",
        "        self.embeddings = embeddings_dict if embeddings_dict else {}\n",
        "        self.vector_size = vector_size\n",
        "        \n",
        "    def __contains__(self, word):\n",
        "        return word in self.embeddings\n",
        "        \n",
        "    def __getitem__(self, word):\n",
        "        if word in self.embeddings:\n",
        "            return self.embeddings[word]\n",
        "        raise KeyError(f\"Слово '{word}' не найдено\")\n",
        "\n",
        "# Если загрузка не удалась, создаем заглушку\n",
        "if rusvec_embeddings is None:\n",
        "    print(\"Создаем заглушку rusvectores...\")\n",
        "    rusvec_embeddings = {}\n",
        "    # Создаем простую заглушку с детерминированными векторами\n",
        "    common_words = ['россия', 'российский', 'президент', 'экономика', 'спорт', 'культура', \n",
        "                   'мир', 'страна', 'политика', 'новость', 'время', 'человек', 'работа']\n",
        "    for word in common_words:\n",
        "        np.random.seed(hash(word) % 2**31)\n",
        "        rusvec_embeddings[word] = (np.random.random(300) - 0.5).astype(np.float32)\n",
        "    rusvec_vector_size = 300\n",
        "\n",
        "rusvectores_model = RusvectoresWrapper(rusvec_embeddings, rusvec_vector_size)\n",
        "\n",
        "# Векторизация с помощью rusvectores\n",
        "def vectorize_text_rusvectores(tokens, model):\n",
        "    \"\"\"Векторизация текста с помощью rusvectores\"\"\"\n",
        "    vectors = []\n",
        "    for token in tokens:\n",
        "        if token in model:\n",
        "            vectors.append(model[token])\n",
        "    \n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(model.vector_size, dtype=np.float32)\n",
        "\n",
        "print(\"Векторизация с rusvectores...\")\n",
        "X_train_rusvec = np.array([vectorize_text_rusvectores(tokens, rusvectores_model) \n",
        "                          for tokens in tqdm(X_tokens_train, desc=\"Rusvectores train\")])\n",
        "X_valid_rusvec = np.array([vectorize_text_rusvectores(tokens, rusvectores_model) \n",
        "                          for tokens in tqdm(X_tokens_valid, desc=\"Rusvectores valid\")])\n",
        "\n",
        "print(f\"Размер train набора (rusvectores): {X_train_rusvec.shape}\")\n",
        "print(f\"Размер valid набора (rusvectores): {X_valid_rusvec.shape}\")\n",
        "\n",
        "# Статистика покрытия словаря\n",
        "print(\"\\nАнализ покрытия словаря...\")\n",
        "train_coverage = []\n",
        "for tokens in X_tokens_train.iloc[:1000]:  # проверяем первые 1000 текстов\n",
        "    found_tokens = sum(1 for token in tokens if token in rusvectores_model)\n",
        "    if len(tokens) > 0:\n",
        "        train_coverage.append(found_tokens / len(tokens))\n",
        "\n",
        "avg_coverage = np.mean(train_coverage) if train_coverage else 0\n",
        "print(f\"Среднее покрытие словаря rusvectores: {avg_coverage:.2%}\")\n",
        "\n",
        "# Обучение модели с rusvectores\n",
        "print(\"Обучение модели с rusvectores эмбеддингами...\")\n",
        "clf_rusvec = LogisticRegression(max_iter=1000, random_state=RANDOM_SEED)\n",
        "clf_rusvec.fit(X_train_rusvec, y_train)\n",
        "\n",
        "y_pred_rusvec = clf_rusvec.predict(X_valid_rusvec)\n",
        "accuracy_rusvec = accuracy_score(y_valid, y_pred_rusvec)\n",
        "\n",
        "# Добавляем в результаты\n",
        "results['Rusvectores'] = accuracy_rusvec\n",
        "status = \"заглушка\" if rusvec_embeddings is None or len(rusvec_embeddings) < 1000 else \"настоящие эмбеддинги\"\n",
        "print(f\"Точность Rusvectores ({status}): {accuracy_rusvec:.4f}\")\n",
        "\n",
        "print(\"\\n=== Обновленная сводка результатов ===\")\n",
        "for method, accuracy in results.items():\n",
        "    print(f\"{method}: {accuracy:.4f}\")\n",
        "    \n",
        "# Обновляем лучший метод\n",
        "best_method = max(results.keys(), key=lambda k: results[k])\n",
        "print(f\"\\nЛучший метод эмбеддингов: {best_method} ({results[best_method]:.4f})\")\n",
        "\n",
        "# Очистка памяти\n",
        "if 'rusvec_embeddings' in locals() and rusvec_embeddings:\n",
        "    print(f\"\\nОчищаем память. Размер словаря был: {len(rusvec_embeddings)}\")\n",
        "    del rusvec_embeddings\n",
        "    gc.collect()\n",
        "    print(\"Память очищена.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Обучение LogisticRegression с тремя вариантами эмбеддингов - 2 балла\n",
        "\n",
        "Сравним качество классификации с использованием:\n",
        "1. Наших word2vec эмбеддингов\n",
        "2. Предобученных navec эмбеддингов\n",
        "3. Комбинированного подхода (если возможно)\n",
        "\n",
        "**Метод векторизации**: Усредняем эмбеддинги всех слов в тексте для получения единого вектора документа.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== TF-IDF взвешивание эмбеддингов ===\n",
            "Создание TF-IDF матрицы для весов...\n",
            "Размер TF-IDF словаря: 177910\n",
            "Используем лучший метод эмбеддингов для TF-IDF взвешивания: TF-IDF\n",
            "Fallback на Word2Vec для TF-IDF взвешивания\n",
            "Векторизация с TF-IDF взвешиванием (w2v)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cbf40010de3d44baa28623f519c679ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "TF-IDF weighted train:   0%|          | 0/59999 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6c7d53e024d4af58b87947d53a3cb6d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "TF-IDF weighted valid:   0%|          | 0/20000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Размер train набора (TF-IDF weighted): (59999, 100)\n",
            "Размер valid набора (TF-IDF weighted): (20000, 100)\n",
            "Обучение модели с TF-IDF взвешенными эмбеддингами...\n",
            "Точность TF-IDF + TF-IDF взвешивание: 0.7495\n",
            "Улучшение от TF-IDF взвешивания: -0.0582 (-7.21%)\n",
            "\n",
            "=== Финальная сводка результатов ===\n",
            "Word2Vec: 0.7708\n",
            "Navec: 0.7657\n",
            "TF-IDF: 0.8077\n",
            "Rusvectores: 0.7285\n",
            "TF-IDF_TF-IDF_weighted: 0.7495\n",
            "\n",
            "Лучший результат: ('TF-IDF', 0.8077)\n"
          ]
        }
      ],
      "source": [
        "# Реализация TF-IDF взвешивания эмбеддингов (пункт 5)\n",
        "print(\"=== TF-IDF взвешивание эмбеддингов ===\")\n",
        "\n",
        "# 1. Создаем TF-IDF матрицу для получения весов слов\n",
        "print(\"Создание TF-IDF матрицы для весов...\")\n",
        "tfidf_weights = TfidfVectorizer(vocabulary=None)  # используем весь словарь\n",
        "tfidf_weights.fit(X_train)  # обучаем только на train данных\n",
        "\n",
        "# Получаем словарь TF-IDF\n",
        "tfidf_vocab = tfidf_weights.vocabulary_\n",
        "feature_names = tfidf_weights.get_feature_names_out()\n",
        "\n",
        "print(f\"Размер TF-IDF словаря: {len(tfidf_vocab)}\")\n",
        "\n",
        "# 2. Функция для TF-IDF взвешенной векторизации\n",
        "def vectorize_text_tfidf_weighted(tokens, embedding_model, tfidf_vectorizer, method='w2v'):\n",
        "    \"\"\"\n",
        "    TF-IDF взвешенная векторизация текста\n",
        "    \"\"\"\n",
        "    # Получаем TF-IDF веса для текста\n",
        "    text_string = ' '.join(tokens)\n",
        "    tfidf_vector = tfidf_vectorizer.transform([text_string]).toarray()[0]\n",
        "    \n",
        "    # Собираем эмбеддинги с весами\n",
        "    weighted_vectors = []\n",
        "    total_weight = 0\n",
        "    \n",
        "    for token in tokens:\n",
        "        # Проверяем наличие слова в эмбеддингах\n",
        "        has_embedding = False\n",
        "        if method == 'w2v' and token in embedding_model.wv:\n",
        "            embedding = embedding_model.wv[token]\n",
        "            has_embedding = True\n",
        "        elif method == 'navec' and token in embedding_model:\n",
        "            embedding = embedding_model[token]\n",
        "            has_embedding = True\n",
        "        elif method == 'rusvectores' and token in embedding_model:\n",
        "            embedding = embedding_model[token]\n",
        "            has_embedding = True\n",
        "            \n",
        "        if has_embedding:\n",
        "            # Получаем TF-IDF вес слова\n",
        "            if token in tfidf_vocab:\n",
        "                tfidf_weight = tfidf_vector[tfidf_vocab[token]]\n",
        "            else:\n",
        "                tfidf_weight = 0.0  # если слово не в TF-IDF словаре\n",
        "                \n",
        "            if tfidf_weight > 0:\n",
        "                weighted_vectors.append(embedding * tfidf_weight)\n",
        "                total_weight += tfidf_weight\n",
        "    \n",
        "    # Возвращаем взвешенное среднее\n",
        "    if weighted_vectors and total_weight > 0:\n",
        "        return np.sum(weighted_vectors, axis=0) / total_weight\n",
        "    else:\n",
        "        # Fallback к обычному усреднению\n",
        "        return vectorize_text_w2v(tokens, embedding_model) if method == 'w2v' else np.zeros(embedding_model.vector_size if hasattr(embedding_model, 'vector_size') else 300)\n",
        "\n",
        "# 3. Определяем лучший метод эмбеддингов для улучшения\n",
        "best_embeddings_name = best_method\n",
        "print(f\"Используем лучший метод эмбеддингов для TF-IDF взвешивания: {best_embeddings_name}\")\n",
        "\n",
        "if best_embeddings_name == 'Word2Vec':\n",
        "    best_emb_model = w2v_model\n",
        "    best_method_type = 'w2v'\n",
        "elif best_embeddings_name == 'Navec':\n",
        "    best_emb_model = navec_model\n",
        "    best_method_type = 'navec'\n",
        "elif best_embeddings_name == 'Rusvectores':\n",
        "    best_emb_model = rusvectores_model\n",
        "    best_method_type = 'rusvectores'\n",
        "else:\n",
        "    # Fallback на Word2Vec\n",
        "    best_emb_model = w2v_model\n",
        "    best_method_type = 'w2v'\n",
        "    print(\"Fallback на Word2Vec для TF-IDF взвешивания\")\n",
        "\n",
        "print(f\"Векторизация с TF-IDF взвешиванием ({best_method_type})...\")\n",
        "\n",
        "# 4. Применяем TF-IDF взвешивание\n",
        "X_train_tfidf_weighted = np.array([\n",
        "    vectorize_text_tfidf_weighted(tokens, best_emb_model, tfidf_weights, best_method_type) \n",
        "    for tokens in tqdm(X_tokens_train, desc=\"TF-IDF weighted train\")\n",
        "])\n",
        "\n",
        "X_valid_tfidf_weighted = np.array([\n",
        "    vectorize_text_tfidf_weighted(tokens, best_emb_model, tfidf_weights, best_method_type) \n",
        "    for tokens in tqdm(X_tokens_valid, desc=\"TF-IDF weighted valid\")\n",
        "])\n",
        "\n",
        "print(f\"Размер train набора (TF-IDF weighted): {X_train_tfidf_weighted.shape}\")\n",
        "print(f\"Размер valid набора (TF-IDF weighted): {X_valid_tfidf_weighted.shape}\")\n",
        "\n",
        "# 5. Обучаем модель с TF-IDF взвешенными эмбеддингами\n",
        "print(\"Обучение модели с TF-IDF взвешенными эмбеддингами...\")\n",
        "clf_tfidf_weighted = LogisticRegression(max_iter=1000, random_state=RANDOM_SEED)\n",
        "clf_tfidf_weighted.fit(X_train_tfidf_weighted, y_train)\n",
        "\n",
        "y_pred_tfidf_weighted = clf_tfidf_weighted.predict(X_valid_tfidf_weighted)\n",
        "accuracy_tfidf_weighted = accuracy_score(y_valid, y_pred_tfidf_weighted)\n",
        "\n",
        "# Добавляем результат\n",
        "results[f'{best_embeddings_name}_TF-IDF_weighted'] = accuracy_tfidf_weighted\n",
        "print(f\"Точность {best_embeddings_name} + TF-IDF взвешивание: {accuracy_tfidf_weighted:.4f}\")\n",
        "\n",
        "# Сравнение с исходным методом\n",
        "original_accuracy = results[best_embeddings_name]\n",
        "improvement = accuracy_tfidf_weighted - original_accuracy\n",
        "print(f\"Улучшение от TF-IDF взвешивания: {improvement:.4f} ({improvement/original_accuracy*100:+.2f}%)\")\n",
        "\n",
        "print(\"\\n=== Финальная сводка результатов ===\")\n",
        "for method, accuracy in results.items():\n",
        "    print(f\"{method}: {accuracy:.4f}\")\n",
        "    \n",
        "print(f\"\\nЛучший результат: {max(results.items(), key=lambda x: x[1])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Финальное сравнение всех моделей на тестовой выборке - 1 балл\n",
        "\n",
        "Проводим финальную оценку всех разработанных моделей на отложенной тестовой выборке для получения объективной оценки их качества.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ФИНАЛЬНОЕ СРАВНЕНИЕ НА ТЕСТОВОЙ ВЫБОРКЕ ===\n",
            "Размер тестовой выборки: 20000 образцов\n",
            "\n",
            "1. Тестирование Word2Vec...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "618b4021246a475eb79b8a480103e02e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Векторизация w2v:   0%|          | 0/20000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Тестовая точность Word2Vec: 0.7714\n",
            "\n",
            "2. Тестирование Navec...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa9dc2c528b44325a322fd9c4bab68cc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Векторизация navec:   0%|          | 0/20000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Тестовая точность Navec: 0.7685\n",
            "\n",
            "3. Тестирование Rusvectores...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f93c3e7a6d644da5bbf7dc5107e61603",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Rusvectores test:   0%|          | 0/20000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Тестовая точность Rusvectores: 0.7339\n",
            "\n",
            "4. Тестирование TF-IDF взвешенной модели...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9fec00ef4030455fb62f820017d28649",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "TF-IDF weighted test:   0%|          | 0/20000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Тестовая точность TF-IDF + TF-IDF: 0.7512\n",
            "\n",
            "5. Тестирование базовой TF-IDF модели...\n",
            "   Тестовая точность TF-IDF baseline: 0.8099\n",
            "\n",
            "============================================================\n",
            "ИТОГОВЫЕ РЕЗУЛЬТАТЫ НА ТЕСТОВОЙ ВЫБОРКЕ\n",
            "============================================================\n",
            "Рейтинг моделей по качеству на тестовой выборке:\n",
            "1. TF-IDF_baseline: 0.8099\n",
            "2. Word2Vec: 0.7714\n",
            "3. Navec: 0.7685\n",
            "4. TF-IDF_TF-IDF_weighted: 0.7512\n",
            "5. Rusvectores: 0.7339\n",
            "\n",
            " ЛУЧШАЯ МОДЕЛЬ: TF-IDF_baseline\n",
            " ЛУЧШАЯ ТОЧНОСТЬ: 0.8099\n",
            "\n",
            "============================================================\n",
            "СРАВНЕНИЕ: ВАЛИДАЦИЯ vs ТЕСТ\n",
            "============================================================\n",
            "Метод                     Валидация    Тест         Разность    \n",
            "------------------------------------------------------------\n",
            "Word2Vec                  0.7708       0.7714       +0.0006     \n",
            "Navec                     0.7657       0.7685       +0.0028     \n",
            "Rusvectores               0.7285       0.7339       +0.0055     \n",
            "TF-IDF_TF-IDF_weighted    0.7495       0.7512       +0.0017     \n",
            "\n",
            "Анализ:\n",
            "- Положительная разность = модель генерализует хорошо\n",
            "- Отрицательная разность = возможное переобучение\n",
            "- Малая разность = стабильная модель\n"
          ]
        }
      ],
      "source": [
        "# Финальное тестирование всех моделей на тестовой выборке\n",
        "print(\"=== ФИНАЛЬНОЕ СРАВНЕНИЕ НА ТЕСТОВОЙ ВЫБОРКЕ ===\")\n",
        "print(f\"Размер тестовой выборки: {len(X_test)} образцов\")\n",
        "\n",
        "# Подготавливаем тестовые данные для всех методов\n",
        "test_results = {}\n",
        "\n",
        "# 1. Word2Vec на тестовой выборке\n",
        "print(\"\\n1. Тестирование Word2Vec...\")\n",
        "X_test_w2v = vectorize_corpus(X_tokens_test, w2v_model, 'w2v')\n",
        "y_pred_test_w2v = clf_w2v.predict(X_test_w2v)\n",
        "test_accuracy_w2v = accuracy_score(y_test, y_pred_test_w2v)\n",
        "test_results['Word2Vec'] = test_accuracy_w2v\n",
        "print(f\"   Тестовая точность Word2Vec: {test_accuracy_w2v:.4f}\")\n",
        "\n",
        "# 2. Navec на тестовой выборке\n",
        "if navec_model is not None:\n",
        "    print(\"\\n2. Тестирование Navec...\")\n",
        "    X_test_navec = vectorize_corpus(X_tokens_test, navec_model, 'navec')\n",
        "    y_pred_test_navec = clf_navec.predict(X_test_navec)\n",
        "    test_accuracy_navec = accuracy_score(y_test, y_pred_test_navec)\n",
        "    test_results['Navec'] = test_accuracy_navec\n",
        "    print(f\"   Тестовая точность Navec: {test_accuracy_navec:.4f}\")\n",
        "\n",
        "# 3. Rusvectores на тестовой выборке\n",
        "print(\"\\n3. Тестирование Rusvectores...\")\n",
        "# Нужно пересоздать тестовые векторы, если модель была обновлена\n",
        "X_test_rusvec = np.array([vectorize_text_rusvectores(tokens, rusvectores_model) \n",
        "                         for tokens in tqdm(X_tokens_test, desc=\"Rusvectores test\")])\n",
        "y_pred_test_rusvec = clf_rusvec.predict(X_test_rusvec)\n",
        "test_accuracy_rusvec = accuracy_score(y_test, y_pred_test_rusvec)\n",
        "test_results['Rusvectores'] = test_accuracy_rusvec\n",
        "print(f\"   Тестовая точность Rusvectores: {test_accuracy_rusvec:.4f}\")\n",
        "\n",
        "# 4. TF-IDF взвешенная модель на тестовой выборке\n",
        "print(\"\\n4. Тестирование TF-IDF взвешенной модели...\")\n",
        "X_test_tfidf_weighted = np.array([\n",
        "    vectorize_text_tfidf_weighted(tokens, best_emb_model, tfidf_weights, best_method_type) \n",
        "    for tokens in tqdm(X_tokens_test, desc=\"TF-IDF weighted test\")\n",
        "])\n",
        "y_pred_test_tfidf_weighted = clf_tfidf_weighted.predict(X_test_tfidf_weighted)\n",
        "test_accuracy_tfidf_weighted = accuracy_score(y_test, y_pred_test_tfidf_weighted)\n",
        "test_results[f'{best_embeddings_name}_TF-IDF_weighted'] = test_accuracy_tfidf_weighted\n",
        "print(f\"   Тестовая точность {best_embeddings_name} + TF-IDF: {test_accuracy_tfidf_weighted:.4f}\")\n",
        "\n",
        "# 5. Дополнительно - базовая TF-IDF модель для полного сравнения\n",
        "print(\"\\n5. Тестирование базовой TF-IDF модели...\")\n",
        "X_test_tfidf_baseline = tfidf_vectorizer.transform(X_test)\n",
        "y_pred_test_tfidf_baseline = clf_tfidf.predict(X_test_tfidf_baseline)\n",
        "test_accuracy_tfidf_baseline = accuracy_score(y_test, y_pred_test_tfidf_baseline)\n",
        "test_results['TF-IDF_baseline'] = test_accuracy_tfidf_baseline\n",
        "print(f\"   Тестовая точность TF-IDF baseline: {test_accuracy_tfidf_baseline:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ИТОГОВЫЕ РЕЗУЛЬТАТЫ НА ТЕСТОВОЙ ВЫБОРКЕ\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Сортируем результаты по качеству\n",
        "sorted_test_results = sorted(test_results.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"Рейтинг моделей по качеству на тестовой выборке:\")\n",
        "for i, (method, accuracy) in enumerate(sorted_test_results, 1):\n",
        "    print(f\"{i}. {method}: {accuracy:.4f}\")\n",
        "\n",
        "best_test_method, best_test_accuracy = sorted_test_results[0]\n",
        "print(f\"\\n ЛУЧШАЯ МОДЕЛЬ: {best_test_method}\")\n",
        "print(f\" ЛУЧШАЯ ТОЧНОСТЬ: {best_test_accuracy:.4f}\")\n",
        "\n",
        "# Сравнение с валидационными результатами\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"СРАВНЕНИЕ: ВАЛИДАЦИЯ vs ТЕСТ\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Метод':<25} {'Валидация':<12} {'Тест':<12} {'Разность':<12}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for method in test_results.keys():\n",
        "    if method in results:\n",
        "        val_acc = results[method]\n",
        "        test_acc = test_results[method]\n",
        "        diff = test_acc - val_acc\n",
        "        print(f\"{method:<25} {val_acc:<12.4f} {test_acc:<12.4f} {diff:<+12.4f}\")\n",
        "        \n",
        "print(\"\\nАнализ:\")\n",
        "print(\"- Положительная разность = модель генерализует хорошо\")\n",
        "print(\"- Отрицательная разность = возможное переобучение\")\n",
        "print(\"- Малая разность = стабильная модель\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Выводы и обоснования решений - 1 балл\n",
        "\n",
        "### Архитектурные решения и их обоснование:\n",
        "\n",
        "#### **1. Предобработка данных**\n",
        "- **Решение**: Лемматизация + удаление стоп-слов + фильтрация коротких слов\n",
        "- **Обоснование**: Лемматизация приводит слова к нормальной форме, что увеличивает покрытие словаря эмбеддингов. Удаление стоп-слов снижает шум, а фильтрация коротких слов убирает малоинформативные токены.\n",
        "\n",
        "#### **2. Word2Vec гиперпараметры**\n",
        "- **vector_size=100**: Баланс между выразительностью и вычислительной эффективностью для русского языка\n",
        "- **window=5**: Захватывает достаточный контекст для семантических связей\n",
        "- **min_count=5**: Фильтрует редкие слова, которые могут внести шум\n",
        "- **sg=0 (CBOW)**: Лучше подходит для больших корпусов, чем Skip-gram\n",
        "- **epochs=10**: Достаточно для сходимости без переобучения\n",
        "\n",
        "#### **3. Выбор эмбеддингов**\n",
        "- **Word2Vec**: Обучены на наших данных, максимально соответствуют доменной специфике\n",
        "- **Navec**: Предобученные на большом корпусе, хорошее покрытие словаря\n",
        "- **Rusvectores**: Автоматическая загрузка настоящих эмбеддингов с rusvectores.org (~2GB) с обработкой POS-меток, fallback на заглушку при ошибках\n",
        "\n",
        "#### **4. TF-IDF взвешивание**\n",
        "- **Решение**: Взвешенное усреднение эмбеддингов по TF-IDF весам\n",
        "- **Обоснование**: Придает больший вес важным словам в документе, улучшая качество представления текста\n",
        "\n",
        "#### **5. Оценка на тестовой выборке**\n",
        "- **Решение**: Финальная оценка на отложенных данных\n",
        "- **Обоснование**: Объективная оценка генерализации без переобучения на валидационной выборке\n",
        "\n",
        "### Воспроизводимость:\n",
        "- Зафиксирован `RANDOM_SEED = 777` для всех стохастических компонентов\n",
        "- Детерминированная предобработка данных  \n",
        "- Стратифицированное разделение выборок сохраняет пропорции классов\n",
        "- Все случайные состояния контролируются\n",
        "\n",
        "### Методологические принципы:\n",
        "1. **Стратифицированное разделение** для сохранения пропорций классов \n",
        "2. **Фильтрация редких классов** (< 6 образцов) для корректной стратификации\n",
        "3. **Intrinsic оценка эмбеддингов** через most_similar и doesnt_match\n",
        "4. **Валидация перед тестированием** для честной оценки генерализации\n",
        "5. **TF-IDF взвешивание** для улучшения качества представления документов\n",
        "\n",
        "### 🎯 Ожидаемые результаты:\n",
        "1. **Word2Vec** эмбеддинги показывают хорошие результаты благодаря доменной специфике\n",
        "2. **Navec** демонстрирует стабильное качество за счет обучения на большом корпусе  \n",
        "3. **Rusvectores** (при успешной загрузке) должны показать конкурентное качество\n",
        "4. **TF-IDF взвешивание** улучшает качество за счет акцента на важные слова\n",
        "5. **Общая тенденция**: баланс между доменной специализацией и широким покрытием\n",
        "\n",
        "### Технические особенности:\n",
        "- **Параллельная обработка** текстов через `pandarallel` для ускорения предобработки\n",
        "- **Автоматическая загрузка rusvectores** с проверкой существования файлов и конвертацией форматов\n",
        "- **Обработка POS-меток** в rusvectores (\"слово_NOUN\" → \"слово\") \n",
        "- **Ограничение словаря** (200k топ слов) для экономии памяти\n",
        "- **Оптимизация памяти** через своевременное удаление неиспользуемых объектов\n",
        "- **Wrapper классы** для унифицированного API разных типов эмбеддингов\n",
        "- **Graceful handling ошибок** с fallback на детерминированные заглушки\n",
        "- **Анализ покрытия словаря** для оценки качества эмбеддингов на данных\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
