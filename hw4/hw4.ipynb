{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec676557",
   "metadata": {},
   "source": [
    "## Домашнее задание 4 — NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db2b0d4",
   "metadata": {},
   "source": [
    "### 1. Установка и импорт библиотек"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aae496",
   "metadata": {},
   "source": [
    "### СЕКЦИЯ 1: ПЕРВОЕ NER ОБУЧЕНИЕ\n",
    "\n",
    "**Выполните все ячейки этой секции до ячейки с рестартом ядра**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a7a7910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers (from -r requirements.txt (line 1))\n",
      "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting datasets (from -r requirements.txt (line 2))\n",
      "  Downloading datasets-4.1.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: torch in /Users/bitcoin/ITMO/ITMO-DL-NLP/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (2.8.0)\n",
      "Requirement already satisfied: pandas in /Users/bitcoin/ITMO/ITMO-DL-NLP/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (2.3.2)\n",
      "Requirement already satisfied: scikit-learn in /Users/bitcoin/ITMO/ITMO-DL-NLP/.venv/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (1.6.1)\n",
      "Collecting seqeval (from -r requirements.txt (line 6))\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25lPython(42874) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "done\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/bitcoin/ITMO/ITMO-DL-NLP/.venv/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 1)) (3.19.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers->-r requirements.txt (line 1))\n",
      "  Downloading huggingface_hub-0.35.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/bitcoin/ITMO/ITMO-DL-NLP/.venv/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/bitcoin/ITMO/ITMO-DL-NLP/.venv/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 1)) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers->-r requirements.txt (line 1))\n",
      "  Downloading PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/bitcoin/ITMO/ITMO-DL-NLP/.venv/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 1)) (2025.9.18)\n",
      "Collecting requests (from transformers->-r requirements.txt (line 1))\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers->-r requirements.txt (line 1))\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers->-r requirements.txt (line 1))\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/bitcoin/ITMO/ITMO-DL-NLP/.venv/lib/python3.9/site-packages (from transformers->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/bitcoin/ITMO/ITMO-DL-NLP/.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers->-r requirements.txt (line 1)) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/bitcoin/ITMO/ITMO-DL-NLP/.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers->-r requirements.txt (line 1)) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers->-r requirements.txt (line 1))\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.7 kB)\n",
      "Collecting pyarrow>=21.0.0 (from datasets->-r requirements.txt (line 2))\n",
      "  Downloading pyarrow-21.0.0-cp39-cp39-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/bitcoin/ITMO/ITMO-DL-NLP/.venv/lib/python3.9/site-packages (from datasets->-r requirements.txt (line 2)) (0.4.0)\n",
      "Collecting xxhash (from datasets->-r requirements.txt (line 2))\n",
      "  Downloading xxhash-3.5.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets->-r requirements.txt (line 2))\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements.txt (line 2))\n",
      "  Downloading aiohttp-3.12.15-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/bitcoin/ITMO/ITMO-DL-NLP/.venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 3)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/bitcoin/ITMO/ITMO-DL-NLP/.venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 3)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/bitcoin/ITMO/ITMO-DL-NLP/.venv/lib/python3.9/site-packages (from torch->-r requirements.txt (line 3)) (3.1.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/bitcoin/ITMO/ITMO-DL-NLP/.venv/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/bitcoin/ITMO/ITMO-DL-NLP/.venv/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/bitcoin/ITMO/ITMO-DL-NLP/.venv/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/bitcoin/ITMO/ITMO-DL-NLP/.venv/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/bitcoin/ITMO/ITMO-DL-NLP/.venv/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/bitcoin/ITMO/ITMO-DL-NLP/.venv/lib/python3.9/site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.6.0)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements.txt (line 2))\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements.txt (line 2))\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements.txt (line 2))\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements.txt (line 2))\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements.txt (line 2))\n",
      "  Downloading frozenlist-1.7.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements.txt (line 2))\n",
      "  Downloading multidict-6.6.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements.txt (line 2))\n",
      "  Downloading propcache-0.3.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements.txt (line 2))\n",
      "  Downloading yarl-1.20.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (73 kB)\n",
      "Collecting idna>=2.0 (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->-r requirements.txt (line 2))\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/bitcoin/ITMO/ITMO-DL-NLP/.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 4)) (1.17.0)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers->-r requirements.txt (line 1))\n",
      "  Downloading charset_normalizer-3.4.3-cp39-cp39-macosx_10_9_universal2.whl.metadata (36 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers->-r requirements.txt (line 1))\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers->-r requirements.txt (line 1))\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/bitcoin/ITMO/ITMO-DL-NLP/.venv/lib/python3.9/site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/bitcoin/ITMO/ITMO-DL-NLP/.venv/lib/python3.9/site-packages (from jinja2->torch->-r requirements.txt (line 3)) (3.0.2)\n",
      "Downloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.35.1-py3-none-any.whl (563 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.3/563.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.1.1-py3-none-any.whl (503 kB)\n",
      "Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "Downloading aiohttp-3.12.15-cp39-cp39-macosx_11_0_arm64.whl (469 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading multidict-6.6.4-cp39-cp39-macosx_11_0_arm64.whl (44 kB)\n",
      "Downloading yarl-1.20.1-cp39-cp39-macosx_11_0_arm64.whl (89 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.7.0-cp39-cp39-macosx_11_0_arm64.whl (47 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading propcache-0.3.2-cp39-cp39-macosx_11_0_arm64.whl (43 kB)\n",
      "Downloading pyarrow-21.0.0-cp39-cp39-macosx_12_0_arm64.whl (31.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.2/31.2 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl (172 kB)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.3-cp39-cp39-macosx_10_9_universal2.whl (207 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-macosx_11_0_arm64.whl (432 kB)\n",
      "Downloading xxhash-3.5.0-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (pyproject.toml) ... \u001b[?25lPython(42897) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "done\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16250 sha256=3d5fa5dc4265793cfdeb5c75e0837042f145919b7691dd7f6173c7afc6c945a1\n",
      "  Stored in directory: /Users/bitcoin/Library/Caches/pip/wheels/e2/a5/92/2c80d1928733611c2747a9820e1324a6835524d9411510c142\n",
      "Successfully built seqeval\n",
      "Installing collected packages: xxhash, urllib3, safetensors, pyyaml, pyarrow, propcache, multiprocess, multidict, idna, hf-xet, frozenlist, charset_normalizer, certifi, attrs, async-timeout, aiohappyeyeballs, yarl, requests, aiosignal, seqeval, huggingface-hub, aiohttp, tokenizers, transformers, datasets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/25\u001b[0m [datasets]/25\u001b[0m [datasets]ers]ub]er]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 async-timeout-5.0.1 attrs-25.3.0 certifi-2025.8.3 charset_normalizer-3.4.3 datasets-4.1.1 frozenlist-1.7.0 hf-xet-1.1.10 huggingface-hub-0.35.1 idna-3.10 multidict-6.6.4 multiprocess-0.70.16 propcache-0.3.2 pyarrow-21.0.0 pyyaml-6.0.2 requests-2.32.5 safetensors-0.6.2 seqeval-1.2.2 tokenizers-0.22.1 transformers-4.56.2 urllib3-2.5.0 xxhash-3.5.0 yarl-1.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Установка необходимых библиотек\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ba5febc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0eca13",
   "metadata": {},
   "source": [
    "#### Импорт необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b86f8d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bitcoin/ITMO-DL-NLP/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM SEED WAS SET TO: 777\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "from seqeval.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from corus import load_ne5\n",
    "import corus.sources.ne5 as ne5\n",
    "import razdel\n",
    "from razdel.substring import Substring\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# Установка random seed для воспроизводимости результатов\n",
    "RANDOM_SEED = 777\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"RANDOM SEED WAS SET TO:\", RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa0b651",
   "metadata": {},
   "source": [
    "### 2. Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bd01455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-09-26 12:00:22--  http://www.labinform.ru/pub/named_entities/collection5.zip\n",
      "Resolving www.labinform.ru (www.labinform.ru)... 64:ff9b::5fb5:e6b5\n",
      "Connecting to www.labinform.ru (www.labinform.ru)|64:ff9b::5fb5:e6b5|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1899530 (1.8M) [application/zip]\n",
      "Saving to: ‘collection5.zip’\n",
      "\n",
      "collection5.zip     100%[===================>]   1.81M  2.29MB/s    in 0.8s    \n",
      "\n",
      "2025-09-26 12:00:23 (2.29 MB/s) - ‘collection5.zip’ saved [1899530/1899530]\n",
      "\n",
      "Archive:  collection5.zip\n",
      "   creating: Collection5/\n",
      "  inflating: Collection5/001.ann     \n",
      "  inflating: Collection5/001.txt     \n",
      "  inflating: Collection5/002.ann     \n",
      "  inflating: Collection5/002.txt     \n",
      "  inflating: Collection5/003.ann     \n",
      "  inflating: Collection5/003.txt     \n",
      "  inflating: Collection5/004.ann     \n",
      "  inflating: Collection5/004.txt     \n",
      "  inflating: Collection5/005.ann     \n",
      "  inflating: Collection5/005.txt     \n",
      "  inflating: Collection5/006.ann     \n",
      "  inflating: Collection5/006.txt     \n",
      "  inflating: Collection5/007.ann     \n",
      "  inflating: Collection5/007.txt     \n",
      "  inflating: Collection5/008.ann     \n",
      "  inflating: Collection5/008.txt     \n",
      "  inflating: Collection5/009.ann     \n",
      "  inflating: Collection5/009.txt     \n",
      "  inflating: Collection5/010.ann     \n",
      "  inflating: Collection5/010.txt     \n",
      "  inflating: Collection5/011.ann     \n",
      "  inflating: Collection5/011.txt     \n",
      "  inflating: Collection5/012.ann     \n",
      "  inflating: Collection5/012.txt     \n",
      "  inflating: Collection5/013.ann     \n",
      "  inflating: Collection5/013.txt     \n",
      "  inflating: Collection5/014.ann     \n",
      "  inflating: Collection5/014.txt     \n",
      "  inflating: Collection5/015 (!).ann  \n",
      "  inflating: Collection5/015 (!).txt  \n",
      "  inflating: Collection5/016.ann     \n",
      "  inflating: Collection5/016.txt     \n",
      "  inflating: Collection5/017.ann     \n",
      "  inflating: Collection5/017.txt     \n",
      "  inflating: Collection5/018.ann     \n",
      "  inflating: Collection5/018.txt     \n",
      "  inflating: Collection5/019.ann     \n",
      "  inflating: Collection5/019.txt     \n",
      "  inflating: Collection5/020.ann     \n",
      "  inflating: Collection5/020.txt     \n",
      "  inflating: Collection5/021.ann     \n",
      "  inflating: Collection5/021.txt     \n",
      "  inflating: Collection5/022.ann     \n",
      "  inflating: Collection5/022.txt     \n",
      "  inflating: Collection5/023.ann     \n",
      "  inflating: Collection5/023.txt     \n",
      "  inflating: Collection5/025.ann     \n",
      "  inflating: Collection5/025.txt     \n",
      "  inflating: Collection5/026.ann     \n",
      "  inflating: Collection5/026.txt     \n",
      "  inflating: Collection5/027.ann     \n",
      "  inflating: Collection5/027.txt     \n",
      "  inflating: Collection5/028.ann     \n",
      "  inflating: Collection5/028.txt     \n",
      "  inflating: Collection5/029.ann     \n",
      "  inflating: Collection5/029.txt     \n",
      "  inflating: Collection5/030.ann     \n",
      "  inflating: Collection5/030.txt     \n",
      "  inflating: Collection5/031.ann     \n",
      "  inflating: Collection5/031.txt     \n",
      "  inflating: Collection5/032.ann     \n",
      "  inflating: Collection5/032.txt     \n",
      "  inflating: Collection5/033.ann     \n",
      "  inflating: Collection5/033.txt     \n",
      "  inflating: Collection5/034.ann     \n",
      "  inflating: Collection5/034.txt     \n",
      "  inflating: Collection5/035.ann     \n",
      "  inflating: Collection5/035.txt     \n",
      "  inflating: Collection5/036.ann     \n",
      "  inflating: Collection5/036.txt     \n",
      "  inflating: Collection5/037.ann     \n",
      "  inflating: Collection5/037.txt     \n",
      "  inflating: Collection5/038.ann     \n",
      "  inflating: Collection5/038.txt     \n",
      "  inflating: Collection5/039.ann     \n",
      "  inflating: Collection5/039.txt     \n",
      "  inflating: Collection5/03_12_12a.ann  \n",
      "  inflating: Collection5/03_12_12a.txt  \n",
      "  inflating: Collection5/03_12_12b.ann  \n",
      "  inflating: Collection5/03_12_12b.txt  \n",
      "  inflating: Collection5/03_12_12c.ann  \n",
      "  inflating: Collection5/03_12_12c.txt  \n",
      "  inflating: Collection5/03_12_12d.ann  \n",
      "  inflating: Collection5/03_12_12d.txt  \n",
      "  inflating: Collection5/03_12_12g.ann  \n",
      "  inflating: Collection5/03_12_12g.txt  \n",
      "  inflating: Collection5/03_12_12h.ann  \n",
      "  inflating: Collection5/03_12_12h.txt  \n",
      "  inflating: Collection5/040.ann     \n",
      "  inflating: Collection5/040.txt     \n",
      "  inflating: Collection5/041.ann     \n",
      "  inflating: Collection5/041.txt     \n",
      "  inflating: Collection5/042.ann     \n",
      "  inflating: Collection5/042.txt     \n",
      "  inflating: Collection5/043.ann     \n",
      "  inflating: Collection5/043.txt     \n",
      "  inflating: Collection5/044.ann     \n",
      "  inflating: Collection5/044.txt     \n",
      "  inflating: Collection5/045.ann     \n",
      "  inflating: Collection5/045.txt     \n",
      "  inflating: Collection5/046.ann     \n",
      "  inflating: Collection5/046.txt     \n",
      "  inflating: Collection5/047.ann     \n",
      "  inflating: Collection5/047.txt     \n",
      "  inflating: Collection5/048.ann     \n",
      "  inflating: Collection5/048.txt     \n",
      "  inflating: Collection5/049.ann     \n",
      "  inflating: Collection5/049.txt     \n",
      "  inflating: Collection5/04_02_13a_abdulatipov.ann  \n",
      "  inflating: Collection5/04_02_13a_abdulatipov.txt  \n",
      "  inflating: Collection5/04_03_13a_sorokin.ann  \n",
      "  inflating: Collection5/04_03_13a_sorokin.txt  \n",
      "  inflating: Collection5/04_12_12b.ann  \n",
      "  inflating: Collection5/04_12_12b.txt  \n",
      "  inflating: Collection5/04_12_12d.ann  \n",
      "  inflating: Collection5/04_12_12d.txt  \n",
      "  inflating: Collection5/04_12_12f.ann  \n",
      "  inflating: Collection5/04_12_12f.txt  \n",
      "  inflating: Collection5/04_12_12g.ann  \n",
      "  inflating: Collection5/04_12_12g.txt  \n",
      "  inflating: Collection5/04_12_12h_corr.ann  \n",
      "  inflating: Collection5/04_12_12h_corr.txt  \n",
      "  inflating: Collection5/050.ann     \n",
      "  inflating: Collection5/050.txt     \n",
      "  inflating: Collection5/051.ann     \n",
      "  inflating: Collection5/051.txt     \n",
      "  inflating: Collection5/052.ann     \n",
      "  inflating: Collection5/052.txt     \n",
      "  inflating: Collection5/053.ann     \n",
      "  inflating: Collection5/053.txt     \n",
      "  inflating: Collection5/054.ann     \n",
      "  inflating: Collection5/054.txt     \n",
      "  inflating: Collection5/055.ann     \n",
      "  inflating: Collection5/055.txt     \n",
      "  inflating: Collection5/056.ann     \n",
      "  inflating: Collection5/056.txt     \n",
      "  inflating: Collection5/057.ann     \n",
      "  inflating: Collection5/057.txt     \n",
      "  inflating: Collection5/058.ann     \n",
      "  inflating: Collection5/058.txt     \n",
      "  inflating: Collection5/059.ann     \n",
      "  inflating: Collection5/059.txt     \n",
      "  inflating: Collection5/060.ann     \n",
      "  inflating: Collection5/060.txt     \n",
      "  inflating: Collection5/061.ann     \n",
      "  inflating: Collection5/061.txt     \n",
      "  inflating: Collection5/062.ann     \n",
      "  inflating: Collection5/062.txt     \n",
      "  inflating: Collection5/063.ann     \n",
      "  inflating: Collection5/063.txt     \n",
      "  inflating: Collection5/064.ann     \n",
      "  inflating: Collection5/064.txt     \n",
      "  inflating: Collection5/065.ann     \n",
      "  inflating: Collection5/065.txt     \n",
      "  inflating: Collection5/066.ann     \n",
      "  inflating: Collection5/066.txt     \n",
      "  inflating: Collection5/067.ann     \n",
      "  inflating: Collection5/067.txt     \n",
      "  inflating: Collection5/068.ann     \n",
      "  inflating: Collection5/068.txt     \n",
      "  inflating: Collection5/069.ann     \n",
      "  inflating: Collection5/069.txt     \n",
      "  inflating: Collection5/070.ann     \n",
      "  inflating: Collection5/070.txt     \n",
      "  inflating: Collection5/071.ann     \n",
      "  inflating: Collection5/071.txt     \n",
      "  inflating: Collection5/072.ann     \n",
      "  inflating: Collection5/072.txt     \n",
      "  inflating: Collection5/073.ann     \n",
      "  inflating: Collection5/073.txt     \n",
      "  inflating: Collection5/074.ann     \n",
      "  inflating: Collection5/074.txt     \n",
      "  inflating: Collection5/075.ann     \n",
      "  inflating: Collection5/075.txt     \n",
      "  inflating: Collection5/076.ann     \n",
      "  inflating: Collection5/076.txt     \n",
      "  inflating: Collection5/077.ann     \n",
      "  inflating: Collection5/077.txt     \n",
      "  inflating: Collection5/078.ann     \n",
      "  inflating: Collection5/078.txt     \n",
      "  inflating: Collection5/079.ann     \n",
      "  inflating: Collection5/079.txt     \n",
      "  inflating: Collection5/080.ann     \n",
      "  inflating: Collection5/080.txt     \n",
      "  inflating: Collection5/081.ann     \n",
      "  inflating: Collection5/081.txt     \n",
      "  inflating: Collection5/082.ann     \n",
      "  inflating: Collection5/082.txt     \n",
      "  inflating: Collection5/083.ann     \n",
      "  inflating: Collection5/083.txt     \n",
      "  inflating: Collection5/084.ann     \n",
      "  inflating: Collection5/084.txt     \n",
      "  inflating: Collection5/085.ann     \n",
      "  inflating: Collection5/085.txt     \n",
      "  inflating: Collection5/086.ann     \n",
      "  inflating: Collection5/086.txt     \n",
      "  inflating: Collection5/087.ann     \n",
      "  inflating: Collection5/087.txt     \n",
      "  inflating: Collection5/088.ann     \n",
      "  inflating: Collection5/088.txt     \n",
      "  inflating: Collection5/089.ann     \n",
      "  inflating: Collection5/089.txt     \n",
      "  inflating: Collection5/090.ann     \n",
      "  inflating: Collection5/090.txt     \n",
      "  inflating: Collection5/091.ann     \n",
      "  inflating: Collection5/091.txt     \n",
      "  inflating: Collection5/092.ann     \n",
      "  inflating: Collection5/092.txt     \n",
      "  inflating: Collection5/093.ann     \n",
      "  inflating: Collection5/093.txt     \n",
      "  inflating: Collection5/094.ann     \n",
      "  inflating: Collection5/094.txt     \n",
      "  inflating: Collection5/095.ann     \n",
      "  inflating: Collection5/095.txt     \n",
      "  inflating: Collection5/096.ann     \n",
      "  inflating: Collection5/096.txt     \n",
      "  inflating: Collection5/097.ann     \n",
      "  inflating: Collection5/097.txt     \n",
      "  inflating: Collection5/098.ann     \n",
      "  inflating: Collection5/098.txt     \n",
      "  inflating: Collection5/099.ann     \n",
      "  inflating: Collection5/099.txt     \n",
      "  inflating: Collection5/09_01_13.ann  \n",
      "  inflating: Collection5/09_01_13.txt  \n",
      "  inflating: Collection5/09_01_13a.ann  \n",
      "  inflating: Collection5/09_01_13a.txt  \n",
      "  inflating: Collection5/09_01_13c.ann  \n",
      "  inflating: Collection5/09_01_13c.txt  \n",
      "  inflating: Collection5/09_01_13d.ann  \n",
      "  inflating: Collection5/09_01_13d.txt  \n",
      "  inflating: Collection5/09_01_13e.ann  \n",
      "  inflating: Collection5/09_01_13e.txt  \n",
      "  inflating: Collection5/09_01_13h.ann  \n",
      "  inflating: Collection5/09_01_13h.txt  \n",
      "  inflating: Collection5/09_01_13i.ann  \n",
      "  inflating: Collection5/09_01_13i.txt  \n",
      "  inflating: Collection5/100.ann     \n",
      "  inflating: Collection5/100.txt     \n",
      "  inflating: Collection5/1000.ann    \n",
      "  inflating: Collection5/1000.txt    \n",
      "  inflating: Collection5/1001.ann    \n",
      "  inflating: Collection5/1001.txt    \n",
      "  inflating: Collection5/1002.ann    \n",
      "  inflating: Collection5/1002.txt    \n",
      "  inflating: Collection5/1003.ann    \n",
      "  inflating: Collection5/1003.txt    \n",
      "  inflating: Collection5/1004.ann    \n",
      "  inflating: Collection5/1004.txt    \n",
      "  inflating: Collection5/1005.ann    \n",
      "  inflating: Collection5/1005.txt    \n",
      "  inflating: Collection5/1006.ann    \n",
      "  inflating: Collection5/1006.txt    \n",
      "  inflating: Collection5/1007.ann    \n",
      "  inflating: Collection5/1007.txt    \n",
      "  inflating: Collection5/1008.ann    \n",
      "  inflating: Collection5/1008.txt    \n",
      "  inflating: Collection5/1009.ann    \n",
      "  inflating: Collection5/1009.txt    \n",
      "  inflating: Collection5/101.ann     \n",
      "  inflating: Collection5/101.txt     \n",
      "  inflating: Collection5/1010.ann    \n",
      "  inflating: Collection5/1010.txt    \n",
      "  inflating: Collection5/1011.ann    \n",
      "  inflating: Collection5/1011.txt    \n",
      "  inflating: Collection5/1012.ann    \n",
      "  inflating: Collection5/1012.txt    \n",
      "  inflating: Collection5/1013.ann    \n",
      "  inflating: Collection5/1013.txt    \n",
      "  inflating: Collection5/1014.ann    \n",
      "  inflating: Collection5/1014.txt    \n",
      "  inflating: Collection5/1015.ann    \n",
      "  inflating: Collection5/1015.txt    \n",
      "  inflating: Collection5/1016.ann    \n",
      "  inflating: Collection5/1016.txt    \n",
      "  inflating: Collection5/1017.ann    \n",
      "  inflating: Collection5/1017.txt    \n",
      "  inflating: Collection5/1018.ann    \n",
      "  inflating: Collection5/1018.txt    \n",
      "  inflating: Collection5/1019.ann    \n",
      "  inflating: Collection5/1019.txt    \n",
      "  inflating: Collection5/102.ann     \n",
      "  inflating: Collection5/102.txt     \n",
      "  inflating: Collection5/1020.ann    \n",
      "  inflating: Collection5/1020.txt    \n",
      "  inflating: Collection5/1021.ann    \n",
      "  inflating: Collection5/1021.txt    \n",
      "  inflating: Collection5/1022.ann    \n",
      "  inflating: Collection5/1022.txt    \n",
      "  inflating: Collection5/1023.ann    \n",
      "  inflating: Collection5/1023.txt    \n",
      "  inflating: Collection5/1024.ann    \n",
      "  inflating: Collection5/1024.txt    \n",
      "  inflating: Collection5/1025.ann    \n",
      "  inflating: Collection5/1025.txt    \n",
      "  inflating: Collection5/1026.ann    \n",
      "  inflating: Collection5/1026.txt    \n",
      "  inflating: Collection5/1027.ann    \n",
      "  inflating: Collection5/1027.txt    \n",
      "  inflating: Collection5/1028.ann    \n",
      "  inflating: Collection5/1028.txt    \n",
      "  inflating: Collection5/1029.ann    \n",
      "  inflating: Collection5/1029.txt    \n",
      "  inflating: Collection5/103.ann     \n",
      "  inflating: Collection5/103.txt     \n",
      "  inflating: Collection5/1030.ann    \n",
      "  inflating: Collection5/1030.txt    \n",
      "  inflating: Collection5/1031.ann    \n",
      "  inflating: Collection5/1031.txt    \n",
      "  inflating: Collection5/1032.ann    \n",
      "  inflating: Collection5/1032.txt    \n",
      "  inflating: Collection5/1033.ann    \n",
      "  inflating: Collection5/1033.txt    \n",
      "  inflating: Collection5/1034.ann    \n",
      "  inflating: Collection5/1034.txt    \n",
      "  inflating: Collection5/1035.ann    \n",
      "  inflating: Collection5/1035.txt    \n",
      "  inflating: Collection5/1036.ann    \n",
      "  inflating: Collection5/1036.txt    \n",
      "  inflating: Collection5/1037.ann    \n",
      "  inflating: Collection5/1037.txt    \n",
      "  inflating: Collection5/1038.ann    \n",
      "  inflating: Collection5/1038.txt    \n",
      "  inflating: Collection5/1039.ann    \n",
      "  inflating: Collection5/1039.txt    \n",
      "  inflating: Collection5/104.ann     \n",
      "  inflating: Collection5/104.txt     \n",
      "  inflating: Collection5/1040.ann    \n",
      "  inflating: Collection5/1040.txt    \n",
      "  inflating: Collection5/1041.ann    \n",
      "  inflating: Collection5/1041.txt    \n",
      "  inflating: Collection5/1042.ann    \n",
      "  inflating: Collection5/1042.txt    \n",
      "  inflating: Collection5/1043.ann    \n",
      "  inflating: Collection5/1043.txt    \n",
      "  inflating: Collection5/1044.ann    \n",
      "  inflating: Collection5/1044.txt    \n",
      "  inflating: Collection5/1045.ann    \n",
      "  inflating: Collection5/1045.txt    \n",
      "  inflating: Collection5/1046.ann    \n",
      "  inflating: Collection5/1046.txt    \n",
      "  inflating: Collection5/1047.ann    \n",
      "  inflating: Collection5/1047.txt    \n",
      "  inflating: Collection5/1048.ann    \n",
      "  inflating: Collection5/1048.txt    \n",
      "  inflating: Collection5/1049.ann    \n",
      "  inflating: Collection5/1049.txt    \n",
      "  inflating: Collection5/105.ann     \n",
      "  inflating: Collection5/105.txt     \n",
      "  inflating: Collection5/1050.ann    \n",
      "  inflating: Collection5/1050.txt    \n",
      "  inflating: Collection5/106.ann     \n",
      "  inflating: Collection5/106.txt     \n",
      "  inflating: Collection5/107.ann     \n",
      "  inflating: Collection5/107.txt     \n",
      "  inflating: Collection5/108.ann     \n",
      "  inflating: Collection5/108.txt     \n",
      "  inflating: Collection5/109.ann     \n",
      "  inflating: Collection5/109.txt     \n",
      "  inflating: Collection5/10_01_13a.ann  \n",
      "  inflating: Collection5/10_01_13a.txt  \n",
      "  inflating: Collection5/10_01_13d.ann  \n",
      "  inflating: Collection5/10_01_13d.txt  \n",
      "  inflating: Collection5/10_01_13i.ann  \n",
      "  inflating: Collection5/10_01_13i.txt  \n",
      "  inflating: Collection5/110.ann     \n",
      "  inflating: Collection5/110.txt     \n",
      "  inflating: Collection5/1100.ann    \n",
      "  inflating: Collection5/1100.txt    \n",
      "  inflating: Collection5/1101.ann    \n",
      "  inflating: Collection5/1101.txt    \n",
      "  inflating: Collection5/1102.ann    \n",
      "  inflating: Collection5/1102.txt    \n",
      "  inflating: Collection5/1103.ann    \n",
      "  inflating: Collection5/1103.txt    \n",
      "  inflating: Collection5/1104.ann    \n",
      "  inflating: Collection5/1104.txt    \n",
      "  inflating: Collection5/1105.ann    \n",
      "  inflating: Collection5/1105.txt    \n",
      "  inflating: Collection5/1106.ann    \n",
      "  inflating: Collection5/1106.txt    \n",
      "  inflating: Collection5/1107.ann    \n",
      "  inflating: Collection5/1107.txt    \n",
      "  inflating: Collection5/1108.ann    \n",
      "  inflating: Collection5/1108.txt    \n",
      "  inflating: Collection5/1109.ann    \n",
      "  inflating: Collection5/1109.txt    \n",
      "  inflating: Collection5/111.ann     \n",
      "  inflating: Collection5/111.txt     \n",
      "  inflating: Collection5/1110.ann    \n",
      "  inflating: Collection5/1110.txt    \n",
      "  inflating: Collection5/1111.ann    \n",
      "  inflating: Collection5/1111.txt    \n",
      "  inflating: Collection5/1112.ann    \n",
      "  inflating: Collection5/1112.txt    \n",
      "  inflating: Collection5/1113.ann    \n",
      "  inflating: Collection5/1113.txt    \n",
      "  inflating: Collection5/1114.ann    \n",
      "  inflating: Collection5/1114.txt    \n",
      "  inflating: Collection5/1115.ann    \n",
      "  inflating: Collection5/1115.txt    \n",
      "  inflating: Collection5/1116.ann    \n",
      "  inflating: Collection5/1116.txt    \n",
      "  inflating: Collection5/1117.ann    \n",
      "  inflating: Collection5/1117.txt    \n",
      "  inflating: Collection5/1118.ann    \n",
      "  inflating: Collection5/1118.txt    \n",
      "  inflating: Collection5/1119.ann    \n",
      "  inflating: Collection5/1119.txt    \n",
      "  inflating: Collection5/112.ann     \n",
      "  inflating: Collection5/112.txt     \n",
      "  inflating: Collection5/1120.ann    \n",
      "  inflating: Collection5/1120.txt    \n",
      "  inflating: Collection5/1121.ann    \n",
      "  inflating: Collection5/1121.txt    \n",
      "  inflating: Collection5/1122.ann    \n",
      "  inflating: Collection5/1122.txt    \n",
      "  inflating: Collection5/1123.ann    \n",
      "  inflating: Collection5/1123.txt    \n",
      "  inflating: Collection5/1124.ann    \n",
      "  inflating: Collection5/1124.txt    \n",
      "  inflating: Collection5/1125.ann    \n",
      "  inflating: Collection5/1125.txt    \n",
      "  inflating: Collection5/1126.ann    \n",
      "  inflating: Collection5/1126.txt    \n",
      "  inflating: Collection5/1127.ann    \n",
      "  inflating: Collection5/1127.txt    \n",
      "  inflating: Collection5/1128.ann    \n",
      "  inflating: Collection5/1128.txt    \n",
      "  inflating: Collection5/113.ann     \n",
      "  inflating: Collection5/113.txt     \n",
      "  inflating: Collection5/1130.ann    \n",
      "  inflating: Collection5/1130.txt    \n",
      "  inflating: Collection5/1131.ann    \n",
      "  inflating: Collection5/1131.txt    \n",
      "  inflating: Collection5/1132.ann    \n",
      "  inflating: Collection5/1132.txt    \n",
      "  inflating: Collection5/1133.ann    \n",
      "  inflating: Collection5/1133.txt    \n",
      "  inflating: Collection5/1134.ann    \n",
      "  inflating: Collection5/1134.txt    \n",
      "  inflating: Collection5/1135.ann    \n",
      "  inflating: Collection5/1135.txt    \n",
      "  inflating: Collection5/1136.ann    \n",
      "  inflating: Collection5/1136.txt    \n",
      "  inflating: Collection5/1137.ann    \n",
      "  inflating: Collection5/1137.txt    \n",
      "  inflating: Collection5/1138.ann    \n",
      "  inflating: Collection5/1138.txt    \n",
      "  inflating: Collection5/1139.ann    \n",
      "  inflating: Collection5/1139.txt    \n",
      "  inflating: Collection5/114.ann     \n",
      "  inflating: Collection5/114.txt     \n",
      "  inflating: Collection5/1140.ann    \n",
      "  inflating: Collection5/1140.txt    \n",
      "  inflating: Collection5/1141.ann    \n",
      "  inflating: Collection5/1141.txt    \n",
      "  inflating: Collection5/1142.ann    \n",
      "  inflating: Collection5/1142.txt    \n",
      "  inflating: Collection5/1143.ann    \n",
      "  inflating: Collection5/1143.txt    \n",
      "  inflating: Collection5/1144.ann    \n",
      "  inflating: Collection5/1144.txt    \n",
      "  inflating: Collection5/1145.ann    \n",
      "  inflating: Collection5/1145.txt    \n",
      "  inflating: Collection5/1146.ann    \n",
      "  inflating: Collection5/1146.txt    \n",
      "  inflating: Collection5/1147.ann    \n",
      "  inflating: Collection5/1147.txt    \n",
      "  inflating: Collection5/1148.ann    \n",
      "  inflating: Collection5/1148.txt    \n",
      "  inflating: Collection5/1149.ann    \n",
      "  inflating: Collection5/1149.txt    \n",
      "  inflating: Collection5/115.ann     \n",
      "  inflating: Collection5/115.txt     \n",
      "  inflating: Collection5/1150.ann    \n",
      "  inflating: Collection5/1150.txt    \n",
      "  inflating: Collection5/1151.ann    \n",
      "  inflating: Collection5/1151.txt    \n",
      "  inflating: Collection5/1152.ann    \n",
      "  inflating: Collection5/1152.txt    \n",
      "  inflating: Collection5/1153.ann    \n",
      "  inflating: Collection5/1153.txt    \n",
      "  inflating: Collection5/1154.ann    \n",
      "  inflating: Collection5/1154.txt    \n",
      "  inflating: Collection5/1155.ann    \n",
      "  inflating: Collection5/1155.txt    \n",
      "  inflating: Collection5/1156.ann    \n",
      "  inflating: Collection5/1156.txt    \n",
      "  inflating: Collection5/1157.ann    \n",
      "  inflating: Collection5/1157.txt    \n",
      "  inflating: Collection5/1158.ann    \n",
      "  inflating: Collection5/1158.txt    \n",
      "  inflating: Collection5/1159.ann    \n",
      "  inflating: Collection5/1159.txt    \n",
      "  inflating: Collection5/116.ann     \n",
      "  inflating: Collection5/116.txt     \n",
      "  inflating: Collection5/1160.ann    \n",
      "  inflating: Collection5/1160.txt    \n",
      "  inflating: Collection5/1161.ann    \n",
      "  inflating: Collection5/1161.txt    \n",
      "  inflating: Collection5/1162.ann    \n",
      "  inflating: Collection5/1162.txt    \n",
      "  inflating: Collection5/1163.ann    \n",
      "  inflating: Collection5/1163.txt    \n",
      "  inflating: Collection5/1164.ann    \n",
      "  inflating: Collection5/1164.txt    \n",
      "  inflating: Collection5/1165.ann    \n",
      "  inflating: Collection5/1165.txt    \n",
      "  inflating: Collection5/1166.ann    \n",
      "  inflating: Collection5/1166.txt    \n",
      "  inflating: Collection5/1167.ann    \n",
      "  inflating: Collection5/1167.txt    \n",
      "  inflating: Collection5/1168.ann    \n",
      "  inflating: Collection5/1168.txt    \n",
      "  inflating: Collection5/1169.ann    \n",
      "  inflating: Collection5/1169.txt    \n",
      "  inflating: Collection5/117.ann     \n",
      "  inflating: Collection5/117.txt     \n",
      "  inflating: Collection5/1170.ann    \n",
      "  inflating: Collection5/1170.txt    \n",
      "  inflating: Collection5/1171.ann    \n",
      "  inflating: Collection5/1171.txt    \n",
      "  inflating: Collection5/1172.ann    \n",
      "  inflating: Collection5/1172.txt    \n",
      "  inflating: Collection5/1173.ann    \n",
      "  inflating: Collection5/1173.txt    \n",
      "  inflating: Collection5/1174.ann    \n",
      "  inflating: Collection5/1174.txt    \n",
      "  inflating: Collection5/1175.ann    \n",
      "  inflating: Collection5/1175.txt    \n",
      "  inflating: Collection5/1176.ann    \n",
      "  inflating: Collection5/1176.txt    \n",
      "  inflating: Collection5/1177.ann    \n",
      "  inflating: Collection5/1177.txt    \n",
      "  inflating: Collection5/1178.ann    \n",
      "  inflating: Collection5/1178.txt    \n",
      "  inflating: Collection5/1179.ann    \n",
      "  inflating: Collection5/1179.txt    \n",
      "  inflating: Collection5/118.ann     \n",
      "  inflating: Collection5/118.txt     \n",
      "  inflating: Collection5/1180.ann    \n",
      "  inflating: Collection5/1180.txt    \n",
      "  inflating: Collection5/1181.ann    \n",
      "  inflating: Collection5/1181.txt    \n",
      "  inflating: Collection5/1182.ann    \n",
      "  inflating: Collection5/1182.txt    \n",
      "  inflating: Collection5/1183.ann    \n",
      "  inflating: Collection5/1183.txt    \n",
      "  inflating: Collection5/1184.ann    \n",
      "  inflating: Collection5/1184.txt    \n",
      "  inflating: Collection5/1185.ann    \n",
      "  inflating: Collection5/1185.txt    \n",
      "  inflating: Collection5/1186.ann    \n",
      "  inflating: Collection5/1186.txt    \n",
      "  inflating: Collection5/1187.ann    \n",
      "  inflating: Collection5/1187.txt    \n",
      "  inflating: Collection5/1188.ann    \n",
      "  inflating: Collection5/1188.txt    \n",
      "  inflating: Collection5/1189.ann    \n",
      "  inflating: Collection5/1189.txt    \n",
      "  inflating: Collection5/119.ann     \n",
      "  inflating: Collection5/119.txt     \n",
      "  inflating: Collection5/1190.ann    \n",
      "  inflating: Collection5/1190.txt    \n",
      "  inflating: Collection5/1191.ann    \n",
      "  inflating: Collection5/1191.txt    \n",
      "  inflating: Collection5/1192.ann    \n",
      "  inflating: Collection5/1192.txt    \n",
      "  inflating: Collection5/1193.ann    \n",
      "  inflating: Collection5/1193.txt    \n",
      "  inflating: Collection5/1194.ann    \n",
      "  inflating: Collection5/1194.txt    \n",
      "  inflating: Collection5/1195.ann    \n",
      "  inflating: Collection5/1195.txt    \n",
      "  inflating: Collection5/1196.ann    \n",
      "  inflating: Collection5/1196.txt    \n",
      "  inflating: Collection5/1197.ann    \n",
      "  inflating: Collection5/1197.txt    \n",
      "  inflating: Collection5/1198.ann    \n",
      "  inflating: Collection5/1198.txt    \n",
      "  inflating: Collection5/1199.ann    \n",
      "  inflating: Collection5/1199.txt    \n",
      "  inflating: Collection5/11_01_13b.ann  \n",
      "  inflating: Collection5/11_01_13b.txt  \n",
      "  inflating: Collection5/11_01_13e.ann  \n",
      "  inflating: Collection5/11_01_13e.txt  \n",
      "  inflating: Collection5/120.ann     \n",
      "  inflating: Collection5/120.txt     \n",
      "  inflating: Collection5/1200.ann    \n",
      "  inflating: Collection5/1200.txt    \n",
      "  inflating: Collection5/121.ann     \n",
      "  inflating: Collection5/121.txt     \n",
      "  inflating: Collection5/122.ann     \n",
      "  inflating: Collection5/122.txt     \n",
      "  inflating: Collection5/123.ann     \n",
      "  inflating: Collection5/123.txt     \n",
      "  inflating: Collection5/124.ann     \n",
      "  inflating: Collection5/124.txt     \n",
      "  inflating: Collection5/125.ann     \n",
      "  inflating: Collection5/125.txt     \n",
      "  inflating: Collection5/126.ann     \n",
      "  inflating: Collection5/126.txt     \n",
      "  inflating: Collection5/127.ann     \n",
      "  inflating: Collection5/127.txt     \n",
      "  inflating: Collection5/128.ann     \n",
      "  inflating: Collection5/128.txt     \n",
      "  inflating: Collection5/129.ann     \n",
      "  inflating: Collection5/129.txt     \n",
      "  inflating: Collection5/130.ann     \n",
      "  inflating: Collection5/130.txt     \n",
      "  inflating: Collection5/131.ann     \n",
      "  inflating: Collection5/131.txt     \n",
      "  inflating: Collection5/132.ann     \n",
      "  inflating: Collection5/132.txt     \n",
      "  inflating: Collection5/133.ann     \n",
      "  inflating: Collection5/133.txt     \n",
      "  inflating: Collection5/134.ann     \n",
      "  inflating: Collection5/134.txt     \n",
      "  inflating: Collection5/135.ann     \n",
      "  inflating: Collection5/135.txt     \n",
      "  inflating: Collection5/136.ann     \n",
      "  inflating: Collection5/136.txt     \n",
      "  inflating: Collection5/137.ann     \n",
      "  inflating: Collection5/137.txt     \n",
      "  inflating: Collection5/138.ann     \n",
      "  inflating: Collection5/138.txt     \n",
      "  inflating: Collection5/139.ann     \n",
      "  inflating: Collection5/139.txt     \n",
      "  inflating: Collection5/140.ann     \n",
      "  inflating: Collection5/140.txt     \n",
      "  inflating: Collection5/141.ann     \n",
      "  inflating: Collection5/141.txt     \n",
      "  inflating: Collection5/142.ann     \n",
      "  inflating: Collection5/142.txt     \n",
      "  inflating: Collection5/143.ann     \n",
      "  inflating: Collection5/143.txt     \n",
      "  inflating: Collection5/144.ann     \n",
      "  inflating: Collection5/144.txt     \n",
      "  inflating: Collection5/145.ann     \n",
      "  inflating: Collection5/145.txt     \n",
      "  inflating: Collection5/146.ann     \n",
      "  inflating: Collection5/146.txt     \n",
      "  inflating: Collection5/147.ann     \n",
      "  inflating: Collection5/147.txt     \n",
      "  inflating: Collection5/148.ann     \n",
      "  inflating: Collection5/148.txt     \n",
      "  inflating: Collection5/149.ann     \n",
      "  inflating: Collection5/149.txt     \n",
      "  inflating: Collection5/14_01_13c.ann  \n",
      "  inflating: Collection5/14_01_13c.txt  \n",
      "  inflating: Collection5/14_01_13g.ann  \n",
      "  inflating: Collection5/14_01_13g.txt  \n",
      "  inflating: Collection5/14_01_13i.ann  \n",
      "  inflating: Collection5/14_01_13i.txt  \n",
      "  inflating: Collection5/150.ann     \n",
      "  inflating: Collection5/150.txt     \n",
      "  inflating: Collection5/151.ann     \n",
      "  inflating: Collection5/151.txt     \n",
      "  inflating: Collection5/152.ann     \n",
      "  inflating: Collection5/152.txt     \n",
      "  inflating: Collection5/153.ann     \n",
      "  inflating: Collection5/153.txt     \n",
      "  inflating: Collection5/154.ann     \n",
      "  inflating: Collection5/154.txt     \n",
      "  inflating: Collection5/155.ann     \n",
      "  inflating: Collection5/155.txt     \n",
      "  inflating: Collection5/156.ann     \n",
      "  inflating: Collection5/156.txt     \n",
      "  inflating: Collection5/157.ann     \n",
      "  inflating: Collection5/157.txt     \n",
      "  inflating: Collection5/158.ann     \n",
      "  inflating: Collection5/158.txt     \n",
      "  inflating: Collection5/159.ann     \n",
      "  inflating: Collection5/159.txt     \n",
      "  inflating: Collection5/15_01_13a.ann  \n",
      "  inflating: Collection5/15_01_13a.txt  \n",
      "  inflating: Collection5/15_01_13b.ann  \n",
      "  inflating: Collection5/15_01_13b.txt  \n",
      "  inflating: Collection5/15_01_13e.ann  \n",
      "  inflating: Collection5/15_01_13e.txt  \n",
      "  inflating: Collection5/15_01_13f.ann  \n",
      "  inflating: Collection5/15_01_13f.txt  \n",
      "  inflating: Collection5/160.ann     \n",
      "  inflating: Collection5/160.txt     \n",
      "  inflating: Collection5/161.ann     \n",
      "  inflating: Collection5/161.txt     \n",
      "  inflating: Collection5/162.ann     \n",
      "  inflating: Collection5/162.txt     \n",
      "  inflating: Collection5/163.ann     \n",
      "  inflating: Collection5/163.txt     \n",
      "  inflating: Collection5/164.ann     \n",
      "  inflating: Collection5/164.txt     \n",
      "  inflating: Collection5/165.ann     \n",
      "  inflating: Collection5/165.txt     \n",
      "  inflating: Collection5/166.ann     \n",
      "  inflating: Collection5/166.txt     \n",
      "  inflating: Collection5/167.ann     \n",
      "  inflating: Collection5/167.txt     \n",
      "  inflating: Collection5/168.ann     \n",
      "  inflating: Collection5/168.txt     \n",
      "  inflating: Collection5/169.ann     \n",
      "  inflating: Collection5/169.txt     \n",
      "  inflating: Collection5/170.ann     \n",
      "  inflating: Collection5/170.txt     \n",
      "  inflating: Collection5/171.ann     \n",
      "  inflating: Collection5/171.txt     \n",
      "  inflating: Collection5/172.ann     \n",
      "  inflating: Collection5/172.txt     \n",
      "  inflating: Collection5/173.ann     \n",
      "  inflating: Collection5/173.txt     \n",
      "  inflating: Collection5/174.ann     \n",
      "  inflating: Collection5/174.txt     \n",
      "  inflating: Collection5/175.ann     \n",
      "  inflating: Collection5/175.txt     \n",
      "  inflating: Collection5/176.ann     \n",
      "  inflating: Collection5/176.txt     \n",
      "  inflating: Collection5/177.ann     \n",
      "  inflating: Collection5/177.txt     \n",
      "  inflating: Collection5/178.ann     \n",
      "  inflating: Collection5/178.txt     \n",
      "  inflating: Collection5/179.ann     \n",
      "  inflating: Collection5/179.txt     \n",
      "  inflating: Collection5/180.ann     \n",
      "  inflating: Collection5/180.txt     \n",
      "  inflating: Collection5/181.ann     \n",
      "  inflating: Collection5/181.txt     \n",
      "  inflating: Collection5/182.ann     \n",
      "  inflating: Collection5/182.txt     \n",
      "  inflating: Collection5/183.ann     \n",
      "  inflating: Collection5/183.txt     \n",
      "  inflating: Collection5/184.ann     \n",
      "  inflating: Collection5/184.txt     \n",
      "  inflating: Collection5/185.ann     \n",
      "  inflating: Collection5/185.txt     \n",
      "  inflating: Collection5/186.ann     \n",
      "  inflating: Collection5/186.txt     \n",
      "  inflating: Collection5/187.ann     \n",
      "  inflating: Collection5/187.txt     \n",
      "  inflating: Collection5/188.ann     \n",
      "  inflating: Collection5/188.txt     \n",
      "  inflating: Collection5/189.ann     \n",
      "  inflating: Collection5/189.txt     \n",
      "  inflating: Collection5/190.ann     \n",
      "  inflating: Collection5/190.txt     \n",
      "  inflating: Collection5/191.ann     \n",
      "  inflating: Collection5/191.txt     \n",
      "  inflating: Collection5/192.ann     \n",
      "  inflating: Collection5/192.txt     \n",
      "  inflating: Collection5/193.ann     \n",
      "  inflating: Collection5/193.txt     \n",
      "  inflating: Collection5/194.ann     \n",
      "  inflating: Collection5/194.txt     \n",
      "  inflating: Collection5/195.ann     \n",
      "  inflating: Collection5/195.txt     \n",
      "  inflating: Collection5/196.ann     \n",
      "  inflating: Collection5/196.txt     \n",
      "  inflating: Collection5/197.ann     \n",
      "  inflating: Collection5/197.txt     \n",
      "  inflating: Collection5/198.ann     \n",
      "  inflating: Collection5/198.txt     \n",
      "  inflating: Collection5/199.ann     \n",
      "  inflating: Collection5/199.txt     \n",
      "  inflating: Collection5/19_11_12d.ann  \n",
      "  inflating: Collection5/19_11_12d.txt  \n",
      "  inflating: Collection5/19_11_12h.ann  \n",
      "  inflating: Collection5/19_11_12h.txt  \n",
      "  inflating: Collection5/200.ann     \n",
      "  inflating: Collection5/200.txt     \n",
      "  inflating: Collection5/2001.ann    \n",
      "  inflating: Collection5/2001.txt    \n",
      "  inflating: Collection5/2002.ann    \n",
      "  inflating: Collection5/2002.txt    \n",
      "  inflating: Collection5/2003.ann    \n",
      "  inflating: Collection5/2003.txt    \n",
      "  inflating: Collection5/2004.ann    \n",
      "  inflating: Collection5/2004.txt    \n",
      "  inflating: Collection5/2005.ann    \n",
      "  inflating: Collection5/2005.txt    \n",
      "  inflating: Collection5/2006.ann    \n",
      "  inflating: Collection5/2006.txt    \n",
      "  inflating: Collection5/2007.ann    \n",
      "  inflating: Collection5/2007.txt    \n",
      "  inflating: Collection5/2008.ann    \n",
      "  inflating: Collection5/2008.txt    \n",
      "  inflating: Collection5/2009.ann    \n",
      "  inflating: Collection5/2009.txt    \n",
      "  inflating: Collection5/201.ann     \n",
      "  inflating: Collection5/201.txt     \n",
      "  inflating: Collection5/2010.ann    \n",
      "  inflating: Collection5/2010.txt    \n",
      "  inflating: Collection5/2011.ann    \n",
      "  inflating: Collection5/2011.txt    \n",
      "  inflating: Collection5/2012.ann    \n",
      "  inflating: Collection5/2012.txt    \n",
      "  inflating: Collection5/2013.ann    \n",
      "  inflating: Collection5/2013.txt    \n",
      "  inflating: Collection5/2014.ann    \n",
      "  inflating: Collection5/2014.txt    \n",
      "  inflating: Collection5/2015.ann    \n",
      "  inflating: Collection5/2015.txt    \n",
      "  inflating: Collection5/2016.ann    \n",
      "  inflating: Collection5/2016.txt    \n",
      "  inflating: Collection5/2017.ann    \n",
      "  inflating: Collection5/2017.txt    \n",
      "  inflating: Collection5/2018.ann    \n",
      "  inflating: Collection5/2018.txt    \n",
      "  inflating: Collection5/2019.ann    \n",
      "  inflating: Collection5/2019.txt    \n",
      "  inflating: Collection5/202.ann     \n",
      "  inflating: Collection5/202.txt     \n",
      "  inflating: Collection5/2020.ann    \n",
      "  inflating: Collection5/2020.txt    \n",
      "  inflating: Collection5/2021.ann    \n",
      "  inflating: Collection5/2021.txt    \n",
      "  inflating: Collection5/2022.ann    \n",
      "  inflating: Collection5/2022.txt    \n",
      "  inflating: Collection5/2023.ann    \n",
      "  inflating: Collection5/2023.txt    \n",
      "  inflating: Collection5/2024.ann    \n",
      "  inflating: Collection5/2024.txt    \n",
      "  inflating: Collection5/2025.ann    \n",
      "  inflating: Collection5/2025.txt    \n",
      "  inflating: Collection5/2026.ann    \n",
      "  inflating: Collection5/2026.txt    \n",
      "  inflating: Collection5/2027.ann    \n",
      "  inflating: Collection5/2027.txt    \n",
      "  inflating: Collection5/2028.ann    \n",
      "  inflating: Collection5/2028.txt    \n",
      "  inflating: Collection5/2029.ann    \n",
      "  inflating: Collection5/2029.txt    \n",
      "  inflating: Collection5/203.ann     \n",
      "  inflating: Collection5/203.txt     \n",
      "  inflating: Collection5/2030.ann    \n",
      "  inflating: Collection5/2030.txt    \n",
      "  inflating: Collection5/2031.ann    \n",
      "  inflating: Collection5/2031.txt    \n",
      "  inflating: Collection5/2032.ann    \n",
      "  inflating: Collection5/2032.txt    \n",
      "  inflating: Collection5/2034.ann    \n",
      "  inflating: Collection5/2034.txt    \n",
      "  inflating: Collection5/2035.ann    \n",
      "  inflating: Collection5/2035.txt    \n",
      "  inflating: Collection5/2036.ann    \n",
      "  inflating: Collection5/2036.txt    \n",
      "  inflating: Collection5/2037.ann    \n",
      "  inflating: Collection5/2037.txt    \n",
      "  inflating: Collection5/2038.ann    \n",
      "  inflating: Collection5/2038.txt    \n",
      "  inflating: Collection5/2039.ann    \n",
      "  inflating: Collection5/2039.txt    \n",
      "  inflating: Collection5/204.ann     \n",
      "  inflating: Collection5/204.txt     \n",
      "  inflating: Collection5/2040.ann    \n",
      "  inflating: Collection5/2040.txt    \n",
      "  inflating: Collection5/2041.ann    \n",
      "  inflating: Collection5/2041.txt    \n",
      "  inflating: Collection5/2042.ann    \n",
      "  inflating: Collection5/2042.txt    \n",
      "  inflating: Collection5/2043.ann    \n",
      "  inflating: Collection5/2043.txt    \n",
      "  inflating: Collection5/2044.ann    \n",
      "  inflating: Collection5/2044.txt    \n",
      "  inflating: Collection5/2045.ann    \n",
      "  inflating: Collection5/2045.txt    \n",
      "  inflating: Collection5/2046.ann    \n",
      "  inflating: Collection5/2046.txt    \n",
      "  inflating: Collection5/2047.ann    \n",
      "  inflating: Collection5/2047.txt    \n",
      "  inflating: Collection5/2048.ann    \n",
      "  inflating: Collection5/2048.txt    \n",
      "  inflating: Collection5/2049.ann    \n",
      "  inflating: Collection5/2049.txt    \n",
      "  inflating: Collection5/205.ann     \n",
      "  inflating: Collection5/205.txt     \n",
      "  inflating: Collection5/2050.ann    \n",
      "  inflating: Collection5/2050.txt    \n",
      "  inflating: Collection5/206.ann     \n",
      "  inflating: Collection5/206.txt     \n",
      "  inflating: Collection5/207.ann     \n",
      "  inflating: Collection5/207.txt     \n",
      "  inflating: Collection5/208.ann     \n",
      "  inflating: Collection5/208.txt     \n",
      "  inflating: Collection5/209.ann     \n",
      "  inflating: Collection5/209.txt     \n",
      "  inflating: Collection5/20_11_12a.ann  \n",
      "  inflating: Collection5/20_11_12a.txt  \n",
      "  inflating: Collection5/20_11_12b.ann  \n",
      "  inflating: Collection5/20_11_12b.txt  \n",
      "  inflating: Collection5/20_11_12c.ann  \n",
      "  inflating: Collection5/20_11_12c.txt  \n",
      "  inflating: Collection5/20_11_12d.ann  \n",
      "  inflating: Collection5/20_11_12d.txt  \n",
      "  inflating: Collection5/20_11_12i.ann  \n",
      "  inflating: Collection5/20_11_12i.txt  \n",
      "  inflating: Collection5/210.ann     \n",
      "  inflating: Collection5/210.txt     \n",
      "  inflating: Collection5/211.ann     \n",
      "  inflating: Collection5/211.txt     \n",
      "  inflating: Collection5/212.ann     \n",
      "  inflating: Collection5/212.txt     \n",
      "  inflating: Collection5/213.ann     \n",
      "  inflating: Collection5/213.txt     \n",
      "  inflating: Collection5/214.ann     \n",
      "  inflating: Collection5/214.txt     \n",
      "  inflating: Collection5/215.ann     \n",
      "  inflating: Collection5/215.txt     \n",
      "  inflating: Collection5/216.ann     \n",
      "  inflating: Collection5/216.txt     \n",
      "  inflating: Collection5/217.ann     \n",
      "  inflating: Collection5/217.txt     \n",
      "  inflating: Collection5/218.ann     \n",
      "  inflating: Collection5/218.txt     \n",
      "  inflating: Collection5/219.ann     \n",
      "  inflating: Collection5/219.txt     \n",
      "  inflating: Collection5/21_11_12c.ann  \n",
      "  inflating: Collection5/21_11_12c.txt  \n",
      "  inflating: Collection5/21_11_12h.ann  \n",
      "  inflating: Collection5/21_11_12h.txt  \n",
      "  inflating: Collection5/21_11_12i.ann  \n",
      "  inflating: Collection5/21_11_12i.txt  \n",
      "  inflating: Collection5/21_11_12j.ann  \n",
      "  inflating: Collection5/21_11_12j.txt  \n",
      "  inflating: Collection5/220.ann     \n",
      "  inflating: Collection5/220.txt     \n",
      "  inflating: Collection5/221.ann     \n",
      "  inflating: Collection5/221.txt     \n",
      "  inflating: Collection5/222.ann     \n",
      "  inflating: Collection5/222.txt     \n",
      "  inflating: Collection5/223.ann     \n",
      "  inflating: Collection5/223.txt     \n",
      "  inflating: Collection5/224.ann     \n",
      "  inflating: Collection5/224.txt     \n",
      "  inflating: Collection5/225.ann     \n",
      "  inflating: Collection5/225.txt     \n",
      "  inflating: Collection5/226.ann     \n",
      "  inflating: Collection5/226.txt     \n",
      "  inflating: Collection5/227.ann     \n",
      "  inflating: Collection5/227.txt     \n",
      "  inflating: Collection5/228.ann     \n",
      "  inflating: Collection5/228.txt     \n",
      "  inflating: Collection5/229.ann     \n",
      "  inflating: Collection5/229.txt     \n",
      "  inflating: Collection5/22_11_12a.ann  \n",
      "  inflating: Collection5/22_11_12a.txt  \n",
      "  inflating: Collection5/22_11_12c.ann  \n",
      "  inflating: Collection5/22_11_12c.txt  \n",
      "  inflating: Collection5/22_11_12d.ann  \n",
      "  inflating: Collection5/22_11_12d.txt  \n",
      "  inflating: Collection5/22_11_12g.ann  \n",
      "  inflating: Collection5/22_11_12g.txt  \n",
      "  inflating: Collection5/22_11_12h.ann  \n",
      "  inflating: Collection5/22_11_12h.txt  \n",
      "  inflating: Collection5/22_11_12i.ann  \n",
      "  inflating: Collection5/22_11_12i.txt  \n",
      "  inflating: Collection5/22_11_12j.ann  \n",
      "  inflating: Collection5/22_11_12j.txt  \n",
      "  inflating: Collection5/230.ann     \n",
      "  inflating: Collection5/230.txt     \n",
      "  inflating: Collection5/231.ann     \n",
      "  inflating: Collection5/231.txt     \n",
      "  inflating: Collection5/232.ann     \n",
      "  inflating: Collection5/232.txt     \n",
      "  inflating: Collection5/233.ann     \n",
      "  inflating: Collection5/233.txt     \n",
      "  inflating: Collection5/234.ann     \n",
      "  inflating: Collection5/234.txt     \n",
      "  inflating: Collection5/235.ann     \n",
      "  inflating: Collection5/235.txt     \n",
      "  inflating: Collection5/236.ann     \n",
      "  inflating: Collection5/236.txt     \n",
      "  inflating: Collection5/237.ann     \n",
      "  inflating: Collection5/237.txt     \n",
      "  inflating: Collection5/238.ann     \n",
      "  inflating: Collection5/238.txt     \n",
      "  inflating: Collection5/239.ann     \n",
      "  inflating: Collection5/239.txt     \n",
      "  inflating: Collection5/23_11_12a.ann  \n",
      "  inflating: Collection5/23_11_12a.txt  \n",
      "  inflating: Collection5/23_11_12b.ann  \n",
      "  inflating: Collection5/23_11_12b.txt  \n",
      "  inflating: Collection5/23_11_12c.ann  \n",
      "  inflating: Collection5/23_11_12c.txt  \n",
      "  inflating: Collection5/23_11_12d.ann  \n",
      "  inflating: Collection5/23_11_12d.txt  \n",
      "  inflating: Collection5/23_11_12e.ann  \n",
      "  inflating: Collection5/23_11_12e.txt  \n",
      "  inflating: Collection5/23_11_12f.ann  \n",
      "  inflating: Collection5/23_11_12f.txt  \n",
      "  inflating: Collection5/240.ann     \n",
      "  inflating: Collection5/240.txt     \n",
      "  inflating: Collection5/241.ann     \n",
      "  inflating: Collection5/241.txt     \n",
      "  inflating: Collection5/242.ann     \n",
      "  inflating: Collection5/242.txt     \n",
      "  inflating: Collection5/243.ann     \n",
      "  inflating: Collection5/243.txt     \n",
      "  inflating: Collection5/244.ann     \n",
      "  inflating: Collection5/244.txt     \n",
      "  inflating: Collection5/245.ann     \n",
      "  inflating: Collection5/245.txt     \n",
      "  inflating: Collection5/246.ann     \n",
      "  inflating: Collection5/246.txt     \n",
      "  inflating: Collection5/247.ann     \n",
      "  inflating: Collection5/247.txt     \n",
      "  inflating: Collection5/248.ann     \n",
      "  inflating: Collection5/248.txt     \n",
      "  inflating: Collection5/249.ann     \n",
      "  inflating: Collection5/249.txt     \n",
      "  inflating: Collection5/250.ann     \n",
      "  inflating: Collection5/250.txt     \n",
      "  inflating: Collection5/251.ann     \n",
      "  inflating: Collection5/251.txt     \n",
      "  inflating: Collection5/252.ann     \n",
      "  inflating: Collection5/252.txt     \n",
      "  inflating: Collection5/253.ann     \n",
      "  inflating: Collection5/253.txt     \n",
      "  inflating: Collection5/254.ann     \n",
      "  inflating: Collection5/254.txt     \n",
      "  inflating: Collection5/255.ann     \n",
      "  inflating: Collection5/255.txt     \n",
      "  inflating: Collection5/256.ann     \n",
      "  inflating: Collection5/256.txt     \n",
      "  inflating: Collection5/257.ann     \n",
      "  inflating: Collection5/257.txt     \n",
      "  inflating: Collection5/258.ann     \n",
      "  inflating: Collection5/258.txt     \n",
      "  inflating: Collection5/259.ann     \n",
      "  inflating: Collection5/259.txt     \n",
      "  inflating: Collection5/25_12_12a.ann  \n",
      "  inflating: Collection5/25_12_12a.txt  \n",
      "  inflating: Collection5/25_12_12c.ann  \n",
      "  inflating: Collection5/25_12_12c.txt  \n",
      "  inflating: Collection5/25_12_12d.ann  \n",
      "  inflating: Collection5/25_12_12d.txt  \n",
      "  inflating: Collection5/25_12_12e.ann  \n",
      "  inflating: Collection5/25_12_12e.txt  \n",
      "  inflating: Collection5/260.ann     \n",
      "  inflating: Collection5/260.txt     \n",
      "  inflating: Collection5/261.ann     \n",
      "  inflating: Collection5/261.txt     \n",
      "  inflating: Collection5/262.ann     \n",
      "  inflating: Collection5/262.txt     \n",
      "  inflating: Collection5/263.ann     \n",
      "  inflating: Collection5/263.txt     \n",
      "  inflating: Collection5/264.ann     \n",
      "  inflating: Collection5/264.txt     \n",
      "  inflating: Collection5/265.ann     \n",
      "  inflating: Collection5/265.txt     \n",
      "  inflating: Collection5/266.ann     \n",
      "  inflating: Collection5/266.txt     \n",
      "  inflating: Collection5/267.ann     \n",
      "  inflating: Collection5/267.txt     \n",
      "  inflating: Collection5/268.ann     \n",
      "  inflating: Collection5/268.txt     \n",
      "  inflating: Collection5/269.ann     \n",
      "  inflating: Collection5/269.txt     \n",
      "  inflating: Collection5/26_11_12b.ann  \n",
      "  inflating: Collection5/26_11_12b.txt  \n",
      "  inflating: Collection5/26_11_12c.ann  \n",
      "  inflating: Collection5/26_11_12c.txt  \n",
      "  inflating: Collection5/26_11_12e.ann  \n",
      "  inflating: Collection5/26_11_12e.txt  \n",
      "  inflating: Collection5/26_11_12f.ann  \n",
      "  inflating: Collection5/26_11_12f.txt  \n",
      "  inflating: Collection5/270.ann     \n",
      "  inflating: Collection5/270.txt     \n",
      "  inflating: Collection5/271.ann     \n",
      "  inflating: Collection5/271.txt     \n",
      "  inflating: Collection5/272.ann     \n",
      "  inflating: Collection5/272.txt     \n",
      "  inflating: Collection5/273.ann     \n",
      "  inflating: Collection5/273.txt     \n",
      "  inflating: Collection5/274.ann     \n",
      "  inflating: Collection5/274.txt     \n",
      "  inflating: Collection5/275.ann     \n",
      "  inflating: Collection5/275.txt     \n",
      "  inflating: Collection5/276.ann     \n",
      "  inflating: Collection5/276.txt     \n",
      "  inflating: Collection5/277.ann     \n",
      "  inflating: Collection5/277.txt     \n",
      "  inflating: Collection5/278.ann     \n",
      "  inflating: Collection5/278.txt     \n",
      "  inflating: Collection5/279.ann     \n",
      "  inflating: Collection5/279.txt     \n",
      "  inflating: Collection5/27_11_12a.ann  \n",
      "  inflating: Collection5/27_11_12a.txt  \n",
      "  inflating: Collection5/27_11_12c.ann  \n",
      "  inflating: Collection5/27_11_12c.txt  \n",
      "  inflating: Collection5/27_11_12d.ann  \n",
      "  inflating: Collection5/27_11_12d.txt  \n",
      "  inflating: Collection5/27_11_12e.ann  \n",
      "  inflating: Collection5/27_11_12e.txt  \n",
      "  inflating: Collection5/27_11_12j.ann  \n",
      "  inflating: Collection5/27_11_12j.txt  \n",
      "  inflating: Collection5/280.ann     \n",
      "  inflating: Collection5/280.txt     \n",
      "  inflating: Collection5/281.ann     \n",
      "  inflating: Collection5/281.txt     \n",
      "  inflating: Collection5/282.ann     \n",
      "  inflating: Collection5/282.txt     \n",
      "  inflating: Collection5/283.ann     \n",
      "  inflating: Collection5/283.txt     \n",
      "  inflating: Collection5/284.ann     \n",
      "  inflating: Collection5/284.txt     \n",
      "  inflating: Collection5/285.ann     \n",
      "  inflating: Collection5/285.txt     \n",
      "  inflating: Collection5/286.ann     \n",
      "  inflating: Collection5/286.txt     \n",
      "  inflating: Collection5/287.ann     \n",
      "  inflating: Collection5/287.txt     \n",
      "  inflating: Collection5/288.ann     \n",
      "  inflating: Collection5/288.txt     \n",
      "  inflating: Collection5/289.ann     \n",
      "  inflating: Collection5/289.txt     \n",
      "  inflating: Collection5/28_11_12a.ann  \n",
      "  inflating: Collection5/28_11_12a.txt  \n",
      "  inflating: Collection5/28_11_12f.ann  \n",
      "  inflating: Collection5/28_11_12f.txt  \n",
      "  inflating: Collection5/28_11_12g.ann  \n",
      "  inflating: Collection5/28_11_12g.txt  \n",
      "  inflating: Collection5/28_11_12h.ann  \n",
      "  inflating: Collection5/28_11_12h.txt  \n",
      "  inflating: Collection5/28_11_12i.ann  \n",
      "  inflating: Collection5/28_11_12i.txt  \n",
      "  inflating: Collection5/28_11_12j.ann  \n",
      "  inflating: Collection5/28_11_12j.txt  \n",
      "  inflating: Collection5/290.ann     \n",
      "  inflating: Collection5/290.txt     \n",
      "  inflating: Collection5/291.ann     \n",
      "  inflating: Collection5/291.txt     \n",
      "  inflating: Collection5/292.ann     \n",
      "  inflating: Collection5/292.txt     \n",
      "  inflating: Collection5/293.ann     \n",
      "  inflating: Collection5/293.txt     \n",
      "  inflating: Collection5/294.ann     \n",
      "  inflating: Collection5/294.txt     \n",
      "  inflating: Collection5/295.ann     \n",
      "  inflating: Collection5/295.txt     \n",
      "  inflating: Collection5/296.ann     \n",
      "  inflating: Collection5/296.txt     \n",
      "  inflating: Collection5/297.ann     \n",
      "  inflating: Collection5/297.txt     \n",
      "  inflating: Collection5/298.ann     \n",
      "  inflating: Collection5/298.txt     \n",
      "  inflating: Collection5/299.ann     \n",
      "  inflating: Collection5/299.txt     \n",
      "  inflating: Collection5/29_11_12a.ann  \n",
      "  inflating: Collection5/29_11_12a.txt  \n",
      "  inflating: Collection5/29_11_12b.ann  \n",
      "  inflating: Collection5/29_11_12b.txt  \n",
      "  inflating: Collection5/300.ann     \n",
      "  inflating: Collection5/300.txt     \n",
      "  inflating: Collection5/301.ann     \n",
      "  inflating: Collection5/301.txt     \n",
      "  inflating: Collection5/302.ann     \n",
      "  inflating: Collection5/302.txt     \n",
      "  inflating: Collection5/303.ann     \n",
      "  inflating: Collection5/303.txt     \n",
      "  inflating: Collection5/304.ann     \n",
      "  inflating: Collection5/304.txt     \n",
      "  inflating: Collection5/305.ann     \n",
      "  inflating: Collection5/305.txt     \n",
      "  inflating: Collection5/306.ann     \n",
      "  inflating: Collection5/306.txt     \n",
      "  inflating: Collection5/307.ann     \n",
      "  inflating: Collection5/307.txt     \n",
      "  inflating: Collection5/308.ann     \n",
      "  inflating: Collection5/308.txt     \n",
      "  inflating: Collection5/309.ann     \n",
      "  inflating: Collection5/309.txt     \n",
      "  inflating: Collection5/30_11_12b.ann  \n",
      "  inflating: Collection5/30_11_12b.txt  \n",
      "  inflating: Collection5/30_11_12h.ann  \n",
      "  inflating: Collection5/30_11_12h.txt  \n",
      "  inflating: Collection5/30_11_12i.ann  \n",
      "  inflating: Collection5/30_11_12i.txt  \n",
      "  inflating: Collection5/310.ann     \n",
      "  inflating: Collection5/310.txt     \n",
      "  inflating: Collection5/311.ann     \n",
      "  inflating: Collection5/311.txt     \n",
      "  inflating: Collection5/312.ann     \n",
      "  inflating: Collection5/312.txt     \n",
      "  inflating: Collection5/313.ann     \n",
      "  inflating: Collection5/313.txt     \n",
      "  inflating: Collection5/314.ann     \n",
      "  inflating: Collection5/314.txt     \n",
      "  inflating: Collection5/315.ann     \n",
      "  inflating: Collection5/315.txt     \n",
      "  inflating: Collection5/316.ann     \n",
      "  inflating: Collection5/316.txt     \n",
      "  inflating: Collection5/317.ann     \n",
      "  inflating: Collection5/317.txt     \n",
      "  inflating: Collection5/318.ann     \n",
      "  inflating: Collection5/318.txt     \n",
      "  inflating: Collection5/319.ann     \n",
      "  inflating: Collection5/319.txt     \n",
      "  inflating: Collection5/320.ann     \n",
      "  inflating: Collection5/320.txt     \n",
      "  inflating: Collection5/321.ann     \n",
      "  inflating: Collection5/321.txt     \n",
      "  inflating: Collection5/322.ann     \n",
      "  inflating: Collection5/322.txt     \n",
      "  inflating: Collection5/323.ann     \n",
      "  inflating: Collection5/323.txt     \n",
      "  inflating: Collection5/324.ann     \n",
      "  inflating: Collection5/324.txt     \n",
      "  inflating: Collection5/325.ann     \n",
      "  inflating: Collection5/325.txt     \n",
      "  inflating: Collection5/326.ann     \n",
      "  inflating: Collection5/326.txt     \n",
      "  inflating: Collection5/327.ann     \n",
      "  inflating: Collection5/327.txt     \n",
      "  inflating: Collection5/328.ann     \n",
      "  inflating: Collection5/328.txt     \n",
      "  inflating: Collection5/329.ann     \n",
      "  inflating: Collection5/329.txt     \n",
      "  inflating: Collection5/330.ann     \n",
      "  inflating: Collection5/330.txt     \n",
      "  inflating: Collection5/331.ann     \n",
      "  inflating: Collection5/331.txt     \n",
      "  inflating: Collection5/332.ann     \n",
      "  inflating: Collection5/332.txt     \n",
      "  inflating: Collection5/333.ann     \n",
      "  inflating: Collection5/333.txt     \n",
      "  inflating: Collection5/334.ann     \n",
      "  inflating: Collection5/334.txt     \n",
      "  inflating: Collection5/335.ann     \n",
      "  inflating: Collection5/335.txt     \n",
      "  inflating: Collection5/336.ann     \n",
      "  inflating: Collection5/336.txt     \n",
      "  inflating: Collection5/337.ann     \n",
      "  inflating: Collection5/337.txt     \n",
      "  inflating: Collection5/338.ann     \n",
      "  inflating: Collection5/338.txt     \n",
      "  inflating: Collection5/339.ann     \n",
      "  inflating: Collection5/339.txt     \n",
      "  inflating: Collection5/340.ann     \n",
      "  inflating: Collection5/340.txt     \n",
      "  inflating: Collection5/341.ann     \n",
      "  inflating: Collection5/341.txt     \n",
      "  inflating: Collection5/342.ann     \n",
      "  inflating: Collection5/342.txt     \n",
      "  inflating: Collection5/343.ann     \n",
      "  inflating: Collection5/343.txt     \n",
      "  inflating: Collection5/344.ann     \n",
      "  inflating: Collection5/344.txt     \n",
      "  inflating: Collection5/345.ann     \n",
      "  inflating: Collection5/345.txt     \n",
      "  inflating: Collection5/346.ann     \n",
      "  inflating: Collection5/346.txt     \n",
      "  inflating: Collection5/347.ann     \n",
      "  inflating: Collection5/347.txt     \n",
      "  inflating: Collection5/348.ann     \n",
      "  inflating: Collection5/348.txt     \n",
      "  inflating: Collection5/349.ann     \n",
      "  inflating: Collection5/349.txt     \n",
      "  inflating: Collection5/350.ann     \n",
      "  inflating: Collection5/350.txt     \n",
      "  inflating: Collection5/351.ann     \n",
      "  inflating: Collection5/351.txt     \n",
      "  inflating: Collection5/352.ann     \n",
      "  inflating: Collection5/352.txt     \n",
      "  inflating: Collection5/353.ann     \n",
      "  inflating: Collection5/353.txt     \n",
      "  inflating: Collection5/354.ann     \n",
      "  inflating: Collection5/354.txt     \n",
      "  inflating: Collection5/355.ann     \n",
      "  inflating: Collection5/355.txt     \n",
      "  inflating: Collection5/356.ann     \n",
      "  inflating: Collection5/356.txt     \n",
      "  inflating: Collection5/357.ann     \n",
      "  inflating: Collection5/357.txt     \n",
      "  inflating: Collection5/358.ann     \n",
      "  inflating: Collection5/358.txt     \n",
      "  inflating: Collection5/359.ann     \n",
      "  inflating: Collection5/359.txt     \n",
      "  inflating: Collection5/360.ann     \n",
      "  inflating: Collection5/360.txt     \n",
      "  inflating: Collection5/361.ann     \n",
      "  inflating: Collection5/361.txt     \n",
      "  inflating: Collection5/362.ann     \n",
      "  inflating: Collection5/362.txt     \n",
      "  inflating: Collection5/363.ann     \n",
      "  inflating: Collection5/363.txt     \n",
      "  inflating: Collection5/364.ann     \n",
      "  inflating: Collection5/364.txt     \n",
      "  inflating: Collection5/365.ann     \n",
      "  inflating: Collection5/365.txt     \n",
      "  inflating: Collection5/366.ann     \n",
      "  inflating: Collection5/366.txt     \n",
      "  inflating: Collection5/367.ann     \n",
      "  inflating: Collection5/367.txt     \n",
      "  inflating: Collection5/368.ann     \n",
      "  inflating: Collection5/368.txt     \n",
      "  inflating: Collection5/369.ann     \n",
      "  inflating: Collection5/369.txt     \n",
      "  inflating: Collection5/370.ann     \n",
      "  inflating: Collection5/370.txt     \n",
      "  inflating: Collection5/371.ann     \n",
      "  inflating: Collection5/371.txt     \n",
      "  inflating: Collection5/372.ann     \n",
      "  inflating: Collection5/372.txt     \n",
      "  inflating: Collection5/373.ann     \n",
      "  inflating: Collection5/373.txt     \n",
      "  inflating: Collection5/374.ann     \n",
      "  inflating: Collection5/374.txt     \n",
      "  inflating: Collection5/375.ann     \n",
      "  inflating: Collection5/375.txt     \n",
      "  inflating: Collection5/376.ann     \n",
      "  inflating: Collection5/376.txt     \n",
      "  inflating: Collection5/377.ann     \n",
      "  inflating: Collection5/377.txt     \n",
      "  inflating: Collection5/378.ann     \n",
      "  inflating: Collection5/378.txt     \n",
      "  inflating: Collection5/379.ann     \n",
      "  inflating: Collection5/379.txt     \n",
      "  inflating: Collection5/380.ann     \n",
      "  inflating: Collection5/380.txt     \n",
      "  inflating: Collection5/381.ann     \n",
      "  inflating: Collection5/381.txt     \n",
      "  inflating: Collection5/382.ann     \n",
      "  inflating: Collection5/382.txt     \n",
      "  inflating: Collection5/383.ann     \n",
      "  inflating: Collection5/383.txt     \n",
      "  inflating: Collection5/384.ann     \n",
      "  inflating: Collection5/384.txt     \n",
      "  inflating: Collection5/385.ann     \n",
      "  inflating: Collection5/385.txt     \n",
      "  inflating: Collection5/386.ann     \n",
      "  inflating: Collection5/386.txt     \n",
      "  inflating: Collection5/387.ann     \n",
      "  inflating: Collection5/387.txt     \n",
      "  inflating: Collection5/388.ann     \n",
      "  inflating: Collection5/388.txt     \n",
      "  inflating: Collection5/389.ann     \n",
      "  inflating: Collection5/389.txt     \n",
      "  inflating: Collection5/390.ann     \n",
      "  inflating: Collection5/390.txt     \n",
      "  inflating: Collection5/391.ann     \n",
      "  inflating: Collection5/391.txt     \n",
      "  inflating: Collection5/392.ann     \n",
      "  inflating: Collection5/392.txt     \n",
      "  inflating: Collection5/393.ann     \n",
      "  inflating: Collection5/393.txt     \n",
      "  inflating: Collection5/394.ann     \n",
      "  inflating: Collection5/394.txt     \n",
      "  inflating: Collection5/395.ann     \n",
      "  inflating: Collection5/395.txt     \n",
      "  inflating: Collection5/396.ann     \n",
      "  inflating: Collection5/396.txt     \n",
      "  inflating: Collection5/397.ann     \n",
      "  inflating: Collection5/397.txt     \n",
      "  inflating: Collection5/398.ann     \n",
      "  inflating: Collection5/398.txt     \n",
      "  inflating: Collection5/399.ann     \n",
      "  inflating: Collection5/399.txt     \n",
      "  inflating: Collection5/400.ann     \n",
      "  inflating: Collection5/400.txt     \n",
      "  inflating: Collection5/401.ann     \n",
      "  inflating: Collection5/401.txt     \n",
      "  inflating: Collection5/402.ann     \n",
      "  inflating: Collection5/402.txt     \n",
      "  inflating: Collection5/403.ann     \n",
      "  inflating: Collection5/403.txt     \n",
      "  inflating: Collection5/404.ann     \n",
      "  inflating: Collection5/404.txt     \n",
      "  inflating: Collection5/405.ann     \n",
      "  inflating: Collection5/405.txt     \n",
      "  inflating: Collection5/406.ann     \n",
      "  inflating: Collection5/406.txt     \n",
      "  inflating: Collection5/407.ann     \n",
      "  inflating: Collection5/407.txt     \n",
      "  inflating: Collection5/408.ann     \n",
      "  inflating: Collection5/408.txt     \n",
      "  inflating: Collection5/409.ann     \n",
      "  inflating: Collection5/409.txt     \n",
      "  inflating: Collection5/410.ann     \n",
      "  inflating: Collection5/410.txt     \n",
      "  inflating: Collection5/411.ann     \n",
      "  inflating: Collection5/411.txt     \n",
      "  inflating: Collection5/412.ann     \n",
      "  inflating: Collection5/412.txt     \n",
      "  inflating: Collection5/413.ann     \n",
      "  inflating: Collection5/413.txt     \n",
      "  inflating: Collection5/414.ann     \n",
      "  inflating: Collection5/414.txt     \n",
      "  inflating: Collection5/415.ann     \n",
      "  inflating: Collection5/415.txt     \n",
      "  inflating: Collection5/416.ann     \n",
      "  inflating: Collection5/416.txt     \n",
      "  inflating: Collection5/417.ann     \n",
      "  inflating: Collection5/417.txt     \n",
      "  inflating: Collection5/418.ann     \n",
      "  inflating: Collection5/418.txt     \n",
      "  inflating: Collection5/419.ann     \n",
      "  inflating: Collection5/419.txt     \n",
      "  inflating: Collection5/420.ann     \n",
      "  inflating: Collection5/420.txt     \n",
      "  inflating: Collection5/421.ann     \n",
      "  inflating: Collection5/421.txt     \n",
      "  inflating: Collection5/422.ann     \n",
      "  inflating: Collection5/422.txt     \n",
      "  inflating: Collection5/423.ann     \n",
      "  inflating: Collection5/423.txt     \n",
      "  inflating: Collection5/424.ann     \n",
      "  inflating: Collection5/424.txt     \n",
      "  inflating: Collection5/425.ann     \n",
      "  inflating: Collection5/425.txt     \n",
      "  inflating: Collection5/426.ann     \n",
      "  inflating: Collection5/426.txt     \n",
      "  inflating: Collection5/427.ann     \n",
      "  inflating: Collection5/427.txt     \n",
      "  inflating: Collection5/428.ann     \n",
      "  inflating: Collection5/428.txt     \n",
      "  inflating: Collection5/429.ann     \n",
      "  inflating: Collection5/429.txt     \n",
      "  inflating: Collection5/430.ann     \n",
      "  inflating: Collection5/430.txt     \n",
      "  inflating: Collection5/431.ann     \n",
      "  inflating: Collection5/431.txt     \n",
      "  inflating: Collection5/432.ann     \n",
      "  inflating: Collection5/432.txt     \n",
      "  inflating: Collection5/433.ann     \n",
      "  inflating: Collection5/433.txt     \n",
      "  inflating: Collection5/434.ann     \n",
      "  inflating: Collection5/434.txt     \n",
      "  inflating: Collection5/435.ann     \n",
      "  inflating: Collection5/435.txt     \n",
      "  inflating: Collection5/436.ann     \n",
      "  inflating: Collection5/436.txt     \n",
      "  inflating: Collection5/437.ann     \n",
      "  inflating: Collection5/437.txt     \n",
      "  inflating: Collection5/438.ann     \n",
      "  inflating: Collection5/438.txt     \n",
      "  inflating: Collection5/439.ann     \n",
      "  inflating: Collection5/439.txt     \n",
      "  inflating: Collection5/440.ann     \n",
      "  inflating: Collection5/440.txt     \n",
      "  inflating: Collection5/441.ann     \n",
      "  inflating: Collection5/441.txt     \n",
      "  inflating: Collection5/442.ann     \n",
      "  inflating: Collection5/442.txt     \n",
      "  inflating: Collection5/443.ann     \n",
      "  inflating: Collection5/443.txt     \n",
      "  inflating: Collection5/444.ann     \n",
      "  inflating: Collection5/444.txt     \n",
      "  inflating: Collection5/445.ann     \n",
      "  inflating: Collection5/445.txt     \n",
      "  inflating: Collection5/446.ann     \n",
      "  inflating: Collection5/446.txt     \n",
      "  inflating: Collection5/447.ann     \n",
      "  inflating: Collection5/447.txt     \n",
      "  inflating: Collection5/448.ann     \n",
      "  inflating: Collection5/448.txt     \n",
      "  inflating: Collection5/449.ann     \n",
      "  inflating: Collection5/449.txt     \n",
      "  inflating: Collection5/450.ann     \n",
      "  inflating: Collection5/450.txt     \n",
      "  inflating: Collection5/451.ann     \n",
      "  inflating: Collection5/451.txt     \n",
      "  inflating: Collection5/452.ann     \n",
      "  inflating: Collection5/452.txt     \n",
      "  inflating: Collection5/453.ann     \n",
      "  inflating: Collection5/453.txt     \n",
      "  inflating: Collection5/454.ann     \n",
      "  inflating: Collection5/454.txt     \n",
      "  inflating: Collection5/455.ann     \n",
      "  inflating: Collection5/455.txt     \n",
      "  inflating: Collection5/457.ann     \n",
      "  inflating: Collection5/457.txt     \n",
      "  inflating: Collection5/458.ann     \n",
      "  inflating: Collection5/458.txt     \n",
      "  inflating: Collection5/459.ann     \n",
      "  inflating: Collection5/459.txt     \n",
      "  inflating: Collection5/460.ann     \n",
      "  inflating: Collection5/460.txt     \n",
      "  inflating: Collection5/461.ann     \n",
      "  inflating: Collection5/461.txt     \n",
      "  inflating: Collection5/462.ann     \n",
      "  inflating: Collection5/462.txt     \n",
      "  inflating: Collection5/463.ann     \n",
      "  inflating: Collection5/463.txt     \n",
      "  inflating: Collection5/464.ann     \n",
      "  inflating: Collection5/464.txt     \n",
      "  inflating: Collection5/465.ann     \n",
      "  inflating: Collection5/465.txt     \n",
      "  inflating: Collection5/466.ann     \n",
      "  inflating: Collection5/466.txt     \n",
      "  inflating: Collection5/467.ann     \n",
      "  inflating: Collection5/467.txt     \n",
      "  inflating: Collection5/468.ann     \n",
      "  inflating: Collection5/468.txt     \n",
      "  inflating: Collection5/469.ann     \n",
      "  inflating: Collection5/469.txt     \n",
      "  inflating: Collection5/470.ann     \n",
      "  inflating: Collection5/470.txt     \n",
      "  inflating: Collection5/471.ann     \n",
      "  inflating: Collection5/471.txt     \n",
      "  inflating: Collection5/472.ann     \n",
      "  inflating: Collection5/472.txt     \n",
      "  inflating: Collection5/473.ann     \n",
      "  inflating: Collection5/473.txt     \n",
      "  inflating: Collection5/474.ann     \n",
      "  inflating: Collection5/474.txt     \n",
      "  inflating: Collection5/475.ann     \n",
      "  inflating: Collection5/475.txt     \n",
      "  inflating: Collection5/476.ann     \n",
      "  inflating: Collection5/476.txt     \n",
      "  inflating: Collection5/477.ann     \n",
      "  inflating: Collection5/477.txt     \n",
      "  inflating: Collection5/478.ann     \n",
      "  inflating: Collection5/478.txt     \n",
      "  inflating: Collection5/479.ann     \n",
      "  inflating: Collection5/479.txt     \n",
      "  inflating: Collection5/480.ann     \n",
      "  inflating: Collection5/480.txt     \n",
      "  inflating: Collection5/481.ann     \n",
      "  inflating: Collection5/481.txt     \n",
      "  inflating: Collection5/482.ann     \n",
      "  inflating: Collection5/482.txt     \n",
      "  inflating: Collection5/483.ann     \n",
      "  inflating: Collection5/483.txt     \n",
      "  inflating: Collection5/484.ann     \n",
      "  inflating: Collection5/484.txt     \n",
      "  inflating: Collection5/485.ann     \n",
      "  inflating: Collection5/485.txt     \n",
      "  inflating: Collection5/486.ann     \n",
      "  inflating: Collection5/486.txt     \n",
      "  inflating: Collection5/487.ann     \n",
      "  inflating: Collection5/487.txt     \n",
      "  inflating: Collection5/488.ann     \n",
      "  inflating: Collection5/488.txt     \n",
      "  inflating: Collection5/489.ann     \n",
      "  inflating: Collection5/489.txt     \n",
      "  inflating: Collection5/490.ann     \n",
      "  inflating: Collection5/490.txt     \n",
      "  inflating: Collection5/491.ann     \n",
      "  inflating: Collection5/491.txt     \n",
      "  inflating: Collection5/492.ann     \n",
      "  inflating: Collection5/492.txt     \n",
      "  inflating: Collection5/493.ann     \n",
      "  inflating: Collection5/493.txt     \n",
      "  inflating: Collection5/494.ann     \n",
      "  inflating: Collection5/494.txt     \n",
      "  inflating: Collection5/495.ann     \n",
      "  inflating: Collection5/495.txt     \n",
      "  inflating: Collection5/496.ann     \n",
      "  inflating: Collection5/496.txt     \n",
      "  inflating: Collection5/497.ann     \n",
      "  inflating: Collection5/497.txt     \n",
      "  inflating: Collection5/498.ann     \n",
      "  inflating: Collection5/498.txt     \n",
      "  inflating: Collection5/499.ann     \n",
      "  inflating: Collection5/499.txt     \n",
      "  inflating: Collection5/500.ann     \n",
      "  inflating: Collection5/500.txt     \n",
      "  inflating: Collection5/501.ann     \n",
      "  inflating: Collection5/501.txt     \n",
      "  inflating: Collection5/502.ann     \n",
      "  inflating: Collection5/502.txt     \n",
      "  inflating: Collection5/503.ann     \n",
      "  inflating: Collection5/503.txt     \n",
      "  inflating: Collection5/504.ann     \n",
      "  inflating: Collection5/504.txt     \n",
      "  inflating: Collection5/505.ann     \n",
      "  inflating: Collection5/505.txt     \n",
      "  inflating: Collection5/506.ann     \n",
      "  inflating: Collection5/506.txt     \n",
      "  inflating: Collection5/507.ann     \n",
      "  inflating: Collection5/507.txt     \n",
      "  inflating: Collection5/508.ann     \n",
      "  inflating: Collection5/508.txt     \n",
      "  inflating: Collection5/509.ann     \n",
      "  inflating: Collection5/509.txt     \n",
      "  inflating: Collection5/510.ann     \n",
      "  inflating: Collection5/510.txt     \n",
      "  inflating: Collection5/511.ann     \n",
      "  inflating: Collection5/511.txt     \n",
      "  inflating: Collection5/512.ann     \n",
      "  inflating: Collection5/512.txt     \n",
      "  inflating: Collection5/513.ann     \n",
      "  inflating: Collection5/513.txt     \n",
      "  inflating: Collection5/514.ann     \n",
      "  inflating: Collection5/514.txt     \n",
      "  inflating: Collection5/515.ann     \n",
      "  inflating: Collection5/515.txt     \n",
      "  inflating: Collection5/516.ann     \n",
      "  inflating: Collection5/516.txt     \n",
      "  inflating: Collection5/517.ann     \n",
      "  inflating: Collection5/517.txt     \n",
      "  inflating: Collection5/518.ann     \n",
      "  inflating: Collection5/518.txt     \n",
      "  inflating: Collection5/519.ann     \n",
      "  inflating: Collection5/519.txt     \n",
      "  inflating: Collection5/520.ann     \n",
      "  inflating: Collection5/520.txt     \n",
      "  inflating: Collection5/521.ann     \n",
      "  inflating: Collection5/521.txt     \n",
      "  inflating: Collection5/522.ann     \n",
      "  inflating: Collection5/522.txt     \n",
      "  inflating: Collection5/523.ann     \n",
      "  inflating: Collection5/523.txt     \n",
      "  inflating: Collection5/524.ann     \n",
      "  inflating: Collection5/524.txt     \n",
      "  inflating: Collection5/525.ann     \n",
      "  inflating: Collection5/525.txt     \n",
      "  inflating: Collection5/526.ann     \n",
      "  inflating: Collection5/526.txt     \n",
      "  inflating: Collection5/527.ann     \n",
      "  inflating: Collection5/527.txt     \n",
      "  inflating: Collection5/528.ann     \n",
      "  inflating: Collection5/528.txt     \n",
      "  inflating: Collection5/529.ann     \n",
      "  inflating: Collection5/529.txt     \n",
      "  inflating: Collection5/530.ann     \n",
      "  inflating: Collection5/530.txt     \n",
      "  inflating: Collection5/531.ann     \n",
      "  inflating: Collection5/531.txt     \n",
      "  inflating: Collection5/532.ann     \n",
      "  inflating: Collection5/532.txt     \n",
      "  inflating: Collection5/533 (!).ann  \n",
      "  inflating: Collection5/533 (!).txt  \n",
      "  inflating: Collection5/534.ann     \n",
      "  inflating: Collection5/534.txt     \n",
      "  inflating: Collection5/535.ann     \n",
      "  inflating: Collection5/535.txt     \n",
      "  inflating: Collection5/536.ann     \n",
      "  inflating: Collection5/536.txt     \n",
      "  inflating: Collection5/537.ann     \n",
      "  inflating: Collection5/537.txt     \n",
      "  inflating: Collection5/538.ann     \n",
      "  inflating: Collection5/538.txt     \n",
      "  inflating: Collection5/539.ann     \n",
      "  inflating: Collection5/539.txt     \n",
      "  inflating: Collection5/540.ann     \n",
      "  inflating: Collection5/540.txt     \n",
      "  inflating: Collection5/541.ann     \n",
      "  inflating: Collection5/541.txt     \n",
      "  inflating: Collection5/542.ann     \n",
      "  inflating: Collection5/542.txt     \n",
      "  inflating: Collection5/543.ann     \n",
      "  inflating: Collection5/543.txt     \n",
      "  inflating: Collection5/544.ann     \n",
      "  inflating: Collection5/544.txt     \n",
      "  inflating: Collection5/545.ann     \n",
      "  inflating: Collection5/545.txt     \n",
      "  inflating: Collection5/546.ann     \n",
      "  inflating: Collection5/546.txt     \n",
      "  inflating: Collection5/547.ann     \n",
      "  inflating: Collection5/547.txt     \n",
      "  inflating: Collection5/548.ann     \n",
      "  inflating: Collection5/548.txt     \n",
      "  inflating: Collection5/549.ann     \n",
      "  inflating: Collection5/549.txt     \n",
      "  inflating: Collection5/550.ann     \n",
      "  inflating: Collection5/550.txt     \n",
      "  inflating: Collection5/551.ann     \n",
      "  inflating: Collection5/551.txt     \n",
      "  inflating: Collection5/552.ann     \n",
      "  inflating: Collection5/552.txt     \n",
      "  inflating: Collection5/553.ann     \n",
      "  inflating: Collection5/553.txt     \n",
      "  inflating: Collection5/554.ann     \n",
      "  inflating: Collection5/554.txt     \n",
      "  inflating: Collection5/555 (!).ann  \n",
      "  inflating: Collection5/555 (!).txt  \n",
      "  inflating: Collection5/556.ann     \n",
      "  inflating: Collection5/556.txt     \n",
      "  inflating: Collection5/557.ann     \n",
      "  inflating: Collection5/557.txt     \n",
      "  inflating: Collection5/558.ann     \n",
      "  inflating: Collection5/558.txt     \n",
      "  inflating: Collection5/559.ann     \n",
      "  inflating: Collection5/559.txt     \n",
      "  inflating: Collection5/560.ann     \n",
      "  inflating: Collection5/560.txt     \n",
      "  inflating: Collection5/561.ann     \n",
      "  inflating: Collection5/561.txt     \n",
      "  inflating: Collection5/562.ann     \n",
      "  inflating: Collection5/562.txt     \n",
      "  inflating: Collection5/563.ann     \n",
      "  inflating: Collection5/563.txt     \n",
      "  inflating: Collection5/564.ann     \n",
      "  inflating: Collection5/564.txt     \n",
      "  inflating: Collection5/565.ann     \n",
      "  inflating: Collection5/565.txt     \n",
      "  inflating: Collection5/567.ann     \n",
      "  inflating: Collection5/567.txt     \n",
      "  inflating: Collection5/568.ann     \n",
      "  inflating: Collection5/568.txt     \n",
      "  inflating: Collection5/569.ann     \n",
      "  inflating: Collection5/569.txt     \n",
      "  inflating: Collection5/570.ann     \n",
      "  inflating: Collection5/570.txt     \n",
      "  inflating: Collection5/571.ann     \n",
      "  inflating: Collection5/571.txt     \n",
      "  inflating: Collection5/572.ann     \n",
      "  inflating: Collection5/572.txt     \n",
      "  inflating: Collection5/574.ann     \n",
      "  inflating: Collection5/574.txt     \n",
      "  inflating: Collection5/575.ann     \n",
      "  inflating: Collection5/575.txt     \n",
      "  inflating: Collection5/576.ann     \n",
      "  inflating: Collection5/576.txt     \n",
      "  inflating: Collection5/577.ann     \n",
      "  inflating: Collection5/577.txt     \n",
      "  inflating: Collection5/578.ann     \n",
      "  inflating: Collection5/578.txt     \n",
      "  inflating: Collection5/579.ann     \n",
      "  inflating: Collection5/579.txt     \n",
      "  inflating: Collection5/581.ann     \n",
      "  inflating: Collection5/581.txt     \n",
      "  inflating: Collection5/582.ann     \n",
      "  inflating: Collection5/582.txt     \n",
      "  inflating: Collection5/583.ann     \n",
      "  inflating: Collection5/583.txt     \n",
      "  inflating: Collection5/584 (!).ann  \n",
      "  inflating: Collection5/584 (!).txt  \n",
      "  inflating: Collection5/585.ann     \n",
      "  inflating: Collection5/585.txt     \n",
      "  inflating: Collection5/586.ann     \n",
      "  inflating: Collection5/586.txt     \n",
      "  inflating: Collection5/587.ann     \n",
      "  inflating: Collection5/587.txt     \n",
      "  inflating: Collection5/588.ann     \n",
      "  inflating: Collection5/588.txt     \n",
      "  inflating: Collection5/589.ann     \n",
      "  inflating: Collection5/589.txt     \n",
      "  inflating: Collection5/590.ann     \n",
      "  inflating: Collection5/590.txt     \n",
      "  inflating: Collection5/591.ann     \n",
      "  inflating: Collection5/591.txt     \n",
      "  inflating: Collection5/592.ann     \n",
      "  inflating: Collection5/592.txt     \n",
      "  inflating: Collection5/593.ann     \n",
      "  inflating: Collection5/593.txt     \n",
      "  inflating: Collection5/594.ann     \n",
      "  inflating: Collection5/594.txt     \n",
      "  inflating: Collection5/595.ann     \n",
      "  inflating: Collection5/595.txt     \n",
      "  inflating: Collection5/596.ann     \n",
      "  inflating: Collection5/596.txt     \n",
      "  inflating: Collection5/597.ann     \n",
      "  inflating: Collection5/597.txt     \n",
      "  inflating: Collection5/598 (!).ann  \n",
      "  inflating: Collection5/598 (!).txt  \n",
      "  inflating: Collection5/599.ann     \n",
      "  inflating: Collection5/599.txt     \n",
      "  inflating: Collection5/600.ann     \n",
      "  inflating: Collection5/600.txt     \n",
      "  inflating: Collection5/601.ann     \n",
      "  inflating: Collection5/601.txt     \n",
      "  inflating: Collection5/602.ann     \n",
      "  inflating: Collection5/602.txt     \n",
      "  inflating: Collection5/610.ann     \n",
      "  inflating: Collection5/610.txt     \n",
      "  inflating: Collection5/611.ann     \n",
      "  inflating: Collection5/611.txt     \n",
      "  inflating: Collection5/612.ann     \n",
      "  inflating: Collection5/612.txt     \n",
      "  inflating: Collection5/613.ann     \n",
      "  inflating: Collection5/613.txt     \n",
      "  inflating: Collection5/614.ann     \n",
      "  inflating: Collection5/614.txt     \n",
      "  inflating: Collection5/615.ann     \n",
      "  inflating: Collection5/615.txt     \n",
      "  inflating: Collection5/616.ann     \n",
      "  inflating: Collection5/616.txt     \n",
      "  inflating: Collection5/617.ann     \n",
      "  inflating: Collection5/617.txt     \n",
      "  inflating: Collection5/618.ann     \n",
      "  inflating: Collection5/618.txt     \n",
      "  inflating: Collection5/619.ann     \n",
      "  inflating: Collection5/619.txt     \n",
      "  inflating: Collection5/620.ann     \n",
      "  inflating: Collection5/620.txt     \n",
      "  inflating: Collection5/621.ann     \n",
      "  inflating: Collection5/621.txt     \n",
      "  inflating: Collection5/622.ann     \n",
      "  inflating: Collection5/622.txt     \n",
      "  inflating: Collection5/623.ann     \n",
      "  inflating: Collection5/623.txt     \n",
      "  inflating: Collection5/624.ann     \n",
      "  inflating: Collection5/624.txt     \n",
      "  inflating: Collection5/625.ann     \n",
      "  inflating: Collection5/625.txt     \n",
      "  inflating: Collection5/626.ann     \n",
      "  inflating: Collection5/626.txt     \n",
      "  inflating: Collection5/627.ann     \n",
      "  inflating: Collection5/627.txt     \n",
      "  inflating: Collection5/628.ann     \n",
      "  inflating: Collection5/628.txt     \n",
      "  inflating: Collection5/629.ann     \n",
      "  inflating: Collection5/629.txt     \n",
      "  inflating: Collection5/630.ann     \n",
      "  inflating: Collection5/630.txt     \n",
      "  inflating: Collection5/631.ann     \n",
      "  inflating: Collection5/631.txt     \n",
      "  inflating: Collection5/632.ann     \n",
      "  inflating: Collection5/632.txt     \n",
      "  inflating: Collection5/633.ann     \n",
      "  inflating: Collection5/633.txt     \n",
      "  inflating: Collection5/abdulatipov.ann  \n",
      "  inflating: Collection5/abdulatipov.txt  \n",
      "  inflating: Collection5/artjakov.ann  \n",
      "  inflating: Collection5/artjakov.txt  \n",
      "  inflating: Collection5/Avtovaz.ann  \n",
      "  inflating: Collection5/Avtovaz.txt  \n",
      "  inflating: Collection5/blokhin.ann  \n",
      "  inflating: Collection5/blokhin.txt  \n",
      "  inflating: Collection5/chaves.ann  \n",
      "  inflating: Collection5/chaves.txt  \n",
      "  inflating: Collection5/chirkunov.ann  \n",
      "  inflating: Collection5/chirkunov.txt  \n",
      "  inflating: Collection5/kamchatka.ann  \n",
      "  inflating: Collection5/kamchatka.txt  \n",
      "  inflating: Collection5/klinton.ann  \n",
      "  inflating: Collection5/klinton.txt  \n",
      "  inflating: Collection5/kuleshov.ann  \n",
      "  inflating: Collection5/kuleshov.txt  \n",
      "  inflating: Collection5/last_01.ann  \n",
      "  inflating: Collection5/last_01.txt  \n",
      "  inflating: Collection5/last_02.ann  \n",
      "  inflating: Collection5/last_02.txt  \n",
      "  inflating: Collection5/last_03.ann  \n",
      "  inflating: Collection5/last_03.txt  \n",
      "  inflating: Collection5/last_04.ann  \n",
      "  inflating: Collection5/last_04.txt  \n",
      "  inflating: Collection5/last_05.ann  \n",
      "  inflating: Collection5/last_05.txt  \n",
      "  inflating: Collection5/last_06.ann  \n",
      "  inflating: Collection5/last_06.txt  \n",
      "  inflating: Collection5/last_07_new.ann  \n",
      "  inflating: Collection5/last_07_new.txt  \n",
      "  inflating: Collection5/last_08.ann  \n",
      "  inflating: Collection5/last_08.txt  \n",
      "  inflating: Collection5/last_09.ann  \n",
      "  inflating: Collection5/last_09.txt  \n",
      "  inflating: Collection5/last_10.ann  \n",
      "  inflating: Collection5/last_10.txt  \n",
      "  inflating: Collection5/last_11.ann  \n",
      "  inflating: Collection5/last_11.txt  \n",
      "  inflating: Collection5/last_12.ann  \n",
      "  inflating: Collection5/last_12.txt  \n",
      "  inflating: Collection5/last_13.ann  \n",
      "  inflating: Collection5/last_13.txt  \n",
      "  inflating: Collection5/last_14.ann  \n",
      "  inflating: Collection5/last_14.txt  \n",
      "  inflating: Collection5/last_15.ann  \n",
      "  inflating: Collection5/last_15.txt  \n",
      "  inflating: Collection5/last_16.ann  \n",
      "  inflating: Collection5/last_16.txt  \n",
      "  inflating: Collection5/last_17.ann  \n",
      "  inflating: Collection5/last_17.txt  \n",
      "  inflating: Collection5/last_18.ann  \n",
      "  inflating: Collection5/last_18.txt  \n",
      "  inflating: Collection5/last_19.ann  \n",
      "  inflating: Collection5/last_19.txt  \n",
      "  inflating: Collection5/last_20.ann  \n",
      "  inflating: Collection5/last_20.txt  \n",
      "  inflating: Collection5/last_21.ann  \n",
      "  inflating: Collection5/last_21.txt  \n",
      "  inflating: Collection5/last_22.ann  \n",
      "  inflating: Collection5/last_22.txt  \n",
      "  inflating: Collection5/last_23.ann  \n",
      "  inflating: Collection5/last_23.txt  \n",
      "  inflating: Collection5/last_24.ann  \n",
      "  inflating: Collection5/last_24.txt  \n",
      "  inflating: Collection5/last_25.ann  \n",
      "  inflating: Collection5/last_25.txt  \n",
      "  inflating: Collection5/last_26.ann  \n",
      "  inflating: Collection5/last_26.txt  \n",
      "  inflating: Collection5/last_27.ann  \n",
      "  inflating: Collection5/last_27.txt  \n",
      "  inflating: Collection5/last_28.ann  \n",
      "  inflating: Collection5/last_28.txt  \n",
      "  inflating: Collection5/last_29.ann  \n",
      "  inflating: Collection5/last_29.txt  \n",
      "  inflating: Collection5/last_30_new.ann  \n",
      "  inflating: Collection5/last_30_new.txt  \n",
      "  inflating: Collection5/last_31.ann  \n",
      "  inflating: Collection5/last_31.txt  \n",
      "  inflating: Collection5/last_32.ann  \n",
      "  inflating: Collection5/last_32.txt  \n",
      "  inflating: Collection5/last_33.ann  \n",
      "  inflating: Collection5/last_33.txt  \n",
      "  inflating: Collection5/last_34.ann  \n",
      "  inflating: Collection5/last_34.txt  \n",
      "  inflating: Collection5/last_35.ann  \n",
      "  inflating: Collection5/last_35.txt  \n",
      "  inflating: Collection5/last_36.ann  \n",
      "  inflating: Collection5/last_36.txt  \n",
      "  inflating: Collection5/last_37.ann  \n",
      "  inflating: Collection5/last_37.txt  \n",
      "  inflating: Collection5/last_38.ann  \n",
      "  inflating: Collection5/last_38.txt  \n",
      "  inflating: Collection5/last_39.ann  \n",
      "  inflating: Collection5/last_39.txt  \n",
      "  inflating: Collection5/last_40.ann  \n",
      "  inflating: Collection5/last_40.txt  \n",
      "  inflating: Collection5/last_41.ann  \n",
      "  inflating: Collection5/last_41.txt  \n",
      "  inflating: Collection5/last_42.ann  \n",
      "  inflating: Collection5/last_42.txt  \n",
      "  inflating: Collection5/last_43.ann  \n",
      "  inflating: Collection5/last_43.txt  \n",
      "  inflating: Collection5/last_44.ann  \n",
      "  inflating: Collection5/last_44.txt  \n",
      "  inflating: Collection5/last_45.ann  \n",
      "  inflating: Collection5/last_45.txt  \n",
      "  inflating: Collection5/last_46.ann  \n",
      "  inflating: Collection5/last_46.txt  \n",
      "  inflating: Collection5/last_47.ann  \n",
      "  inflating: Collection5/last_47.txt  \n",
      "  inflating: Collection5/last_48.ann  \n",
      "  inflating: Collection5/last_48.txt  \n",
      "  inflating: Collection5/last_49.ann  \n",
      "  inflating: Collection5/last_49.txt  \n",
      "  inflating: Collection5/last_50.ann  \n",
      "  inflating: Collection5/last_50.txt  \n",
      "  inflating: Collection5/last_51.ann  \n",
      "  inflating: Collection5/last_51.txt  \n",
      "  inflating: Collection5/last_52.ann  \n",
      "  inflating: Collection5/last_52.txt  \n",
      "  inflating: Collection5/last_53.ann  \n",
      "  inflating: Collection5/last_53.txt  \n",
      "  inflating: Collection5/last_54.ann  \n",
      "  inflating: Collection5/last_54.txt  \n",
      "  inflating: Collection5/last_55.ann  \n",
      "  inflating: Collection5/last_55.txt  \n",
      "  inflating: Collection5/last_56.ann  \n",
      "  inflating: Collection5/last_56.txt  \n",
      "  inflating: Collection5/last_57.ann  \n",
      "  inflating: Collection5/last_57.txt  \n",
      "  inflating: Collection5/last_58.ann  \n",
      "  inflating: Collection5/last_58.txt  \n",
      "  inflating: Collection5/last_59.ann  \n",
      "  inflating: Collection5/last_59.txt  \n",
      "  inflating: Collection5/last_60.ann  \n",
      "  inflating: Collection5/last_60.txt  \n",
      "  inflating: Collection5/last_61.ann  \n",
      "  inflating: Collection5/last_61.txt  \n",
      "  inflating: Collection5/last_62.ann  \n",
      "  inflating: Collection5/last_62.txt  \n",
      "  inflating: Collection5/last_63.ann  \n",
      "  inflating: Collection5/last_63.txt  \n",
      "  inflating: Collection5/last_64.ann  \n",
      "  inflating: Collection5/last_64.txt  \n",
      "  inflating: Collection5/last_65.ann  \n",
      "  inflating: Collection5/last_65.txt  \n",
      "  inflating: Collection5/last_66.ann  \n",
      "  inflating: Collection5/last_66.txt  \n",
      "  inflating: Collection5/last_67.ann  \n",
      "  inflating: Collection5/last_67.txt  \n",
      "  inflating: Collection5/last_68.ann  \n",
      "  inflating: Collection5/last_68.txt  \n",
      "  inflating: Collection5/last_69.ann  \n",
      "  inflating: Collection5/last_69.txt  \n",
      "  inflating: Collection5/last_70.ann  \n",
      "  inflating: Collection5/last_70.txt  \n",
      "  inflating: Collection5/last_71.ann  \n",
      "  inflating: Collection5/last_71.txt  \n",
      "  inflating: Collection5/last_72.ann  \n",
      "  inflating: Collection5/last_72.txt  \n",
      "  inflating: Collection5/last_73.ann  \n",
      "  inflating: Collection5/last_73.txt  \n",
      "  inflating: Collection5/last_74.ann  \n",
      "  inflating: Collection5/last_74.txt  \n",
      "  inflating: Collection5/last_75.ann  \n",
      "  inflating: Collection5/last_75.txt  \n",
      "  inflating: Collection5/lenoblast.ann  \n",
      "  inflating: Collection5/lenoblast.txt  \n",
      "  inflating: Collection5/maykl dzhekson.ann  \n",
      "  inflating: Collection5/maykl dzhekson.txt  \n",
      "  inflating: Collection5/mvd.ann     \n",
      "  inflating: Collection5/mvd.txt     \n",
      "  inflating: Collection5/mvd2.ann    \n",
      "  inflating: Collection5/mvd2.txt    \n",
      "  inflating: Collection5/rosobrnadzor.ann  \n",
      "  inflating: Collection5/rosobrnadzor.txt  \n",
      "  inflating: Collection5/ryadovoy chelah.ann  \n",
      "  inflating: Collection5/ryadovoy chelah.txt  \n",
      "  inflating: Collection5/semenenko.ann  \n",
      "  inflating: Collection5/semenenko.txt  \n",
      "  inflating: Collection5/shojgu1.ann  \n",
      "  inflating: Collection5/shojgu1.txt  \n",
      "  inflating: Collection5/shojgu3.ann  \n",
      "  inflating: Collection5/shojgu3.txt  \n",
      "  inflating: Collection5/shojgu4.ann  \n",
      "  inflating: Collection5/shojgu4.txt  \n",
      "  inflating: Collection5/shojgu6.ann  \n",
      "  inflating: Collection5/shojgu6.txt  \n",
      "  inflating: Collection5/si_tzjanpin.ann  \n",
      "  inflating: Collection5/si_tzjanpin.txt  \n",
      "  inflating: Collection5/sobjanin2.ann  \n",
      "  inflating: Collection5/sobjanin2.txt  \n",
      "  inflating: Collection5/turkmenija.ann  \n",
      "  inflating: Collection5/turkmenija.txt  \n",
      "  inflating: Collection5/uchitel.ann  \n",
      "  inflating: Collection5/uchitel.txt  \n"
     ]
    }
   ],
   "source": [
    "!wget http://www.labinform.ru/pub/named_entities/collection5.zip\n",
    "!unzip collection5.zip\n",
    "!rm collection5.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2797511d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаем данные Collection5...\n",
      "Загружено записей: 1000\n",
      "\n",
      "Пример записей:\n",
      "Запись 1:\n",
      "  Текст: Галушка приступил к работе по ликвидации последствий наводнения (ДФО)\n",
      "\n",
      "В район бедствия в Хабаровс...\n",
      "  Spans: [Ne5Span(index='T1', type='PER', start=0, stop=7, text='Галушка'), Ne5Span(index='T2', type='LOC', start=65, stop=68, text='ДФО'), Ne5Span(index='T3', type='LOC', start=92, stop=108, text='Хабаровский край'), Ne5Span(index='T4', type='LOC', start=152, stop=168, text='Дальнего Востока'), Ne5Span(index='T5', type='PER', start=169, stop=186, text='Александр Галушка')]\n",
      "\n",
      "Запись 2:\n",
      "  Текст: \n",
      "Ульяновские депутаты утвердили заместителей главы администрации города\n",
      "\n",
      "Депутаты Ульяновской гор...\n",
      "  Spans: [Ne5Span(index='T1', type='ORG', start=85, stop=111, text='Ульяновской городской думы'), Ne5Span(index='T2', type='LOC', start=180, stop=190, text='Ульяновска'), Ne5Span(index='T3', type='MEDIA', start=216, stop=225, text='ИА REGNUM'), Ne5Span(index='T4', type='ORG', start=241, stop=244, text='УГД'), Ne5Span(index='T5', type='LOC', start=389, stop=399, text='Ульяновска')]\n",
      "\n",
      "Запись 3:\n",
      "  Текст: Либерал-демократы получат в кабинете Кэмерона пять постов\n",
      "\n",
      "Лидер Либерально-демократической партии...\n",
      "  Spans: [Ne5Span(index='T1', type='PER', start=37, stop=45, text='Кэмерона'), Ne5Span(index='T2', type='ORG', start=67, stop=100, text='Либерально-демократической партии'), Ne5Span(index='T3', type='PER', start=101, stop=110, text='Ник Клегг'), Ne5Span(index='T4', type='GEOPOLIT', start=161, stop=175, text='Великобритании'), Ne5Span(index='T5', type='ORG', start=191, stop=212, text='Консервативной партии')]\n",
      "\n",
      "Запись 4:\n",
      "  Текст: Д.Медведев отправил в отставку губернатора Ставрополья\n",
      "\n",
      "Президент России Дмитрий Медведев досрочно...\n",
      "  Spans: [Ne5Span(index='T1', type='PER', start=0, stop=10, text='Д.Медведев'), Ne5Span(index='T2', type='LOC', start=43, stop=54, text='Ставрополья'), Ne5Span(index='T3', type='GEOPOLIT', start=68, stop=74, text='России'), Ne5Span(index='T4', type='PER', start=75, stop=91, text='Дмитрий Медведев'), Ne5Span(index='T5', type='LOC', start=133, stop=153, text='Ставропольского края')]\n",
      "\n",
      "Запись 5:\n",
      "  Текст: Зубков вновь вошел в совет директоров Газпрома\n",
      "\n",
      "\n",
      "Бывший первый вице-премьер РФ Виктор Зубков сохр...\n",
      "  Spans: [Ne5Span(index='T1', type='PER', start=0, stop=6, text='Зубков'), Ne5Span(index='T2', type='ORG', start=38, stop=46, text='Газпрома'), Ne5Span(index='T3', type='GEOPOLIT', start=79, stop=81, text='РФ'), Ne5Span(index='T4', type='PER', start=82, stop=95, text='Виктор Зубков'), Ne5Span(index='T5', type='ORG', start=187, stop=255, text='Российского государственного университета нефти и газа имени Губкина')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Загрузка набора данных Collection5 с использованием метода load_ne5\n",
    "print(\"Загружаем данные Collection5...\")\n",
    "\n",
    "dir = 'Collection5'\n",
    "records = list(load_ne5(dir))\n",
    "print(f\"Загружено записей: {len(records)}\")\n",
    "\n",
    "# Посмотрим на структуру первых нескольких записей\n",
    "print(\"\\nПример записей:\")\n",
    "for i, record in enumerate(records[:5]):\n",
    "    print(f\"Запись {i+1}:\")\n",
    "    print(f\"  Текст: {record.text[:100]}...\")\n",
    "    print(f\"  Spans: {record.spans[:5] if record.spans else 'Нет'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95927db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение классов для работы с данными\n",
    "@dataclass\n",
    "class Ne5Span:\n",
    "    start: int\n",
    "    stop: int\n",
    "    type: str\n",
    "\n",
    "@dataclass\n",
    "class LabelledText:\n",
    "    text: str\n",
    "    entities: list[Ne5Span]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfea8254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_records(records: list) -> tuple[list[LabelledText], dict[str, int]]:\n",
    "    \"\"\"Parse records into LabelledText objects and create a category map.\"\"\"\n",
    "    labelled_texts = []\n",
    "    cats_map = {}\n",
    "\n",
    "    for annot in records:\n",
    "        entities = []\n",
    "        \n",
    "        for ent in annot.spans:\n",
    "            new_ent = Ne5Span(\n",
    "                start=ent.start,\n",
    "                stop=ent.stop,\n",
    "                type=ent.type\n",
    "            )\n",
    "\n",
    "            entities.append(new_ent)\n",
    "            if new_ent.type not in cats_map:\n",
    "                cats_map[new_ent.type] = len(cats_map)\n",
    "\n",
    "        entities.sort(key=lambda x: x.start)\n",
    "        lab_txt = LabelledText(\n",
    "            text=annot.text,\n",
    "            entities=entities\n",
    "        )\n",
    "        labelled_texts.append(lab_txt)\n",
    "\n",
    "    return labelled_texts, cats_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16076d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего записей: 1000\n",
      "Категории объектов: {'PER': 0, 'LOC': 1, 'MEDIA': 2, 'GEOPOLIT': 3, 'ORG': 4}\n"
     ]
    }
   ],
   "source": [
    "labelled_text, categories_map = parse_records(records)\n",
    "\n",
    "print(f'Всего записей: {len(labelled_text)}')\n",
    "print('Категории объектов:', categories_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eecf6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 800\n",
      "Test size: 200\n"
     ]
    }
   ],
   "source": [
    "# Разбивка на train/test части (80/20)\n",
    "train_data, test_data = train_test_split(labelled_text, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Test size: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d192294",
   "metadata": {},
   "source": [
    "### 3. Токенизация датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e1a3871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токенизация и выравнивание меток\n",
    "def tokenize_rus(text: str):\n",
    "    \"\"\"Implement word-wise tokenization for russian language.\"\"\"\n",
    "    return list(razdel.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1213e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symbol_bio_to_word_bio(labelled_text: LabelledText, subwords: list[Substring]) -> list[str]:\n",
    "    \"\"\"Transforms symbol-wise annotation to word-wise BIO notation for NER.\n",
    "\n",
    "    Args:\n",
    "        labelled_text: A LabelledText object containing the original text and\n",
    "            symbol-wise entity annotations.\n",
    "        subwords: A list of Substring objects representing the tokenized text.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings representing BIO tags in the format:\n",
    "        - \"B-CAT\" for beginning of an entity of category CAT\n",
    "        - \"I-CAT\" for continuation of an entity of category CAT\n",
    "        - \"O\" for tokens outside any entity\n",
    "    \"\"\"\n",
    "    bio_tags = [\"O\"] * len(subwords)\n",
    "    entities = sorted(labelled_text.entities, key=lambda x: x.start)\n",
    "    \n",
    "    for entity in entities:\n",
    "        overlapping = []\n",
    "        for i, subword in enumerate(subwords):\n",
    "            if (subword.start < entity.stop and subword.stop > entity.start):\n",
    "                overlapping.append(i)\n",
    "\n",
    "        for j, idx in enumerate(overlapping):\n",
    "            bio_tags[idx] = f\"B-{entity.type}\" if j == 0 else f\"I-{entity.type}\"\n",
    "\n",
    "    return bio_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eb01db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels: list[str], word_ids: list[str]) -> list[str]:\n",
    "    \"\"\"Aligns word-level BIO labels to token-level labels accounting for wordpiece tokenization.\n",
    "    \n",
    "    Args:\n",
    "        labels: List of BIO worl-level labels.\n",
    "        word_ids: List mapping each token to its original word index (from tokenizer.word_ids()).\n",
    "    \n",
    "    Returns:\n",
    "        List of aligned token-level BIO labels.\n",
    "    \"\"\"\n",
    "    new_labels = []\n",
    "    cur_word_id = None\n",
    "    \n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            new_labels.append('Ignored')  # special token\n",
    "\n",
    "        elif word_id != cur_word_id:\n",
    "            cur_word_id = word_id\n",
    "            new_labels.append(labels[word_id])\n",
    "\n",
    "        else:\n",
    "            label = labels[word_id]\n",
    "            if label.startswith(\"B-\"):\n",
    "                label = f\"I-{label[2:]}\"\n",
    "\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1397e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelsTokenizerAligner:\n",
    "    def __init__(self, bio_labels_to_idx: dict[str, int], tokenizer):\n",
    "        self.bio_labels_to_idx = bio_labels_to_idx\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def _tokenize_and_align_labels(self, examples: Dataset):\n",
    "        \"\"\"Tokenizes input text and aligns word-level BIO labels with subword tokens.\n",
    "    \n",
    "        Args:\n",
    "            examples (Dataset): A Hugging Face `Dataset` object containing:\n",
    "                - `words`: List of words.\n",
    "                - `labels_bio`: List of word-level BIO labels.\n",
    "            tokenizer (PreTrainedTokenizer): Tokenizer (e.g., `BertTokenizer`) with subword tokenization.\n",
    "\n",
    "        Returns:\n",
    "            (Dataset): A Hugging Face `Dataset` object.\n",
    "        \"\"\"\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            examples[\"words\"], \n",
    "            truncation=True, \n",
    "            max_length=512,  # Явно устанавливаем максимальную длину для предотвращения CUDA OOM\n",
    "            padding=False,  # Позже используем data_collator для паддинга\n",
    "            is_split_into_words=True\n",
    "        )\n",
    "\n",
    "        aligned_labels = []\n",
    "        for sample_idx, bio_labels in enumerate(examples[\"bio_labels\"]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=sample_idx)\n",
    "            bio_labels_aligned = align_labels_with_tokens(bio_labels, word_ids)\n",
    "            aligned_labels.append([self.bio_labels_to_idx[bla] for bla in bio_labels_aligned])\n",
    "\n",
    "        tokenized_inputs[\"labels\"] = aligned_labels\n",
    "        return tokenized_inputs\n",
    "\n",
    "    def __call__(self, examples: Dataset):\n",
    "        return self._tokenize_and_align_labels(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "558d7f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokenized_dataset(ds_ner: list, bio_labels_to_idx: dict[str, int], tokenizer) -> Dataset:\n",
    "    \"\"\"Make tokenized dataset ready for batching.\n",
    "\n",
    "    Makes(Dataset): a Hugging Face `Dataset` object containing model inputs:\n",
    "        input_ids, token_type_ids, and an attention_mask via the steps:\n",
    "        1) Represent raw dataset as python dictionary of words and bio_labels.\n",
    "        2) Split sentences into words and convert symbol-wise labeling into word-wise BIO format.\n",
    "        3) Tokenize words and align the labels with sub-tokens level.\n",
    "\n",
    "    Args:\n",
    "        ds_ner: List of (LabelledText) objects.\n",
    "        bio_labels_to_idx: Map for BIO labels to int including 'Ignored' key for special tokens.\n",
    "        tokenizer (PreTrainedTokenizer): Tokenizer (e.g., `BertTokenizer`) with subword tokenization.\n",
    "\n",
    "    Returns:\n",
    "        (Dataset): A tokenized Hugging Face `Dataset` object.\n",
    "    \"\"\"\n",
    "    ds_dict = {\"words\": [], \"bio_labels\": []}\n",
    "\n",
    "    for sample in ds_ner:\n",
    "        words_subs = tokenize_rus(sample.text)\n",
    "        words = [ws.text for ws in words_subs]\n",
    "        bio_labels = symbol_bio_to_word_bio(sample, words_subs)\n",
    "        ds_dict[\"words\"].append(words)\n",
    "        ds_dict[\"bio_labels\"].append(bio_labels)\n",
    "\n",
    "    dataset = Dataset.from_dict(ds_dict)\n",
    "    labels_tokenizer_aligner = LabelsTokenizerAligner(bio_labels_to_idx, tokenizer)\n",
    "\n",
    "    tokenized_dataset = dataset.map(             \n",
    "        labels_tokenizer_aligner,\n",
    "        batched=True,\n",
    "        remove_columns=['words', 'bio_labels']\n",
    "    )\n",
    "    \n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78b09de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-PER': 1,\n",
       " 'I-PER': 2,\n",
       " 'B-LOC': 3,\n",
       " 'I-LOC': 4,\n",
       " 'B-MEDIA': 5,\n",
       " 'I-MEDIA': 6,\n",
       " 'B-GEOPOLIT': 7,\n",
       " 'I-GEOPOLIT': 8,\n",
       " 'B-ORG': 9,\n",
       " 'I-ORG': 10,\n",
       " 'Ignored': -100}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bio_labels_to_idx = {'O': 0}\n",
    "\n",
    "for cat in categories_map:\n",
    "    for prefix in 'BI':\n",
    "        bio_labels_to_idx[f'{prefix}-{cat}'] = len(bio_labels_to_idx)\n",
    "\n",
    "bio_labels_to_idx['Ignored'] = -100\n",
    "\n",
    "idx_to_bio_labels = {lbl: bio for bio, lbl in bio_labels_to_idx.items()}\n",
    "\n",
    "bio_labels_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08887d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep 26 16:15:52 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-PCIE-32GB           Off |   00000000:81:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             36W /  250W |    6534MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE-32GB           Off |   00000000:85:00.0 Off |                    0 |\n",
      "| N/A   28C    P0             36W /  250W |   31478MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  Tesla V100-PCIE-32GB           Off |   00000000:86:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             38W /  250W |    1248MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  Tesla V100-PCIE-32GB           Off |   00000000:87:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             37W /  250W |    1126MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A          157537      C   ...env3.10-deeppavlov/bin/python       4718MiB |\n",
      "|    0   N/A  N/A          638194      C   ...i/find_in_go/.venv/bin/python       1504MiB |\n",
      "|    1   N/A  N/A          402403      C   ...rasov-evg/venvs/tf/bin/python      31474MiB |\n",
      "|    2   N/A  N/A          177469      C   ...nning/fresh_ml_env/bin/python        770MiB |\n",
      "|    2   N/A  N/A          228451      C   ...nning/fresh_ml_env/bin/python        474MiB |\n",
      "|    3   N/A  N/A         3810149      C   ...6/venvs/kaggle/bin/python3.11       1122MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c28dd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используется устройство: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# device for training\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Используется устройство: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee6463b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготавливаем tokenizer и датасеты\n",
    "model_checkpoint = \"cointegrated/rubert-tiny2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "648f5c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создаем токенизированные датасеты с max_length=512...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 899.53 examples/s]\n",
      "Map: 100%|██████████| 800/800 [00:00<00:00, 948.79 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Датасеты успешно созданы!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Пересоздаем датасеты с исправленной токенизацией (max_length=512)\n",
    "print(\"Создаем токенизированные датасеты с max_length=512...\")\n",
    "\n",
    "ds_tokenized_test = make_tokenized_dataset(test_data, bio_labels_to_idx, tokenizer)\n",
    "ds_tokenized_train = make_tokenized_dataset(train_data, bio_labels_to_idx, tokenizer)\n",
    "\n",
    "print(\"Датасеты успешно созданы!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fc144ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size: 200\n",
      "train size: 800\n"
     ]
    }
   ],
   "source": [
    "print(f'test size: {len(ds_tokenized_test)}')\n",
    "print(f'train size: {len(ds_tokenized_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d33dc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Анализ длин последовательностей для train ===\n",
      "Количество последовательностей: 800\n",
      "Минимальная длина: 28\n",
      "Максимальная длина: 512\n",
      "Средняя длина: 288.96\n",
      "Медианная длина: 269.00\n",
      "Стандартное отклонение: 123.03\n",
      "Последовательности длиной >= 510: 93\n",
      "\n",
      "=== Анализ длин последовательностей для test ===\n",
      "Количество последовательностей: 200\n",
      "Минимальная длина: 57\n",
      "Максимальная длина: 512\n",
      "Средняя длина: 280.00\n",
      "Медианная длина: 265.00\n",
      "Стандартное отклонение: 119.02\n",
      "Последовательности длиной >= 510: 16\n"
     ]
    }
   ],
   "source": [
    "# Анализ длин последовательностей для диагностики проблем с памятью\n",
    "import numpy as np\n",
    "\n",
    "def analyze_sequence_lengths(dataset, dataset_name):\n",
    "    lengths = [len(item['input_ids']) for item in dataset]\n",
    "    print(f\"\\n=== Анализ длин последовательностей для {dataset_name} ===\")\n",
    "    print(f\"Количество последовательностей: {len(lengths)}\")\n",
    "    print(f\"Минимальная длина: {min(lengths)}\")\n",
    "    print(f\"Максимальная длина: {max(lengths)}\")\n",
    "    print(f\"Средняя длина: {np.mean(lengths):.2f}\")\n",
    "    print(f\"Медианная длина: {np.median(lengths):.2f}\")\n",
    "    print(f\"Стандартное отклонение: {np.std(lengths):.2f}\")\n",
    "    \n",
    "    # Проверим, есть ли последовательности, которые достигают максимальной длины\n",
    "    max_len_count = sum(1 for l in lengths if l >= 510)  # близко к 512\n",
    "    print(f\"Последовательности длиной >= 510: {max_len_count}\")\n",
    "    \n",
    "    return lengths\n",
    "\n",
    "train_lengths = analyze_sequence_lengths(ds_tokenized_train, \"train\")\n",
    "test_lengths = analyze_sequence_lengths(ds_tokenized_test, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cf1521e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw test: Экс-глава X5 Retail Л.Хасис назначен вице-президентом Wal-Mart\n",
      "\n",
      "Крупнейший в мире ритейлер - американский Wal-Mart - назначил экс-главу Х5 Retail Group Льва Хасиса на должность старшего вице-президента по международным операциям. Об этом сегодня сообщила официальный представитель американской компании, передает Reuters.\n",
      "\n",
      "По ее словам, Л.Хасис будет работать в головном офисе Wal-Mart в Бентонвилле (штат Арканзас) и подчиняться напрямую главе Wal-Mart International Дагу Макмиллану. На новом посту экс-глава X5 будет отвечать за \"интеграцию приобретаемых компаний, синергию в глобальных закупках и создание инновационных команд\", отметила представитель компании.\n",
      "\n",
      "Официально Л.Хасис вступит в должность с октября текущего года.\n",
      "\n",
      "Л.Хасис сменит в новой должности Джона Адена, который занимал пост старшего вице-президента по международным операциям Wal-Mart с 2007г. по 2010г.\n",
      "\n",
      "Напомним, Л.Хасис покинул X5 Retail в марте 2011г., чтобы заняться персональными проектами. На посту главного исполнительного директора и члена правления на четырехлетний срок его сменил Андрей Гусев, утвержденный акционерами компании в июне с.г.\n",
      "\n",
      "\n",
      "tok test: Экс - глава X 5 Retail Л. Хасис назначен вице - президентом Wal - Mart Крупнейший в мире ритейлер - американский Wal - Mart - назначил экс - главу Х 5 Retail Group Льва Хасиса на должность старшего вице - президента по международным операциям. Об этом сегодня сообщила официальный представитель американской компании, передает Reuters. По ее словам, Л. Хасис будет работать в головном офисе Wal - Mart в Бентонвилле ( штат Арканзас ) и подчиняться напрямую главе Wal - Mart International Дагу Макмиллану. На новом посту экс - глава X 5 будет отвечать за \" интеграцию приобретаемых компаний, синергию в глобальных закупках и создание инновационных команд \", отметила представитель компании. Официально Л. Хасис вступит в должность с октября текущего года. Л. Хасис сменит в новой должности Джона Адена, который занимал пост старшего вице - президента по международным операциям Wal - Mart с 2007 г. по 2010 г. Напомним, Л. Хасис покинул X 5 Retail в марте 2011 г., чтобы заняться персональными проектами. На посту главного исполнительного директора и члена правления на четырехлетний срок его сменил Андрей Гусев, утвержденный акционерами компании в июне с. г.\n"
     ]
    }
   ],
   "source": [
    "_test_idx = 0\n",
    "print(f'raw test: {test_data[_test_idx].text}')\n",
    "_tokenized_decoded = tokenizer.decode(ds_tokenized_test[_test_idx]['input_ids'], skip_special_tokens=True)\n",
    "print(f\"tok test: {_tokenized_decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb21f569",
   "metadata": {},
   "source": [
    "### 4. Дообучение модели rubert-tiny2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96def750",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5715a31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {label: idx for label, idx in bio_labels_to_idx.items() if idx >= 0}\n",
    "id2label = {idx: label for label, idx in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d0f3600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-PER',\n",
       " 2: 'I-PER',\n",
       " 3: 'B-LOC',\n",
       " 4: 'I-LOC',\n",
       " 5: 'B-MEDIA',\n",
       " 6: 'I-MEDIA',\n",
       " 7: 'B-GEOPOLIT',\n",
       " 8: 'I-GEOPOLIT',\n",
       " 9: 'B-ORG',\n",
       " 10: 'I-ORG'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56cd6442",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38eb1cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47d50579",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")\n",
    "label_names = list(label2id.keys()) \n",
    "\n",
    "def compute_metrics(eval_preds: tuple[np.ndarray, np.ndarray]) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for token classification (NER) using seqeval.\n",
    "\n",
    "    Args:\n",
    "        eval_preds (tuple): A tuple containing:\n",
    "            - logits (np.ndarray): Model output logits of shape (batch_size, seq_len, num_labels)\n",
    "            - labels (np.ndarray): Ground truth label ids of shape (batch_size, seq_len)\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with overall precision, recall, F1 score, and accuracy.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    metrics = metric.compute(predictions=true_predictions, references=true_labels, zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"precision\": metrics[\"overall_precision\"],\n",
    "        \"recall\": metrics[\"overall_recall\"],\n",
    "        \"f1\": metrics[\"overall_f1\"],\n",
    "        \"accuracy\": metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "075f84c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# — оптимизатор AdamW (по умолчанию в HF Trainer)\n",
    "# — lr=2e-5 — стандарт для дообучения BERT-подобных\n",
    "# — batch_size=16 — укладывается в VRAM V100\n",
    "# — epochs=25 — даёт стабильную сходимость на малых корпусах\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_rubert_tiny2_dumps\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=25,\n",
    "    weight_decay=0.01,\n",
    "    seed=RANDOM_SEED,\n",
    "    save_strategy=\"epoch\",          # Сохраняем модель каждую эпоху\n",
    "    save_total_limit=2,             # Храним только 2 последних checkpoint'а\n",
    "    logging_steps=100,              # Логируем каждые 100 шагов\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f84d3ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_tokenized_train,\n",
    "    eval_dataset=ds_tokenized_test,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8e0c2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики до дообучения:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.516990900039673,\n",
       " 'eval_model_preparation_time': 0.0018,\n",
       " 'eval_precision': 0.0025687130747495505,\n",
       " 'eval_recall': 0.026831785345717233,\n",
       " 'eval_f1': 0.004688570707252858,\n",
       " 'eval_accuracy': 0.05463930504847035,\n",
       " 'eval_runtime': 2.1209,\n",
       " 'eval_samples_per_second': 94.298,\n",
       " 'eval_steps_per_second': 6.129}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Метрики до дообучения:\")\n",
    "trainer.evaluate(ds_tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3cf0034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Очищаем кэш GPU...\n",
      "Кэш очищен. Начинаем обучение...\n"
     ]
    }
   ],
   "source": [
    "# Очистка кэша GPU для освобождения памяти перед обучением\n",
    "import gc\n",
    "\n",
    "print(\"Очищаем кэш GPU...\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "# Принудительная очистка памяти Python\n",
    "gc.collect()\n",
    "\n",
    "print(\"Кэш очищен. Начинаем обучение...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c97e9362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 02:10, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.827944</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.764591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.033700</td>\n",
       "      <td>0.504461</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.404519</td>\n",
       "      <td>0.310423</td>\n",
       "      <td>0.351279</td>\n",
       "      <td>0.852197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.033700</td>\n",
       "      <td>0.377137</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.493498</td>\n",
       "      <td>0.493498</td>\n",
       "      <td>0.493498</td>\n",
       "      <td>0.893581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.394600</td>\n",
       "      <td>0.301762</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.544061</td>\n",
       "      <td>0.615480</td>\n",
       "      <td>0.577571</td>\n",
       "      <td>0.919534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.394600</td>\n",
       "      <td>0.258774</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.579245</td>\n",
       "      <td>0.677399</td>\n",
       "      <td>0.624489</td>\n",
       "      <td>0.930775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.259200</td>\n",
       "      <td>0.227866</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.610484</td>\n",
       "      <td>0.713932</td>\n",
       "      <td>0.658168</td>\n",
       "      <td>0.937879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.259200</td>\n",
       "      <td>0.204817</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.645758</td>\n",
       "      <td>0.735191</td>\n",
       "      <td>0.687578</td>\n",
       "      <td>0.944012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.198100</td>\n",
       "      <td>0.188789</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.677513</td>\n",
       "      <td>0.769247</td>\n",
       "      <td>0.720472</td>\n",
       "      <td>0.948904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.198100</td>\n",
       "      <td>0.177083</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.695415</td>\n",
       "      <td>0.788854</td>\n",
       "      <td>0.739194</td>\n",
       "      <td>0.952105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.163000</td>\n",
       "      <td>0.167411</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.712891</td>\n",
       "      <td>0.798968</td>\n",
       "      <td>0.753479</td>\n",
       "      <td>0.954479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.163000</td>\n",
       "      <td>0.159421</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.719537</td>\n",
       "      <td>0.808050</td>\n",
       "      <td>0.761229</td>\n",
       "      <td>0.956314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.139500</td>\n",
       "      <td>0.153346</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.725368</td>\n",
       "      <td>0.814448</td>\n",
       "      <td>0.767331</td>\n",
       "      <td>0.957627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.139500</td>\n",
       "      <td>0.149201</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.732077</td>\n",
       "      <td>0.817750</td>\n",
       "      <td>0.772546</td>\n",
       "      <td>0.958292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.123000</td>\n",
       "      <td>0.144755</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.735424</td>\n",
       "      <td>0.822704</td>\n",
       "      <td>0.776620</td>\n",
       "      <td>0.959119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.123000</td>\n",
       "      <td>0.141817</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.738618</td>\n",
       "      <td>0.827038</td>\n",
       "      <td>0.780331</td>\n",
       "      <td>0.959911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.111600</td>\n",
       "      <td>0.138719</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.742203</td>\n",
       "      <td>0.830134</td>\n",
       "      <td>0.783710</td>\n",
       "      <td>0.960576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.111600</td>\n",
       "      <td>0.135602</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.746296</td>\n",
       "      <td>0.831785</td>\n",
       "      <td>0.786725</td>\n",
       "      <td>0.961727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.103200</td>\n",
       "      <td>0.133983</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.747964</td>\n",
       "      <td>0.834262</td>\n",
       "      <td>0.788760</td>\n",
       "      <td>0.961835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.103200</td>\n",
       "      <td>0.131815</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.754924</td>\n",
       "      <td>0.838596</td>\n",
       "      <td>0.794563</td>\n",
       "      <td>0.962681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.096600</td>\n",
       "      <td>0.131617</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.757300</td>\n",
       "      <td>0.840454</td>\n",
       "      <td>0.796713</td>\n",
       "      <td>0.962824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.096600</td>\n",
       "      <td>0.129862</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.753107</td>\n",
       "      <td>0.837977</td>\n",
       "      <td>0.793279</td>\n",
       "      <td>0.962896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.093700</td>\n",
       "      <td>0.129603</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.755803</td>\n",
       "      <td>0.840041</td>\n",
       "      <td>0.795699</td>\n",
       "      <td>0.963076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.093700</td>\n",
       "      <td>0.128935</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.758749</td>\n",
       "      <td>0.841280</td>\n",
       "      <td>0.797886</td>\n",
       "      <td>0.963346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.090600</td>\n",
       "      <td>0.128699</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.760246</td>\n",
       "      <td>0.842312</td>\n",
       "      <td>0.799178</td>\n",
       "      <td>0.963454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.090600</td>\n",
       "      <td>0.128610</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.760246</td>\n",
       "      <td>0.842312</td>\n",
       "      <td>0.799178</td>\n",
       "      <td>0.963490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=0.22815085678100586, metrics={'train_runtime': 131.4537, 'train_samples_per_second': 152.145, 'train_steps_per_second': 9.509, 'total_flos': 139090358059872.0, 'train_loss': 0.22815085678100586, 'epoch': 25.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1816fb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики после дообучения:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.12861043214797974,\n",
       " 'eval_model_preparation_time': 0.0018,\n",
       " 'eval_precision': 0.7602459016393442,\n",
       " 'eval_recall': 0.842311661506708,\n",
       " 'eval_f1': 0.7991775188485265,\n",
       " 'eval_accuracy': 0.9634898652901926,\n",
       " 'eval_runtime': 1.4995,\n",
       " 'eval_samples_per_second': 133.381,\n",
       " 'eval_steps_per_second': 8.67,\n",
       " 'epoch': 25.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Метрики после дообучения:\")\n",
    "trainer.evaluate(ds_tokenized_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd88576a",
   "metadata": {},
   "source": [
    "### 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7cf2f9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>O</th>\n",
       "      <th>B-PER</th>\n",
       "      <th>I-PER</th>\n",
       "      <th>B-LOC</th>\n",
       "      <th>I-LOC</th>\n",
       "      <th>B-MEDIA</th>\n",
       "      <th>I-MEDIA</th>\n",
       "      <th>B-GEOPOLIT</th>\n",
       "      <th>I-GEOPOLIT</th>\n",
       "      <th>B-ORG</th>\n",
       "      <th>I-ORG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>41919</td>\n",
       "      <td>20</td>\n",
       "      <td>68</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-PER</th>\n",
       "      <td>15</td>\n",
       "      <td>1944</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-PER</th>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>4743</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-LOC</th>\n",
       "      <td>7</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>573</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-LOC</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>109</td>\n",
       "      <td>11</td>\n",
       "      <td>668</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-MEDIA</th>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>224</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-MEDIA</th>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>186</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-GEOPOLIT</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>613</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-GEOPOLIT</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-ORG</th>\n",
       "      <td>51</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1045</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-ORG</th>\n",
       "      <td>271</td>\n",
       "      <td>7</td>\n",
       "      <td>61</td>\n",
       "      <td>7</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>1656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                O  B-PER  I-PER  B-LOC  I-LOC  B-MEDIA  I-MEDIA  B-GEOPOLIT  \\\n",
       "O           41919     20     68      8     34        1        1           6   \n",
       "B-PER          15   1944     36      3      1        1        0           1   \n",
       "I-PER          16     13   4743      0      5        0        3           0   \n",
       "B-LOC           7     44      1    573      5        0        0          27   \n",
       "I-LOC          12      1    109     11    668        0        2           1   \n",
       "B-MEDIA        22      5      1      1      0      224        1           2   \n",
       "I-MEDIA        34      1     18      0      3        9      186           0   \n",
       "B-GEOPOLIT      5     10      2     28      0        0        0         613   \n",
       "I-GEOPOLIT     11      0     43      2     63        0        0           1   \n",
       "B-ORG          51      5      6     27      0        4        1           7   \n",
       "I-ORG         271      7     61      7     46        2        4           8   \n",
       "\n",
       "            I-GEOPOLIT  B-ORG  I-ORG  \n",
       "O                    0     77    296  \n",
       "B-PER                0     10      1  \n",
       "I-PER                0      0     42  \n",
       "B-LOC                0     28      4  \n",
       "I-LOC                0      0     72  \n",
       "B-MEDIA              0     31      1  \n",
       "I-MEDIA              0      9    102  \n",
       "B-GEOPOLIT           0     14      2  \n",
       "I-GEOPOLIT           0      0     25  \n",
       "B-ORG                0   1045     36  \n",
       "I-ORG                0     59   1656  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_output = trainer.predict(ds_tokenized_test)\n",
    "logits = predictions_output.predictions\n",
    "labels = predictions_output.label_ids\n",
    "preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "pred_labels = [[label_names[p] for (p, l) in zip(pred, label) if l != -100] for pred, label in zip(preds, labels)]\n",
    "\n",
    "y_true_flat = [l for seq in true_labels for l in seq]\n",
    "y_pred_flat = [l for seq in pred_labels for l in seq]\n",
    "\n",
    "cm = confusion_matrix(y_true_flat, y_pred_flat, labels=label_names)\n",
    "cm_df = pd.DataFrame(cm, index=label_names, columns=label_names)\n",
    "print(\"Confusion Matrix:\")\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce9a6662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    GEOPOLIT       0.85      0.84      0.85       674\n",
      "         LOC       0.69      0.75      0.72       689\n",
      "       MEDIA       0.78      0.69      0.73       288\n",
      "         ORG       0.59      0.76      0.66      1182\n",
      "         PER       0.88      0.94      0.91      2012\n",
      "\n",
      "   micro avg       0.76      0.84      0.80      4845\n",
      "   macro avg       0.76      0.80      0.77      4845\n",
      "weighted avg       0.77      0.84      0.80      4845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bfa4d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_labelled_text(labelled_text: LabelledText, color_map: dict[str, str]):\n",
    "    \"\"\"\n",
    "    Visualize the labeled data for Named Entity Recognition (NER).\n",
    "\n",
    "    Args:\n",
    "        labelled_text: An instance of LabelledText containing the text and entities.\n",
    "        color_map: A color map for each entity category.\n",
    "\n",
    "    Returns:\n",
    "        None. Displays the HTML representation of the text with highlighted entities.\n",
    "    \"\"\"\n",
    "    # Initialize the HTML string\n",
    "    labelled_text2 = labelled_text\n",
    "    labelled_text2.text = labelled_text.text.replace(\"\\n\", \"  \")\n",
    "    html_str = \"\"\n",
    "    last_index = 0\n",
    "    \n",
    "    # Sort entities by the starting index\n",
    "    entities = sorted(labelled_text2.entities, key=lambda x: x.start)\n",
    "    for entity in entities:\n",
    "        start, stop, type = entity.start, entity.stop, entity.type\n",
    "        html_str += labelled_text2.text[last_index:start]  # non-entity text before the current entity\n",
    "        color = color_map[type]\n",
    "        html_str += (\n",
    "            f'<mark style=\"background-color: {color}\">{labelled_text2.text[start:stop]}'\n",
    "            + f'<sub>({type})</sub></mark>'\n",
    "        )\n",
    "        last_index = stop\n",
    "    html_str += labelled_text2.text[last_index:]\n",
    "    \n",
    "    display(HTML(html_str))  # render HTML using Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cac2ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_MAP = {\n",
    "    'GEOPOLIT': 'yellow',\n",
    "    'LOC': 'lightblue',\n",
    "    'MEDIA': \"green\",\n",
    "    'PER': \"cyan\",\n",
    "    'ORG': \"red\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0bde1412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9655007, 'word': 'Барак Обама', 'start': 0, 'end': 11}, {'entity_group': 'GEOPOLIT', 'score': 0.32570925, 'word': 'Ю', 'start': 33, 'end': 34}, {'entity_group': 'PER', 'score': 0.49656078, 'word': '##ты', 'start': 34, 'end': 36}, {'entity_group': 'GEOPOLIT', 'score': 0.94108385, 'word': 'США', 'start': 44, 'end': 47}, {'entity_group': 'GEOPOLIT', 'score': 0.8766509, 'word': 'Китае', 'start': 50, 'end': 55}, {'entity_group': 'GEOPOLIT', 'score': 0.9230892, 'word': 'США', 'start': 66, 'end': 69}, {'entity_group': 'PER', 'score': 0.96309817, 'word': 'Барак Обама', 'start': 70, 'end': 81}, {'entity_group': 'PER', 'score': 0.9003958, 'word': 'Юта Джона Хантсмена - младшего ( John Huntsman Jr. )', 'start': 116, 'end': 164}, {'entity_group': 'GEOPOLIT', 'score': 0.94151914, 'word': 'США', 'start': 172, 'end': 175}, {'entity_group': 'GEOPOLIT', 'score': 0.85582006, 'word': 'Китае', 'start': 178, 'end': 183}, {'entity_group': 'MEDIA', 'score': 0.7345068, 'word': 'The Washington Times', 'start': 191, 'end': 211}, {'entity_group': 'GEOPOLIT', 'score': 0.46827483, 'word': 'Белого', 'start': 237, 'end': 243}, {'entity_group': 'LOC', 'score': 0.6604451, 'word': 'дома', 'start': 244, 'end': 248}, {'entity_group': 'PER', 'score': 0.9801015, 'word': 'Хантсмена', 'start': 262, 'end': 271}, {'entity_group': 'PER', 'score': 0.8515171, 'word': 'Обаме', 'start': 272, 'end': 277}, {'entity_group': 'PER', 'score': 0.98156404, 'word': 'Джефф Бэйдер ( Jeff Bader )', 'start': 319, 'end': 344}, {'entity_group': 'PER', 'score': 0.5997654, 'word': 'Юты', 'start': 377, 'end': 380}, {'entity_group': 'ORG', 'score': 0.94042933, 'word': 'Республиканской партии', 'start': 524, 'end': 546}, {'entity_group': 'PER', 'score': 0.96471554, 'word': 'Обамы', 'start': 578, 'end': 583}, {'entity_group': 'GEOPOLIT', 'score': 0.9171568, 'word': 'США', 'start': 606, 'end': 609}, {'entity_group': 'PER', 'score': 0.987947, 'word': 'Джон Маккейн', 'start': 610, 'end': 622}, {'entity_group': 'PER', 'score': 0.9719145, 'word': 'Обамы', 'start': 647, 'end': 652}, {'entity_group': 'PER', 'score': 0.98258305, 'word': 'Хантсмен', 'start': 722, 'end': 730}, {'entity_group': 'PER', 'score': 0.98291355, 'word': 'Хантсмен', 'start': 850, 'end': 858}, {'entity_group': 'PER', 'score': 0.9868354, 'word': 'Джона Маккейна', 'start': 892, 'end': 906}, {'entity_group': 'PER', 'score': 0.98107237, 'word': 'Хантсмена', 'start': 1046, 'end': 1055}, {'entity_group': 'PER', 'score': 0.9661292, 'word': 'Обама', 'start': 1064, 'end': 1069}, {'entity_group': 'PER', 'score': 0.71609503, 'word': 'Юты', 'start': 1137, 'end': 1140}, {'entity_group': 'PER', 'score': 0.9811869, 'word': 'Хантсмен', 'start': 1153, 'end': 1161}, {'entity_group': 'GEOPOLIT', 'score': 0.94327873, 'word': 'США', 'start': 1194, 'end': 1197}, {'entity_group': 'PER', 'score': 0.8690132, 'word': 'Джордже Буше - младшем', 'start': 1202, 'end': 1222}, {'entity_group': 'GEOPOLIT', 'score': 0.937545, 'word': 'США', 'start': 1246, 'end': 1249}, {'entity_group': 'GEOPOLIT', 'score': 0.8166378, 'word': 'Сингапуре', 'start': 1252, 'end': 1261}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<mark style=\"background-color: cyan\">Барак Обама<sub>(PER)</sub></mark> назначил губернатора <mark style=\"background-color: yellow\">Ю<sub>(GEOPOLIT)</sub></mark><mark style=\"background-color: cyan\">ты<sub>(PER)</sub></mark> послом <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> в <mark style=\"background-color: yellow\">Китае<sub>(GEOPOLIT)</sub></mark> Президент <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: cyan\">Барак Обама<sub>(PER)</sub></mark> 16 мая назначил губернатора штата <mark style=\"background-color: cyan\">Юта Джона Хантсмена-младшего (John Huntsman Jr.)<sub>(PER)</sub></mark> послом <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> в <mark style=\"background-color: yellow\">Китае<sub>(GEOPOLIT)</sub></mark>, пишет <mark style=\"background-color: green\">The Washington Times<sub>(MEDIA)</sub></mark>. По словам представителя <mark style=\"background-color: yellow\">Белого<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: lightblue\">дома<sub>(LOC)</sub></mark>, кандидатуру <mark style=\"background-color: cyan\">Хантсмена<sub>(PER)</sub></mark> <mark style=\"background-color: cyan\">Обаме<sub>(PER)</sub></mark> предложил помощник по азиатской политике <mark style=\"background-color: cyan\">Джефф Бэйдер (Jeff Bader)<sub>(PER)</sub></mark>, который представил губернатора <mark style=\"background-color: cyan\">Юты<sub>(PER)</sub></mark> как человека, отлично знающего китайский язык, разбирающегося в проблемах региона и способного эффективно решать дипломатические задачи. Члены <mark style=\"background-color: red\">Республиканской партии<sub>(ORG)</sub></mark>, в том числе и бывший соперник <mark style=\"background-color: cyan\">Обамы<sub>(PER)</sub></mark> на выборах президента <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: cyan\">Джон Маккейн<sub>(PER)</sub></mark>, поприветствовали выбор <mark style=\"background-color: cyan\">Обамы<sub>(PER)</sub></mark>. Они, однако, в то же время признали, что в связи с этим назначением <mark style=\"background-color: cyan\">Хантсмен<sub>(PER)</sub></mark> фактически лишился возможности участвовать в следующих президентских выборах. Во время предвыборной кампании 2008 года <mark style=\"background-color: cyan\">Хантсмен<sub>(PER)</sub></mark> был одним из руководителей штаба <mark style=\"background-color: cyan\">Джона Маккейна<sub>(PER)</sub></mark> и, по оценкам экспертов, именно он мог стать кандидатом от республиканцев на следующих выборах. Многие республиканцы отметили, что, сделав <mark style=\"background-color: cyan\">Хантсмена<sub>(PER)</sub></mark> послом, <mark style=\"background-color: cyan\">Обама<sub>(PER)</sub></mark> устранил потенциально опасного соперника. До избрания губернатором <mark style=\"background-color: cyan\">Юты<sub>(PER)</sub></mark> в 2004 году <mark style=\"background-color: cyan\">Хантсмен<sub>(PER)</sub></mark> работал торговым представителем <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> при <mark style=\"background-color: cyan\">Джордже Буше-младшем<sub>(PER)</sub></mark>, а до этого был послом <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> в <mark style=\"background-color: yellow\">Сингапуре<sub>(GEOPOLIT)</sub></mark>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=0)\n",
    "\n",
    "text = \"Барак Обама назначил губернатора Юты послом США в Китае Президент США Барак Обама 16 мая назначил губернатора штата Юта Джона Хантсмена-младшего (John Huntsman Jr.) послом США в Китае, пишет The Washington Times. По словам представителя Белого дома, кандидатуру Хантсмена Обаме предложил помощник по азиатской политике Джефф Бэйдер (Jeff Bader), который представил губернатора Юты как человека, отлично знающего китайский язык, разбирающегося в проблемах региона и способного эффективно решать дипломатические задачи. Члены Республиканской партии, в том числе и бывший соперник Обамы на выборах президента США Джон Маккейн, поприветствовали выбор Обамы. Они, однако, в то же время признали, что в связи с этим назначением Хантсмен фактически лишился возможности участвовать в следующих президентских выборах. Во время предвыборной кампании 2008 года Хантсмен был одним из руководителей штаба Джона Маккейна и, по оценкам экспертов, именно он мог стать кандидатом от республиканцев на следующих выборах. Многие республиканцы отметили, что, сделав Хантсмена послом, Обама устранил потенциально опасного соперника. До избрания губернатором Юты в 2004 году Хантсмен работал торговым представителем США при Джордже Буше-младшем, а до этого был послом США в Сингапуре.\"\n",
    "entities = ner_pipeline(text)\n",
    "print(entities)\n",
    "\n",
    "predicted_entities = [\n",
    "    Ne5Span(start=ent[\"start\"], stop=ent[\"end\"], type=ent[\"entity_group\"])\n",
    "    for ent in entities\n",
    "]\n",
    "\n",
    "predicted_labelled_text = LabelledText(text=text, entities=predicted_entities)\n",
    "\n",
    "visualize_labelled_text(predicted_labelled_text, COLOR_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "910d6e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./ner_rubert_tiny2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f4c1ac",
   "metadata": {},
   "source": [
    "### 6. Дообучение в MLM-режиме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e3da38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 512\n",
    "\n",
    "def group_texts(examples):\n",
    "    \"\"\"\n",
    "    Группирует тексты в блоки фиксированного размера для обучения модели языкового моделирования.\n",
    "    \n",
    "    Эта функция объединяет все тексты из примеров в один длинный текст, затем разбивает его\n",
    "    на блоки размером block_size токенов. Остаток, который меньше block_size, отбрасывается.\n",
    "    \n",
    "    Args:\n",
    "        examples: Словарь с ключами, содержащими списки токенизированных текстов\n",
    "        \n",
    "    Returns:\n",
    "        Словарь с теми же ключами, но значения разбиты на блоки размером block_size.\n",
    "        Также добавляется ключ \"labels\" как копия \"input_ids\" для обучения MLM.\n",
    "    \"\"\"\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of block_size.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d1c4888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b3cfbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 800/800 [00:02<00:00, 335.69 examples/s]\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 732.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds_tokenized_mlm_train = ds_tokenized_train.map(group_texts, batched=True)\n",
    "ds_tokenized_mlm_test = ds_tokenized_test.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35e85b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint).to(device)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59bb6e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_args  = TrainingArguments(\n",
    "    output_dir=\"./mlm_rubert_tiny2_dumps\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=25,\n",
    "    weight_decay=0.01,\n",
    "    seed=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57360e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=mlm_args ,\n",
    "    train_dataset=ds_tokenized_mlm_train,\n",
    "    eval_dataset=ds_tokenized_mlm_train,  # Prevent data leakage\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3ea3cff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты evaluation: {'eval_loss': 2.922074794769287, 'eval_model_preparation_time': 0.0033, 'eval_runtime': 0.8244, 'eval_samples_per_second': 132.224, 'eval_steps_per_second': 8.491}\n"
     ]
    }
   ],
   "source": [
    "results = mlm_trainer.evaluate(ds_tokenized_mlm_test)\n",
    "print(\"Результаты evaluation:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df9a269c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='725' max='725' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [725/725 04:20, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.788134</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.792165</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.738152</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.746068</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.730753</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.704244</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.688667</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.683922</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.676269</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.642901</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.666188</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.656970</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.642132</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.672532</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.609389</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.587254</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.586140</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.984500</td>\n",
       "      <td>2.607278</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.984500</td>\n",
       "      <td>2.621777</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.984500</td>\n",
       "      <td>2.598052</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.984500</td>\n",
       "      <td>2.599105</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.984500</td>\n",
       "      <td>2.633707</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.984500</td>\n",
       "      <td>2.573666</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.984500</td>\n",
       "      <td>2.591261</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.984500</td>\n",
       "      <td>2.601502</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=725, training_loss=2.9608660257273707, metrics={'train_runtime': 260.3461, 'train_samples_per_second': 43.308, 'train_steps_per_second': 2.785, 'total_flos': 86047648051200.0, 'train_loss': 2.9608660257273707, 'epoch': 25.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "001f6658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29' max='29' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [29/29 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.6501502990722656,\n",
       " 'eval_model_preparation_time': 0.0033,\n",
       " 'eval_runtime': 3.6096,\n",
       " 'eval_samples_per_second': 124.943,\n",
       " 'eval_steps_per_second': 8.034,\n",
       " 'epoch': 25.0}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlm_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c1e70d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохраняем MLM обученную модель...\n",
      "MLM модель сохранена в ./mlm_rubert_tiny2\n"
     ]
    }
   ],
   "source": [
    "# Сохранение MLM обученной модели\n",
    "print(\"Сохраняем MLM обученную модель...\")\n",
    "model.save_pretrained(\"./mlm_rubert_tiny2\")\n",
    "tokenizer.save_pretrained(\"./mlm_rubert_tiny2\")\n",
    "print(\"MLM модель сохранена в ./mlm_rubert_tiny2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b03a3d7",
   "metadata": {},
   "source": [
    "### СЕКЦИЯ 3: NER AFTER MLM ОБУЧЕНИЕ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c2fc02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ./mlm_rubert_tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Постобучение NER после MLM\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"./mlm_rubert_tiny2\",\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./mlm_rubert_tiny2\")\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a5f3b54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создаем токенизированные датасеты\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 341.83 examples/s]\n",
      "Map: 100%|██████████| 800/800 [00:02<00:00, 336.76 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Датасеты успешно созданы!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Пересоздаем датасеты с исправленной токенизацией\n",
    "print(\"Создаем токенизированные датасеты\")\n",
    "ds_tokenized_test = make_tokenized_dataset(test_data, bio_labels_to_idx, tokenizer)\n",
    "ds_tokenized_train = make_tokenized_dataset(train_data, bio_labels_to_idx, tokenizer)\n",
    "print(\"Датасеты успешно созданы!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c28f24da",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_args  = TrainingArguments(\n",
    "    output_dir=\"./post_mlm_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=25,\n",
    "    weight_decay=0.01,\n",
    "    seed=RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8d6258ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_mlm_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=post_args ,\n",
    "    train_dataset=ds_tokenized_train,\n",
    "    eval_dataset=ds_tokenized_test, \n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d0588a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"seqeval\")\n",
    "label_names = list(label2id.keys()) \n",
    "\n",
    "def compute_metrics(eval_preds: tuple[np.ndarray, np.ndarray]) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for token classification (NER) using seqeval.\n",
    "\n",
    "    Args:\n",
    "        eval_preds (tuple): A tuple containing:\n",
    "            - logits (np.ndarray): Model output logits of shape (batch_size, seq_len, num_labels)\n",
    "            - labels (np.ndarray): Ground truth label ids of shape (batch_size, seq_len)\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with overall precision, recall, F1 score, and accuracy.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    metrics = metric.compute(predictions=true_predictions, references=true_labels, zero_division=0)\n",
    "\n",
    "    return {\n",
    "        \"precision\": metrics[\"overall_precision\"],\n",
    "        \"recall\": metrics[\"overall_recall\"],\n",
    "        \"f1\": metrics[\"overall_f1\"],\n",
    "        \"accuracy\": metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce8fc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики до дообучения:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.006213413781391725,\n",
       " 'recall': 0.06418988648090815,\n",
       " 'f1': 0.011330103100295093,\n",
       " 'accuracy': 0.05453139332026402}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Метрики до дообучения:\")\n",
    "post_mlm_trainer.evaluate(ds_tokenized_test)\n",
    "\n",
    "compute_metrics(\n",
    "    (post_mlm_trainer.predict(ds_tokenized_test).predictions, ds_tokenized_test['labels'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "135b9fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 01:24, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.837429</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.513879</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.390324</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.308676</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.260150</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.228605</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.205675</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.188440</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.176249</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.418600</td>\n",
       "      <td>0.166532</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.418600</td>\n",
       "      <td>0.158541</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.418600</td>\n",
       "      <td>0.152265</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.418600</td>\n",
       "      <td>0.147554</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.418600</td>\n",
       "      <td>0.143262</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.418600</td>\n",
       "      <td>0.139953</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.418600</td>\n",
       "      <td>0.136766</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.418600</td>\n",
       "      <td>0.134178</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.418600</td>\n",
       "      <td>0.132603</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.418600</td>\n",
       "      <td>0.130457</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.113600</td>\n",
       "      <td>0.129643</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.113600</td>\n",
       "      <td>0.128600</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.113600</td>\n",
       "      <td>0.128173</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.113600</td>\n",
       "      <td>0.127497</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.113600</td>\n",
       "      <td>0.127212</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.113600</td>\n",
       "      <td>0.127104</td>\n",
       "      <td>0.002000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=0.23091659698486328, metrics={'train_runtime': 84.2708, 'train_samples_per_second': 237.33, 'train_steps_per_second': 14.833, 'total_flos': 139090358059872.0, 'train_loss': 0.23091659698486328, 'epoch': 25.0})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_mlm_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cbe6fd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сохраняем финально обученную модель (post-MLM NER)...\n",
      "Финальная модель сохранена в ./final_ner_model\n",
      "\n",
      "Финальная оценка модели:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Финальные метрики: {'eval_loss': 0.1271035522222519, 'eval_model_preparation_time': 0.002, 'eval_runtime': 0.3149, 'eval_samples_per_second': 635.069, 'eval_steps_per_second': 41.279, 'epoch': 25.0}\n"
     ]
    }
   ],
   "source": [
    "# Финальное сохранение обученной модели после всех этапов\n",
    "print(\"Сохраняем финально обученную модель (post-MLM NER)...\")\n",
    "model.save_pretrained(\"./final_ner_model\")\n",
    "tokenizer.save_pretrained(\"./final_ner_model\")\n",
    "print(\"Финальная модель сохранена в ./final_ner_model\")\n",
    "\n",
    "# Финальная оценка модели\n",
    "print(\"\\nФинальная оценка модели:\")\n",
    "final_results = post_mlm_trainer.evaluate(ds_tokenized_test)\n",
    "print(f\"Финальные метрики: {final_results}\")\n",
    "\n",
    "# Очистка памяти"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36cb824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики после дообучения:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.7639622641509434,\n",
       " 'recall': 0.8357069143446852,\n",
       " 'f1': 0.7982257269590932,\n",
       " 'accuracy': 0.963831585762846}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Метрики после дообучения:\")\n",
    "post_mlm_trainer.evaluate()\n",
    "compute_metrics(\n",
    "    (post_mlm_trainer.predict(ds_tokenized_test).predictions, ds_tokenized_test['labels'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b374464",
   "metadata": {},
   "source": [
    "### Оценка качества модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5a308412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>O</th>\n",
       "      <th>B-PER</th>\n",
       "      <th>I-PER</th>\n",
       "      <th>B-LOC</th>\n",
       "      <th>I-LOC</th>\n",
       "      <th>B-MEDIA</th>\n",
       "      <th>I-MEDIA</th>\n",
       "      <th>B-GEOPOLIT</th>\n",
       "      <th>I-GEOPOLIT</th>\n",
       "      <th>B-ORG</th>\n",
       "      <th>I-ORG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>41941</td>\n",
       "      <td>20</td>\n",
       "      <td>52</td>\n",
       "      <td>9</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-PER</th>\n",
       "      <td>20</td>\n",
       "      <td>1937</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-PER</th>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "      <td>4716</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-LOC</th>\n",
       "      <td>12</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>563</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-LOC</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>9</td>\n",
       "      <td>703</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-MEDIA</th>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>221</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-MEDIA</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-GEOPOLIT</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>602</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-GEOPOLIT</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-ORG</th>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1033</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-ORG</th>\n",
       "      <td>279</td>\n",
       "      <td>12</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>1674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                O  B-PER  I-PER  B-LOC  I-LOC  B-MEDIA  I-MEDIA  B-GEOPOLIT  \\\n",
       "O           41941     20     52      9     35        2        2           4   \n",
       "B-PER          20   1937     29      4      4        1        0           0   \n",
       "I-PER          24     15   4716      0      7        0        2           0   \n",
       "B-LOC          12     43      2    563      7        0        0          30   \n",
       "I-LOC          19      0     86      9    703        0        0           1   \n",
       "B-MEDIA        24      5      1      3      0      221        1           1   \n",
       "I-MEDIA        32      1     22      0      1       14      200           1   \n",
       "B-GEOPOLIT      8      9      2     28      0        0        0         602   \n",
       "I-GEOPOLIT      9      0     40      1     58        0        0           1   \n",
       "B-ORG          59      5      5     18      0        5        0           7   \n",
       "I-ORG         279     12     49      6     36        1        0           8   \n",
       "\n",
       "            I-GEOPOLIT  B-ORG  I-ORG  \n",
       "O                    0     61    304  \n",
       "B-PER                0     14      3  \n",
       "I-PER                0      0     58  \n",
       "B-LOC                0     26      6  \n",
       "I-LOC                0      0     58  \n",
       "B-MEDIA              0     31      1  \n",
       "I-MEDIA              0      8     83  \n",
       "B-GEOPOLIT           0     16      9  \n",
       "I-GEOPOLIT           0      0     36  \n",
       "B-ORG                0   1033     50  \n",
       "I-ORG                0     56   1674  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_output = post_mlm_trainer.predict(ds_tokenized_test)\n",
    "logits = predictions_output.predictions\n",
    "labels = predictions_output.label_ids\n",
    "preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "pred_labels = [[label_names[p] for (p, l) in zip(pred, label) if l != -100] for pred, label in zip(preds, labels)]\n",
    "\n",
    "y_true_flat = [l for seq in true_labels for l in seq]\n",
    "y_pred_flat = [l for seq in pred_labels for l in seq]\n",
    "\n",
    "cm = confusion_matrix(y_true_flat, y_pred_flat, labels=label_names)\n",
    "cm_df = pd.DataFrame(cm, index=label_names, columns=label_names)\n",
    "print(\"Confusion Matrix:\")\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e43a748e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    GEOPOLIT       0.85      0.83      0.84       674\n",
      "         LOC       0.70      0.75      0.72       689\n",
      "       MEDIA       0.78      0.70      0.74       288\n",
      "         ORG       0.59      0.75      0.66      1182\n",
      "         PER       0.88      0.94      0.91      2012\n",
      "\n",
      "   micro avg       0.76      0.84      0.80      4845\n",
      "   macro avg       0.76      0.79      0.77      4845\n",
      "weighted avg       0.77      0.84      0.80      4845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e69a28b",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c81a42f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.9535439, 'word': 'Барак Обама', 'start': 0, 'end': 11}, {'entity_group': 'GEOPOLIT', 'score': 0.38565344, 'word': 'Ю', 'start': 33, 'end': 34}, {'entity_group': 'LOC', 'score': 0.38824052, 'word': '##ты', 'start': 34, 'end': 36}, {'entity_group': 'GEOPOLIT', 'score': 0.9504317, 'word': 'США', 'start': 44, 'end': 47}, {'entity_group': 'GEOPOLIT', 'score': 0.9099589, 'word': 'Китае', 'start': 50, 'end': 55}, {'entity_group': 'GEOPOLIT', 'score': 0.95046854, 'word': 'США', 'start': 66, 'end': 69}, {'entity_group': 'PER', 'score': 0.9685683, 'word': 'Барак Обама', 'start': 70, 'end': 81}, {'entity_group': 'PER', 'score': 0.69074476, 'word': 'Юта', 'start': 116, 'end': 119}, {'entity_group': 'PER', 'score': 0.87000597, 'word': 'Джона Хантсмена - младшего ( John Huntsman Jr. )', 'start': 120, 'end': 164}, {'entity_group': 'GEOPOLIT', 'score': 0.9474612, 'word': 'США', 'start': 172, 'end': 175}, {'entity_group': 'GEOPOLIT', 'score': 0.85961217, 'word': 'Китае', 'start': 178, 'end': 183}, {'entity_group': 'MEDIA', 'score': 0.736746, 'word': 'The Washington Times', 'start': 191, 'end': 211}, {'entity_group': 'GEOPOLIT', 'score': 0.45165002, 'word': 'Белого', 'start': 237, 'end': 243}, {'entity_group': 'LOC', 'score': 0.44583645, 'word': 'дома', 'start': 244, 'end': 248}, {'entity_group': 'PER', 'score': 0.97864884, 'word': 'Хантсмена', 'start': 262, 'end': 271}, {'entity_group': 'PER', 'score': 0.8008285, 'word': 'Обаме', 'start': 272, 'end': 277}, {'entity_group': 'PER', 'score': 0.9799283, 'word': 'Джефф Бэйдер ( Jeff Bader )', 'start': 319, 'end': 344}, {'entity_group': 'PER', 'score': 0.3473976, 'word': 'Ю', 'start': 377, 'end': 378}, {'entity_group': 'LOC', 'score': 0.52024126, 'word': '##ты', 'start': 378, 'end': 380}, {'entity_group': 'ORG', 'score': 0.9356996, 'word': 'Республиканской партии', 'start': 524, 'end': 546}, {'entity_group': 'PER', 'score': 0.95812273, 'word': 'Обамы', 'start': 578, 'end': 583}, {'entity_group': 'GEOPOLIT', 'score': 0.9523865, 'word': 'США', 'start': 606, 'end': 609}, {'entity_group': 'PER', 'score': 0.98869914, 'word': 'Джон Маккейн', 'start': 610, 'end': 622}, {'entity_group': 'PER', 'score': 0.96648806, 'word': 'Обамы', 'start': 647, 'end': 652}, {'entity_group': 'PER', 'score': 0.98284894, 'word': 'Хантсмен', 'start': 722, 'end': 730}, {'entity_group': 'PER', 'score': 0.98270625, 'word': 'Хантсмен', 'start': 850, 'end': 858}, {'entity_group': 'PER', 'score': 0.9869875, 'word': 'Джона Маккейна', 'start': 892, 'end': 906}, {'entity_group': 'PER', 'score': 0.980748, 'word': 'Хантсмена', 'start': 1046, 'end': 1055}, {'entity_group': 'PER', 'score': 0.9636094, 'word': 'Обама', 'start': 1064, 'end': 1069}, {'entity_group': 'PER', 'score': 0.5349351, 'word': 'Юты', 'start': 1137, 'end': 1140}, {'entity_group': 'PER', 'score': 0.9804066, 'word': 'Хантсмен', 'start': 1153, 'end': 1161}, {'entity_group': 'GEOPOLIT', 'score': 0.95474845, 'word': 'США', 'start': 1194, 'end': 1197}, {'entity_group': 'PER', 'score': 0.9605029, 'word': 'Джордже Буше -', 'start': 1202, 'end': 1215}, {'entity_group': 'PER', 'score': 0.88593704, 'word': '##м', 'start': 1221, 'end': 1222}, {'entity_group': 'GEOPOLIT', 'score': 0.94302064, 'word': 'США', 'start': 1246, 'end': 1249}, {'entity_group': 'GEOPOLIT', 'score': 0.7724176, 'word': 'Сингапуре', 'start': 1252, 'end': 1261}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<mark style=\"background-color: cyan\">Барак Обама<sub>(PER)</sub></mark> назначил губернатора <mark style=\"background-color: yellow\">Ю<sub>(GEOPOLIT)</sub></mark><mark style=\"background-color: lightblue\">ты<sub>(LOC)</sub></mark> послом <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> в <mark style=\"background-color: yellow\">Китае<sub>(GEOPOLIT)</sub></mark> Президент <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: cyan\">Барак Обама<sub>(PER)</sub></mark> 16 мая назначил губернатора штата <mark style=\"background-color: cyan\">Юта<sub>(PER)</sub></mark> <mark style=\"background-color: cyan\">Джона Хантсмена-младшего (John Huntsman Jr.)<sub>(PER)</sub></mark> послом <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> в <mark style=\"background-color: yellow\">Китае<sub>(GEOPOLIT)</sub></mark>, пишет <mark style=\"background-color: green\">The Washington Times<sub>(MEDIA)</sub></mark>. По словам представителя <mark style=\"background-color: yellow\">Белого<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: lightblue\">дома<sub>(LOC)</sub></mark>, кандидатуру <mark style=\"background-color: cyan\">Хантсмена<sub>(PER)</sub></mark> <mark style=\"background-color: cyan\">Обаме<sub>(PER)</sub></mark> предложил помощник по азиатской политике <mark style=\"background-color: cyan\">Джефф Бэйдер (Jeff Bader)<sub>(PER)</sub></mark>, который представил губернатора <mark style=\"background-color: cyan\">Ю<sub>(PER)</sub></mark><mark style=\"background-color: lightblue\">ты<sub>(LOC)</sub></mark> как человека, отлично знающего китайский язык, разбирающегося в проблемах региона и способного эффективно решать дипломатические задачи. Члены <mark style=\"background-color: red\">Республиканской партии<sub>(ORG)</sub></mark>, в том числе и бывший соперник <mark style=\"background-color: cyan\">Обамы<sub>(PER)</sub></mark> на выборах президента <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: cyan\">Джон Маккейн<sub>(PER)</sub></mark>, поприветствовали выбор <mark style=\"background-color: cyan\">Обамы<sub>(PER)</sub></mark>. Они, однако, в то же время признали, что в связи с этим назначением <mark style=\"background-color: cyan\">Хантсмен<sub>(PER)</sub></mark> фактически лишился возможности участвовать в следующих президентских выборах. Во время предвыборной кампании 2008 года <mark style=\"background-color: cyan\">Хантсмен<sub>(PER)</sub></mark> был одним из руководителей штаба <mark style=\"background-color: cyan\">Джона Маккейна<sub>(PER)</sub></mark> и, по оценкам экспертов, именно он мог стать кандидатом от республиканцев на следующих выборах. Многие республиканцы отметили, что, сделав <mark style=\"background-color: cyan\">Хантсмена<sub>(PER)</sub></mark> послом, <mark style=\"background-color: cyan\">Обама<sub>(PER)</sub></mark> устранил потенциально опасного соперника. До избрания губернатором <mark style=\"background-color: cyan\">Юты<sub>(PER)</sub></mark> в 2004 году <mark style=\"background-color: cyan\">Хантсмен<sub>(PER)</sub></mark> работал торговым представителем <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> при <mark style=\"background-color: cyan\">Джордже Буше-<sub>(PER)</sub></mark>младше<mark style=\"background-color: cyan\">м<sub>(PER)</sub></mark>, а до этого был послом <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> в <mark style=\"background-color: yellow\">Сингапуре<sub>(GEOPOLIT)</sub></mark>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=0)\n",
    "\n",
    "text = \"Барак Обама назначил губернатора Юты послом США в Китае Президент США Барак Обама 16 мая назначил губернатора штата Юта Джона Хантсмена-младшего (John Huntsman Jr.) послом США в Китае, пишет The Washington Times. По словам представителя Белого дома, кандидатуру Хантсмена Обаме предложил помощник по азиатской политике Джефф Бэйдер (Jeff Bader), который представил губернатора Юты как человека, отлично знающего китайский язык, разбирающегося в проблемах региона и способного эффективно решать дипломатические задачи. Члены Республиканской партии, в том числе и бывший соперник Обамы на выборах президента США Джон Маккейн, поприветствовали выбор Обамы. Они, однако, в то же время признали, что в связи с этим назначением Хантсмен фактически лишился возможности участвовать в следующих президентских выборах. Во время предвыборной кампании 2008 года Хантсмен был одним из руководителей штаба Джона Маккейна и, по оценкам экспертов, именно он мог стать кандидатом от республиканцев на следующих выборах. Многие республиканцы отметили, что, сделав Хантсмена послом, Обама устранил потенциально опасного соперника. До избрания губернатором Юты в 2004 году Хантсмен работал торговым представителем США при Джордже Буше-младшем, а до этого был послом США в Сингапуре.\"\n",
    "entities = ner_pipeline(text)\n",
    "print(entities)\n",
    "\n",
    "predicted_entities = [\n",
    "    Ne5Span(start=ent[\"start\"], stop=ent[\"end\"], type=ent[\"entity_group\"])\n",
    "    for ent in entities\n",
    "]\n",
    "\n",
    "predicted_labelled_text = LabelledText(text=text, entities=predicted_entities)\n",
    "\n",
    "visualize_labelled_text(predicted_labelled_text, COLOR_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76164085",
   "metadata": {},
   "source": [
    "### СЕКЦИЯ 4: Использование дополнительной разметки (`synthetic_labelling.ipynb`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0fd91",
   "metadata": {},
   "source": [
    "### Сама секция\n",
    "\n",
    "Перед выполнением необходимо прогнать ноутбук `synthetic_labelling.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f761d034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>bio_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Американские, фондовые, рынки, открылись, 10,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Венесуэла, намерена, ввести, въездные, визы, ...</td>\n",
       "      <td>[B-LOC, O, O, O, O, O, O, B-LOC, O, O, O, O, B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Ученые, из, Австралии, и, Японии, ,, использу...</td>\n",
       "      <td>[O, O, B-LOC, O, B-LOC, O, O, O, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Авианосец, нового, поколения, для, военно, -,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Тактика, сеяния, хаоса, ,, которую, ведет, те...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, O, O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               words  \\\n",
       "0  [Американские, фондовые, рынки, открылись, 10,...   \n",
       "1  [Венесуэла, намерена, ввести, въездные, визы, ...   \n",
       "2  [Ученые, из, Австралии, и, Японии, ,, использу...   \n",
       "3  [Авианосец, нового, поколения, для, военно, -,...   \n",
       "4  [Тактика, сеяния, хаоса, ,, которую, ведет, те...   \n",
       "\n",
       "                                          bio_labels  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-O...  \n",
       "1  [B-LOC, O, O, O, O, O, O, B-LOC, O, O, O, O, B...  \n",
       "2  [O, O, B-LOC, O, B-LOC, O, O, O, O, O, O, O, O...  \n",
       "3  [O, O, O, O, O, O, O, O, B-LOC, O, O, O, O, O,...  \n",
       "4  [O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, O, O...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenta = pd.read_parquet('synthetic_annotations.parquet')\n",
    "lenta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eddf09e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9706/9706 [00:23<00:00, 416.94 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Подготавливаем tokenizer и датасеты\n",
    "model_checkpoint = \"cointegrated/rubert-tiny2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "lenta_dataset = Dataset.from_pandas(lenta, preserve_index=False)\n",
    "labels_tokenizer_aligner = LabelsTokenizerAligner(bio_labels_to_idx, tokenizer)\n",
    "tokenized_dataset = lenta_dataset.map(labels_tokenizer_aligner, batched=True, remove_columns=['words', 'bio_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "189dc838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': ['Американские', 'фондовые', 'рынки', 'открылись', '10', 'августа', 'резким', 'снижением', 'котировок.', 'За', 'первые', 'минуты', 'торгов', 'индекс', 'Dow', 'Jones', 'упал', 'на', '2,67', 'процента', 'и', 'снова', 'торгуется', 'ниже', 'отметки', 'в', '11', 'тысяч', 'пунктов', ',', 'S', '&', 'P', '500', 'сократился', 'на', '2,63', 'процента', 'до', '1142', 'пунктов', ',', 'а', 'Nasdaq', '-', 'на', '2,76', 'процента', 'до', '2414', 'пунктов.', 'Днем', 'ранее', 'американские', 'индексы', 'выросли', '4', '-', '5', 'процентов.', 'Таким', 'образом', 'инвесторы', 'отреагировали', 'на', 'выступление', 'главы', 'Федеральной', 'резервной', 'системы', 'Бена', 'Бернанке', ',', 'который', 'пообещал', 'сохранить', 'низкие', 'базовые', 'ставки', 'по', 'крайней', 'мере', 'до', '2013', 'года.', 'Кроме', 'того', ',', '9', 'августа', 'американские', 'рынки', 'отыгрывали', 'падение', '8', 'августа', ',', 'когда', 'биржевые', 'показатели', 'сократились', 'на', '5', '-', '6', 'процентов', 'из', '-', 'за', 'снижения', 'кредитного', 'рейтинга', 'США.', 'Reuters', 'отмечает', ',', 'что', 'ралли', 'на', 'фондовом', 'рынке', 'прервалось', 'из', '-', 'за', 'опасений', 'инвесторов', 'относительно', 'роста', 'экономики', 'США', ',', 'а', 'также', 'из', '-', 'за', 'по', '-', 'прежнему', 'высокого', 'уровня', 'госдолга', 'страны.', 'Кроме', 'того', ',', 'они', 'могут', 'продавать', 'акции', 'из', '-', 'за', 'того', ',', 'что', 'Бернанке', '9', 'августа', 'не', 'объявил', 'о', 'новом', 'раунде', '\"', 'количественного', 'смягчения', '\"', ',', 'как', 'предполагал', 'целый', 'ряд', 'аналитиков.', '\"', 'Количественное', 'смягчение', '\"', 'предполагает', 'рост', 'вложений', 'ФРС', 'в', 'облигации', 'и', 'другие', 'ценные', 'бумаги', 'для', 'того', ',', 'чтобы', 'наполнить', 'рынок', 'деньгами.', 'Среди', '\"', 'голубых', 'фишек', '\"', ',', 'торгующихся', 'в', 'США', ',', 'стоит', 'отметить', 'акции', 'Walt', 'Disney', ',', 'которые', 'в', 'начале', 'сессии', 'упали', 'почти', 'на', '13', 'процентов', ',', 'а', 'также', 'ценные', 'бумаги', 'корпорации', 'AOL', '(', 'минус', '7,7', 'процента', ')', '.', 'Обе', 'компании', 'опубликовали', '10', 'августа', 'финансовые', 'отчеты', ',', 'которые', 'разочаровали', 'инвесторов', '.'], 'bio_labels': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'B-ORG', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n"
     ]
    }
   ],
   "source": [
    "print(lenta_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "31278923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ").to(device)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8e044d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 10506\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "combined_ds = concatenate_datasets([ds_tokenized_train, tokenized_dataset])\n",
    "\n",
    "print(combined_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ff379b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_model_synthetic_dumps\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=25,\n",
    "    weight_decay=0.01,\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=combined_ds,\n",
    "    eval_dataset=ds_tokenized_test,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b6923c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики до дообучения:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 01:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.513965129852295,\n",
       " 'eval_model_preparation_time': 0.0022,\n",
       " 'eval_precision': 0.002955963719966651,\n",
       " 'eval_recall': 0.02414860681114551,\n",
       " 'eval_f1': 0.005267185882141089,\n",
       " 'eval_accuracy': 0.05510692253736443,\n",
       " 'eval_runtime': 1.7447,\n",
       " 'eval_samples_per_second': 114.635,\n",
       " 'eval_steps_per_second': 7.451}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Метрики до дообучения:\")\n",
    "trainer.evaluate(ds_tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2a877a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16425' max='16425' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16425/16425 14:38, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.369500</td>\n",
       "      <td>0.295195</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.468927</td>\n",
       "      <td>0.565325</td>\n",
       "      <td>0.512633</td>\n",
       "      <td>0.922735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.115100</td>\n",
       "      <td>0.244491</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.508556</td>\n",
       "      <td>0.588854</td>\n",
       "      <td>0.545768</td>\n",
       "      <td>0.931260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.089100</td>\n",
       "      <td>0.227794</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.528644</td>\n",
       "      <td>0.601858</td>\n",
       "      <td>0.562880</td>\n",
       "      <td>0.935019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.063900</td>\n",
       "      <td>0.203355</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.543773</td>\n",
       "      <td>0.612797</td>\n",
       "      <td>0.576225</td>\n",
       "      <td>0.937807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>0.179921</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.569038</td>\n",
       "      <td>0.638803</td>\n",
       "      <td>0.601906</td>\n",
       "      <td>0.942285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.176716</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.582560</td>\n",
       "      <td>0.648091</td>\n",
       "      <td>0.613581</td>\n",
       "      <td>0.943796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.043100</td>\n",
       "      <td>0.161995</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.620184</td>\n",
       "      <td>0.681115</td>\n",
       "      <td>0.649223</td>\n",
       "      <td>0.948220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.040600</td>\n",
       "      <td>0.152796</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.636977</td>\n",
       "      <td>0.701135</td>\n",
       "      <td>0.667518</td>\n",
       "      <td>0.950918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.038200</td>\n",
       "      <td>0.151597</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.649729</td>\n",
       "      <td>0.717853</td>\n",
       "      <td>0.682095</td>\n",
       "      <td>0.952519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.137759</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.672980</td>\n",
       "      <td>0.740764</td>\n",
       "      <td>0.705247</td>\n",
       "      <td>0.956116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.132491</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.686311</td>\n",
       "      <td>0.752322</td>\n",
       "      <td>0.717802</td>\n",
       "      <td>0.957932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.029200</td>\n",
       "      <td>0.135555</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.683189</td>\n",
       "      <td>0.748194</td>\n",
       "      <td>0.714215</td>\n",
       "      <td>0.957555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>0.136716</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.684032</td>\n",
       "      <td>0.750671</td>\n",
       "      <td>0.715804</td>\n",
       "      <td>0.957968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>0.137133</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.691451</td>\n",
       "      <td>0.756244</td>\n",
       "      <td>0.722397</td>\n",
       "      <td>0.958814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.132568</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.701484</td>\n",
       "      <td>0.770691</td>\n",
       "      <td>0.734461</td>\n",
       "      <td>0.960432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.021900</td>\n",
       "      <td>0.135020</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.703795</td>\n",
       "      <td>0.769453</td>\n",
       "      <td>0.735161</td>\n",
       "      <td>0.960288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.137141</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.699378</td>\n",
       "      <td>0.766357</td>\n",
       "      <td>0.731337</td>\n",
       "      <td>0.960073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.133329</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.714933</td>\n",
       "      <td>0.781631</td>\n",
       "      <td>0.746796</td>\n",
       "      <td>0.962069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.019400</td>\n",
       "      <td>0.130930</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.721948</td>\n",
       "      <td>0.786171</td>\n",
       "      <td>0.752692</td>\n",
       "      <td>0.962770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.133068</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.723509</td>\n",
       "      <td>0.786378</td>\n",
       "      <td>0.753635</td>\n",
       "      <td>0.962878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.132580</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.724753</td>\n",
       "      <td>0.788029</td>\n",
       "      <td>0.755068</td>\n",
       "      <td>0.963058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.133022</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.724957</td>\n",
       "      <td>0.787203</td>\n",
       "      <td>0.754799</td>\n",
       "      <td>0.962914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>0.134745</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.722644</td>\n",
       "      <td>0.785139</td>\n",
       "      <td>0.752597</td>\n",
       "      <td>0.962752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>0.132319</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.728620</td>\n",
       "      <td>0.791331</td>\n",
       "      <td>0.758682</td>\n",
       "      <td>0.963670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>0.133399</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.726064</td>\n",
       "      <td>0.788854</td>\n",
       "      <td>0.756158</td>\n",
       "      <td>0.963274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16425, training_loss=0.04489754396667945, metrics={'train_runtime': 878.0959, 'train_samples_per_second': 299.113, 'train_steps_per_second': 18.705, 'total_flos': 1531921473032496.0, 'train_loss': 0.04489754396667945, 'epoch': 25.0})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4e6fe4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики после дообучения:\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.1333988606929779,\n",
       " 'eval_model_preparation_time': 0.0022,\n",
       " 'eval_precision': 0.726063829787234,\n",
       " 'eval_recall': 0.7888544891640867,\n",
       " 'eval_f1': 0.756157879117618,\n",
       " 'eval_accuracy': 0.96327404183378,\n",
       " 'eval_runtime': 1.4714,\n",
       " 'eval_samples_per_second': 135.928,\n",
       " 'eval_steps_per_second': 8.835,\n",
       " 'epoch': 25.0}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Метрики после дообучения:\")\n",
    "trainer.evaluate(ds_tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d4c90d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>O</th>\n",
       "      <th>B-PER</th>\n",
       "      <th>I-PER</th>\n",
       "      <th>B-LOC</th>\n",
       "      <th>I-LOC</th>\n",
       "      <th>B-MEDIA</th>\n",
       "      <th>I-MEDIA</th>\n",
       "      <th>B-GEOPOLIT</th>\n",
       "      <th>I-GEOPOLIT</th>\n",
       "      <th>B-ORG</th>\n",
       "      <th>I-ORG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>41858</td>\n",
       "      <td>17</td>\n",
       "      <td>133</td>\n",
       "      <td>11</td>\n",
       "      <td>112</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-PER</th>\n",
       "      <td>7</td>\n",
       "      <td>1973</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-PER</th>\n",
       "      <td>35</td>\n",
       "      <td>13</td>\n",
       "      <td>4736</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-LOC</th>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>630</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-LOC</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>73</td>\n",
       "      <td>745</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-MEDIA</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-MEDIA</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-GEOPOLIT</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>259</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>399</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-GEOPOLIT</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-ORG</th>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1112</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-ORG</th>\n",
       "      <td>187</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>1812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                O  B-PER  I-PER  B-LOC  I-LOC  B-MEDIA  I-MEDIA  B-GEOPOLIT  \\\n",
       "O           41858     17    133     11    112        2        1           4   \n",
       "B-PER           7   1973     20      7      3        0        0           0   \n",
       "I-PER          35     13   4736      0     10        0        4           0   \n",
       "B-LOC           2     16      2    630      2        0        1          20   \n",
       "I-LOC          20      0     25     73    745        0        0           1   \n",
       "B-MEDIA        14      0      0      0      0      153        4           1   \n",
       "I-MEDIA        22      0      1      0      0        4      125           0   \n",
       "B-GEOPOLIT      4      3      1    259      1        0        0         399   \n",
       "I-GEOPOLIT      6      2      5      4    107        0        0           1   \n",
       "B-ORG          38      2      0      8      0        1        0           1   \n",
       "I-ORG         187      5     14      3     21        0        0           4   \n",
       "\n",
       "            I-GEOPOLIT  B-ORG  I-ORG  \n",
       "O                    2     42    248  \n",
       "B-PER                0      2      0  \n",
       "I-PER                1      0     23  \n",
       "B-LOC                0     13      3  \n",
       "I-LOC                0      0     12  \n",
       "B-MEDIA              0    114      2  \n",
       "I-MEDIA              0     34    176  \n",
       "B-GEOPOLIT           0      2      5  \n",
       "I-GEOPOLIT          16      0      4  \n",
       "B-ORG                0   1112     20  \n",
       "I-ORG                0     75   1812  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_output = trainer.predict(ds_tokenized_test)\n",
    "logits = predictions_output.predictions\n",
    "labels = predictions_output.label_ids\n",
    "preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "pred_labels = [[label_names[p] for (p, l) in zip(pred, label) if l != -100] for pred, label in zip(preds, labels)]\n",
    "\n",
    "y_true_flat = [l for seq in true_labels for l in seq]\n",
    "y_pred_flat = [l for seq in pred_labels for l in seq]\n",
    "\n",
    "cm = confusion_matrix(y_true_flat, y_pred_flat, labels=label_names)\n",
    "cm_df = pd.DataFrame(cm, index=label_names, columns=label_names)\n",
    "print(\"Confusion Matrix:\")\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fa21da23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    GEOPOLIT       0.84      0.54      0.66       674\n",
      "         LOC       0.49      0.78      0.60       689\n",
      "       MEDIA       0.79      0.49      0.60       288\n",
      "         ORG       0.63      0.81      0.71      1182\n",
      "         PER       0.88      0.91      0.90      2012\n",
      "\n",
      "   micro avg       0.73      0.79      0.76      4845\n",
      "   macro avg       0.73      0.70      0.69      4845\n",
      "weighted avg       0.76      0.79      0.76      4845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b248ab6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'PER', 'score': 0.99825144, 'word': 'Барак Обама', 'start': 0, 'end': 11}, {'entity_group': 'LOC', 'score': 0.694159, 'word': 'Юты', 'start': 33, 'end': 36}, {'entity_group': 'GEOPOLIT', 'score': 0.9564898, 'word': 'США', 'start': 44, 'end': 47}, {'entity_group': 'GEOPOLIT', 'score': 0.95044225, 'word': 'Китае', 'start': 50, 'end': 55}, {'entity_group': 'GEOPOLIT', 'score': 0.973243, 'word': 'США', 'start': 66, 'end': 69}, {'entity_group': 'PER', 'score': 0.997154, 'word': 'Барак Обама', 'start': 70, 'end': 81}, {'entity_group': 'LOC', 'score': 0.96809644, 'word': 'Юта', 'start': 116, 'end': 119}, {'entity_group': 'PER', 'score': 0.9407775, 'word': 'Джона Хантсмена - младшего ( John Huntsman Jr. )', 'start': 120, 'end': 164}, {'entity_group': 'GEOPOLIT', 'score': 0.95686287, 'word': 'США', 'start': 172, 'end': 175}, {'entity_group': 'GEOPOLIT', 'score': 0.94162846, 'word': 'Китае', 'start': 178, 'end': 183}, {'entity_group': 'MEDIA', 'score': 0.6737799, 'word': 'The Washington Times', 'start': 191, 'end': 211}, {'entity_group': 'ORG', 'score': 0.7943875, 'word': '.', 'start': 211, 'end': 212}, {'entity_group': 'GEOPOLIT', 'score': 0.68133724, 'word': 'Белого дома', 'start': 237, 'end': 248}, {'entity_group': 'PER', 'score': 0.9996586, 'word': 'Хантсмена', 'start': 262, 'end': 271}, {'entity_group': 'PER', 'score': 0.99759984, 'word': 'Обаме', 'start': 272, 'end': 277}, {'entity_group': 'PER', 'score': 0.9611982, 'word': 'Джефф Бэйдер ( Jeff Bader )', 'start': 319, 'end': 344}, {'entity_group': 'LOC', 'score': 0.9365358, 'word': 'Юты', 'start': 377, 'end': 380}, {'entity_group': 'ORG', 'score': 0.9988246, 'word': 'Республиканской партии', 'start': 524, 'end': 546}, {'entity_group': 'PER', 'score': 0.9988275, 'word': 'Обамы', 'start': 578, 'end': 583}, {'entity_group': 'GEOPOLIT', 'score': 0.91248256, 'word': 'США', 'start': 606, 'end': 609}, {'entity_group': 'PER', 'score': 0.9997561, 'word': 'Джон Маккейн', 'start': 610, 'end': 622}, {'entity_group': 'PER', 'score': 0.879405, 'word': 'Обамы.', 'start': 647, 'end': 653}, {'entity_group': 'PER', 'score': 0.9996473, 'word': 'Хантсмен', 'start': 722, 'end': 730}, {'entity_group': 'PER', 'score': 0.999481, 'word': 'Хантсмен', 'start': 850, 'end': 858}, {'entity_group': 'PER', 'score': 0.99979925, 'word': 'Джона Маккейна', 'start': 892, 'end': 906}, {'entity_group': 'PER', 'score': 0.9994397, 'word': 'Хантсмена', 'start': 1046, 'end': 1055}, {'entity_group': 'PER', 'score': 0.99939895, 'word': 'Обама', 'start': 1064, 'end': 1069}, {'entity_group': 'LOC', 'score': 0.96707684, 'word': 'Юты', 'start': 1137, 'end': 1140}, {'entity_group': 'PER', 'score': 0.9990546, 'word': 'Хантсмен', 'start': 1153, 'end': 1161}, {'entity_group': 'GEOPOLIT', 'score': 0.9346297, 'word': 'США', 'start': 1194, 'end': 1197}, {'entity_group': 'PER', 'score': 0.99379045, 'word': 'Джордже Буше - младшем', 'start': 1202, 'end': 1222}, {'entity_group': 'GEOPOLIT', 'score': 0.9029505, 'word': 'США', 'start': 1246, 'end': 1249}, {'entity_group': 'GEOPOLIT', 'score': 0.8881487, 'word': 'Сингапуре', 'start': 1252, 'end': 1261}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<mark style=\"background-color: cyan\">Барак Обама<sub>(PER)</sub></mark> назначил губернатора <mark style=\"background-color: lightblue\">Юты<sub>(LOC)</sub></mark> послом <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> в <mark style=\"background-color: yellow\">Китае<sub>(GEOPOLIT)</sub></mark> Президент <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: cyan\">Барак Обама<sub>(PER)</sub></mark> 16 мая назначил губернатора штата <mark style=\"background-color: lightblue\">Юта<sub>(LOC)</sub></mark> <mark style=\"background-color: cyan\">Джона Хантсмена-младшего (John Huntsman Jr.)<sub>(PER)</sub></mark> послом <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> в <mark style=\"background-color: yellow\">Китае<sub>(GEOPOLIT)</sub></mark>, пишет <mark style=\"background-color: green\">The Washington Times<sub>(MEDIA)</sub></mark><mark style=\"background-color: red\">.<sub>(ORG)</sub></mark> По словам представителя <mark style=\"background-color: yellow\">Белого дома<sub>(GEOPOLIT)</sub></mark>, кандидатуру <mark style=\"background-color: cyan\">Хантсмена<sub>(PER)</sub></mark> <mark style=\"background-color: cyan\">Обаме<sub>(PER)</sub></mark> предложил помощник по азиатской политике <mark style=\"background-color: cyan\">Джефф Бэйдер (Jeff Bader)<sub>(PER)</sub></mark>, который представил губернатора <mark style=\"background-color: lightblue\">Юты<sub>(LOC)</sub></mark> как человека, отлично знающего китайский язык, разбирающегося в проблемах региона и способного эффективно решать дипломатические задачи. Члены <mark style=\"background-color: red\">Республиканской партии<sub>(ORG)</sub></mark>, в том числе и бывший соперник <mark style=\"background-color: cyan\">Обамы<sub>(PER)</sub></mark> на выборах президента <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> <mark style=\"background-color: cyan\">Джон Маккейн<sub>(PER)</sub></mark>, поприветствовали выбор <mark style=\"background-color: cyan\">Обамы.<sub>(PER)</sub></mark> Они, однако, в то же время признали, что в связи с этим назначением <mark style=\"background-color: cyan\">Хантсмен<sub>(PER)</sub></mark> фактически лишился возможности участвовать в следующих президентских выборах. Во время предвыборной кампании 2008 года <mark style=\"background-color: cyan\">Хантсмен<sub>(PER)</sub></mark> был одним из руководителей штаба <mark style=\"background-color: cyan\">Джона Маккейна<sub>(PER)</sub></mark> и, по оценкам экспертов, именно он мог стать кандидатом от республиканцев на следующих выборах. Многие республиканцы отметили, что, сделав <mark style=\"background-color: cyan\">Хантсмена<sub>(PER)</sub></mark> послом, <mark style=\"background-color: cyan\">Обама<sub>(PER)</sub></mark> устранил потенциально опасного соперника. До избрания губернатором <mark style=\"background-color: lightblue\">Юты<sub>(LOC)</sub></mark> в 2004 году <mark style=\"background-color: cyan\">Хантсмен<sub>(PER)</sub></mark> работал торговым представителем <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> при <mark style=\"background-color: cyan\">Джордже Буше-младшем<sub>(PER)</sub></mark>, а до этого был послом <mark style=\"background-color: yellow\">США<sub>(GEOPOLIT)</sub></mark> в <mark style=\"background-color: yellow\">Сингапуре<sub>(GEOPOLIT)</sub></mark>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=0)\n",
    "\n",
    "text = \"Барак Обама назначил губернатора Юты послом США в Китае Президент США Барак Обама 16 мая назначил губернатора штата Юта Джона Хантсмена-младшего (John Huntsman Jr.) послом США в Китае, пишет The Washington Times. По словам представителя Белого дома, кандидатуру Хантсмена Обаме предложил помощник по азиатской политике Джефф Бэйдер (Jeff Bader), который представил губернатора Юты как человека, отлично знающего китайский язык, разбирающегося в проблемах региона и способного эффективно решать дипломатические задачи. Члены Республиканской партии, в том числе и бывший соперник Обамы на выборах президента США Джон Маккейн, поприветствовали выбор Обамы. Они, однако, в то же время признали, что в связи с этим назначением Хантсмен фактически лишился возможности участвовать в следующих президентских выборах. Во время предвыборной кампании 2008 года Хантсмен был одним из руководителей штаба Джона Маккейна и, по оценкам экспертов, именно он мог стать кандидатом от республиканцев на следующих выборах. Многие республиканцы отметили, что, сделав Хантсмена послом, Обама устранил потенциально опасного соперника. До избрания губернатором Юты в 2004 году Хантсмен работал торговым представителем США при Джордже Буше-младшем, а до этого был послом США в Сингапуре.\"\n",
    "entities = ner_pipeline(text)\n",
    "print(entities)\n",
    "\n",
    "predicted_entities = [\n",
    "    Ne5Span(start=ent[\"start\"], stop=ent[\"end\"], type=ent[\"entity_group\"])\n",
    "    for ent in entities\n",
    "]\n",
    "\n",
    "predicted_labelled_text = LabelledText(text=text, entities=predicted_entities)\n",
    "\n",
    "visualize_labelled_text(predicted_labelled_text, COLOR_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "173131a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./ner_model_synthetic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99db2ed7",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "\n",
    "- MLM FineTuning перед NER дал наилучшее значение Precision: 0.764 и чуть более низкие Recall и F1 по сравнению с прямым fine‑tuning без MLM (0.842 и 0.799 соотв.).\n",
    "- Accuracy везде одинаково высокая и малоприменимая для данной задачи метрика\n",
    "- Добавление синтетических аннотаций заметно ухудшило все ключевые метрики (F1 упал до 0.756, Precision и Recall также на 4-5 п.п.).Вероятно, дело в низком качестве данных или несоответствия синтетики реальному распределению. В особенности из-за того, что в синтетике не присутствовали теги GEOPOLIT и MEDIA, из-за чего показатели упали для типов сущностей.\n",
    "\n",
    "\n",
    "| Подход            | Precision  | Recall     | F1         | Accuracy   |\n",
    "| ----------------- | ----------:| ----------:| ----------:| ----------:|\n",
    "| Simple Finetuning | 0.76024590 | 0.84231166 | 0.79917751 | 0.96348986 |\n",
    "| MLM FT            | 0.76396226 | 0.83570691 | 0.79822572 | 0.96383158 |\n",
    "| With Synthetic    | 0.72606382 | 0.78885448 | 0.75615787 | 0.96327404 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dff4ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
